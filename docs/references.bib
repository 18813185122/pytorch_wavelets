
@article{field_scale-invariance_1993,
  title = {Scale-Invariance and Self-Similar `Wavelet' Transforms: {{An}} Analysis of Natural Scenes and Mammalian Visual Systems},
  shorttitle = {Scale-Invariance and Self-Similar `Wavelet' Transforms},
  abstract = {The processing of spatial patterns by the mammalian visual system shows a number of similarities to the `wavelet transforms' which have recently attracted considerable interest outside of the...},
  timestamp = {2016-07-28T16:28:34Z},
  urldate = {2016-07-28},
  journal = {ResearchGate},
  author = {Field, D. J.},
  year = {1993},
  note = {00216},
  file = {Field_Scale-invariance and self-similar ‘wavelet’ transforms.pdf:/Users/fergalcotter/Dropbox/Papers/Field_Scale-invariance and self-similar ‘wavelet’ transforms.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/SAPHZHU3/266310150_Scale-invariance_and_self-similar_'wavelet'_transforms_An_analysis_of_natural_scenes_.html:text/html},
  groups = {Sparsity for Vision,Sparsity for Vision}
}

@article{gu_recent_2015,
  title = {Recent {{Advances}} in {{Convolutional Neural Networks}}},
  abstract = {In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Due to the lack of training data and computing power in early days, it is hard to train a large high-capacity convolutional neural network without overfitting. After the rapid growth in the amount of the annotated data and the recent improvements in the strengths of graphics processor units (GPUs), the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. Besides, we also introduce some applications of convolutional neural networks in computer vision.},
  timestamp = {2016-08-09T11:43:40Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.07108},
  primaryClass = {cs},
  urldate = {2016-08-09},
  journal = {arXiv:1512.07108 [cs]},
  author = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang},
  month = dec,
  year = {2015},
  note = {00002},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: review, journal},
  file = {Gu et al_2015_Recent Advances in Convolutional Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Gu et al_2015_Recent Advances in Convolutional Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/72QQ5TWA/1512.html:text/html},
  groups = {summary papers,summary papers,summary papers}
}

@incollection{plate_avoiding_2012,
  series = {Lecture Notes in Computer Science},
  title = {Avoiding {{Roundoff Error}} in {{Backpropagating Derivatives}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {One significant source of roundoff error in backpropagation networks is the calculation of derivatives of unit outputs with respect to their total inputs. The roundoff error can lead result in high relative error in derivatives, and in particular, derivatives being calculated to be zero when in fact they are small but non-zero. This roundoff error is easily avoided with a simple programming trick which has a small memory overhead (one or two extra floating point numbers per unit) and an insignificant computational overhead.},
  language = {en},
  timestamp = {2016-08-09T23:23:40Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Plate, Tony},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00001},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {225--230},
  file = {Plate_2012_Avoiding Roundoff Error in Backpropagating Derivatives.pdf:/Users/fergalcotter/Dropbox/Papers/Plate_2012_Avoiding Roundoff Error in Backpropagating Derivatives.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/5VEV4NP7/978-3-642-35289-8_15.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_15}
}

@misc{nick_kingsbury_visualisation_2015,
  address = {Adelaide University},
  title = {Visualisation of {{Convolutional Networks}} and {{Multiscale Scatter}}-{{Nets}}},
  timestamp = {2015-11-26T20:02:39Z},
  author = {{Nick Kingsbury}},
  month = nov,
  year = {2015},
  note = {00000},
  keywords = {Unread},
  file = {DeconvNets&ScatterNetsTalk1.pdf:/Users/fergalcotter/Dropbox/Papers/DeconvNets&ScatterNetsTalk1.pdf:application/pdf},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@article{kawaguchi_deep_2016,
  title = {Deep {{Learning}} without {{Poor Local Minima}}},
  abstract = {In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. For an expected loss function of a deep nonlinear neural network, we prove the following statements under the independence assumption adopted from recent work: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) the property of saddle points differs for shallow networks (with three layers) and deeper networks (with more than three layers). Moreover, we prove that the same four statements hold for deep linear neural networks with any depth, any widths and no unrealistic assumptions. As a result, we present an instance, for which we can answer to the following question: how difficult to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima and the property of the saddle points). We note that even though we have advanced the theoretical foundations of deep learning, there is still a gap between theory and practice.},
  timestamp = {2016-05-25T09:27:48Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.07110},
  primaryClass = {cs, math, stat},
  urldate = {2016-05-25},
  journal = {arXiv:1605.07110 [cs, math, stat]},
  author = {Kawaguchi, Kenji},
  month = may,
  year = {2016},
  note = {00000},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  annote = {Paper about looking at the mathematical reason why deep networks are good. Interesting to have the background there to provide the basis for doing the work.},
  file = {arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/8RAQN7N8/1605.html:text/html},
  groups = {Rigorous Deep Learning,Rigorous Deep Learning}
}

@inproceedings{kingsbury_dual-tree_2000,
  title = {A Dual-Tree Complex Wavelet Transform with Improved Orthogonality and Symmetry Properties},
  abstract = {We present a new form of the Dual-Tree Complex Wavelet Transform (DT CWT) with improved orthogonality and symmetry properties. Beyond level 1, the previous form used alternate odd-length and even-length bi-orthogonal filter pairs in the two halves of the dual tree, whereas the new form employs a single design of even-length filter with asymmetric coefficients. These are similar to Daubechies orthonormal filters, but designed with the additional constraint that the filter group delay should be approximately one quarter of the sample period. The filters in the two trees are just the time-reverse of each other, as are the analysis and reconstruction filters. This leads to a transform, which can use shorter filters, which is orthonormal beyond level 1, and in which the two trees are very closely matched and have a more symmetric sub-sampling structure, but which preserves the key DT CWT advantages of approximate shift-invariance and good directional selectivity in multiple dimensions.},
  timestamp = {2016-01-20T12:28:29Z},
  author = {Kingsbury, N.},
  year = {2000},
  note = {00355},
  file = {Kingsbury_2000_A dual-tree complex wavelet transform with improved orthogonality and symmetry.pdf:/Users/fergalcotter/Dropbox/Papers/Kingsbury_2000_A dual-tree complex wavelet transform with improved orthogonality and symmetry.pdf:application/pdf},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@article{nguyen_deep_2014,
  title = {Deep {{Neural Networks}} Are {{Easily Fooled}}: {{High Confidence Predictions}} for {{Unrecognizable Images}}},
  shorttitle = {Deep {{Neural Networks}} Are {{Easily Fooled}}},
  abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99\% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
  timestamp = {2016-09-13T11:32:57Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.1897},
  primaryClass = {cs},
  urldate = {2016-08-24},
  journal = {arXiv:1412.1897 [cs]},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  month = dec,
  year = {2014},
  note = {00164},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: To appear at CVPR 2015},
  file = {Nguyen et al_2014_Deep Neural Networks are Easily Fooled.pdf:/Users/fergalcotter/Dropbox/Papers/Nguyen et al_2014_Deep Neural Networks are Easily Fooled.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/2FUQVBDB/1412.html:text/html},
  groups = {criticisms,criticisms,criticisms}
}

@article{magarey_motion_1998,
  title = {Motion Estimation Using a Complex-Valued Wavelet Transform},
  volume = {46},
  doi = {10.1109/78.668557},
  abstract = {This paper describes a new motion estimation algorithm that is potentially useful for both computer vision and video compression applications, It is hierarchical in structure, using a separable two-dimensional (2-D) discrete wavelet transform (DWT) on each frame to efficiently construct a multiresolution pyramid of subimages, The DWT is based on a complex-valued pair of four-tap FIR filters with Gabor-like characteristics. The resulting complex DWT (CDWT) effectively implements an analysis by an ensemble of Gabor-like filters with a variety of orientations and scales, The phase difference between the subband coefficients of each frame at a given subpel bears a predictable relation to a local translation in the region of the reference frame subtended by that subpel, That relation is used to estimate the displacement field at the coarsest scale of the multiresolution pyramid, Each estimate is accompanied by a directional confidence measure in the form of the parameters of a quadratic matching surface, The initial estimate field is progressively refined by a coarse-to-fine strategy in which finer scale information is appropriately incorporated at each stage, The accuracy, efficiency, and robustness of the new algorithm are demonstrated in comparison testing against hierarchical implementations of intensity gradient-based and fractional-precision block matching motion estimators.},
  timestamp = {2016-01-20T12:28:20Z},
  number = {4},
  journal = {Ieee Transactions on Signal Processing},
  author = {Magarey, J. and Kingsbury, N.},
  month = apr,
  year = {1998},
  note = {00228},
  pages = {1069--1084},
  file = {Magarey_Kingsbury_1998_Motion estimation using a complex-valued wavelet transform.pdf:/Users/fergalcotter/Dropbox/Papers/Magarey_Kingsbury_1998_Motion estimation using a complex-valued wavelet transform.pdf:application/pdf},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@incollection{yosinski_how_2014,
  title = {How Transferable Are Features in Deep Neural Networks?},
  timestamp = {2016-07-29T14:44:14Z},
  urldate = {2016-07-15},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  publisher = {{Curran Associates, Inc.}},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  note = {00217},
  keywords = {_tablet},
  pages = {3320--3328},
  file = {Yosinski et al_2014_How transferable are features in deep neural networks.pdf:/Users/fergalcotter/Dropbox/Papers/Yosinski et al_2014_How transferable are features in deep neural networks.pdf:application/pdf;NIPS Snapshort:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/K87W6KT8/5347-how-transferable-are-features-in-deep-neural-networks.html:text/html},
  groups = {Transfer Learning,Transfer Learning,Transfer Learning}
}

@article{bruna_signal_2013,
  title = {Signal {{Recovery}} from {{Pooling Representations}}},
  abstract = {In this work we compute lower Lipschitz bounds of \$$\backslash$ell\_p\$ pooling operators for \$p=1, 2, $\backslash$infty\$ as well as \$$\backslash$ell\_p\$ pooling operators preceded by half-rectification layers. These give sufficient conditions for the design of invertible neural network layers. Numerical experiments on MNIST and image patches confirm that pooling layers can be inverted with phase recovery algorithms. Moreover, the regularity of the inverse pooling, controlled by the lower Lipschitz constant, is empirically verified with a nearest neighbor regression.},
  timestamp = {2016-02-01T16:15:49Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1311.4025},
  primaryClass = {stat},
  urldate = {2016-02-01},
  journal = {arXiv:1311.4025 [stat]},
  author = {Bruna, Joan and Szlam, Arthur and LeCun, Yann},
  month = nov,
  year = {2013},
  note = {00008},
  keywords = {Statistics - Machine Learning,Unread},
  annote = {Comment: 17 pages, 3 figures},
  file = {Bruna et al_2013_Signal Recovery from Pooling Representations.pdf:/Users/fergalcotter/Dropbox/Papers/Bruna et al_2013_Signal Recovery from Pooling Representations.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/AHQCZXAD/1311.html:text/html},
  groups = {advanced,Visualization \& Generative,advanced,advanced,Visualization \& Generative,Visualization \& Generative}
}

@inproceedings{oyallon_deep_2015,
  title = {Deep {{Roto}}-{{Translation Scattering}} for {{Object Classification}}},
  timestamp = {2016-03-01T15:28:30Z},
  urldate = {2016-03-01},
  author = {Oyallon, Edouard and Mallat, Stephane},
  year = {2015},
  note = {00011},
  pages = {2865--2873},
  file = {Oyallon_Mallat_2015_Deep Roto-Translation Scattering for Object Classification.pdf:/Users/fergalcotter/Dropbox/Papers/Oyallon_Mallat_2015_Deep Roto-Translation Scattering for Object Classification.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/IB99F9KA/Oyallon_Deep_Roto-Translation_Scattering_2015_CVPR_paper.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted,Scatternets,Scatternets}
}

@misc{hinton_recognize_2006,
  title = {To {{Recognize Shapes}}, {{First Learn}} to {{Generate Images}}},
  timestamp = {2016-09-16T18:23:54Z},
  author = {Hinton, Geoff},
  month = oct,
  year = {2006},
  note = {00000},
  keywords = {Unread},
  file = {Hinton - 2006 - To Recognize Shapes, First Learn to Generate Image.pdf:/Users/fergalcotter/Dropbox/Papers/Geoff Hinton_2006_To Recognize Shapes, First Learn to Generate Images.pdf:application/pdf},
  groups = {Visualization \& Generative,Visualization \& Generative,Visualization \& Generative}
}

@incollection{rumelhart_parallel_1986,
  address = {Cambridge, MA, USA},
  title = {Parallel {{Distributed Processing}}: {{Explorations}} in the {{Microstructure}} of {{Cognition}}, {{Vol}}. 1},
  isbn = {978-0-262-68053-0},
  shorttitle = {Parallel {{Distributed Processing}}},
  timestamp = {2016-08-24T00:42:57Z},
  urldate = {2016-08-24},
  publisher = {{MIT Press}},
  author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
  editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
  year = {1986},
  note = {00087},
  pages = {318--362},
  file = {Rumelhart et al_1986_Parallel Distributed Processing.pdf:/Users/fergalcotter/Dropbox/Papers/Rumelhart et al_1986_Parallel Distributed Processing.pdf:application/pdf},
  groups = {CNNs,CNNs}
}

@incollection{lawrence_neural_2012,
  series = {Lecture Notes in Computer Science},
  title = {Neural {{Network Classification}} and {{Prior Class Probabilities}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {A commonly encountered problem in MLP (multi-layer perceptron) classification problems is related to the prior probabilities of the individual classes \textendash{} if the number of training examples that correspond to each class varies significantly between the classes, then it may be harder for the network to learn the rarer classes in some cases. Such practical experience does not match theoretical results which show that MLPs approximate Bayesian a posteriori probabilities (independent of the prior class probabilities). Our investigation of the problem shows that the difference between the theoretical and practical results lies with the assumptions made in the theory (accurate estimation of Bayesian a posteriori probabilities requires the network to be large enough, training to converge to a global minimum, infinite training data, and the a priori class probabilities of the test set to be correctly represented in the training set). Specifically, the problem can often be traced to the fact that efficient MLP training mechanisms lead to sub-optimal solutions for most practical problems. In this chapter, we demonstrate the problem, discuss possible methods for alleviating it, and introduce new heuristics which are shown to perform well on a sample ECG classification problem. The heuristics may also be used as a simple means of adjusting for unequal misclassification costs.},
  language = {en},
  timestamp = {2016-08-09T23:22:24Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Lawrence, Steve and Burns, Ian and Back, Andrew and Tsoi, Ah Chung and Giles, C. Lee},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00117},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {295--309},
  file = {Lawrence et al_2012_Neural Network Classification and Prior Class Probabilities.pdf:/Users/fergalcotter/Dropbox/Papers/Lawrence et al_2012_Neural Network Classification and Prior Class Probabilities.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/S29BKNUN/978-3-642-35289-8_19.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_19}
}

@article{kingma_adam:_2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  timestamp = {2016-08-07T12:59:11Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  primaryClass = {cs},
  urldate = {2016-08-07},
  journal = {arXiv:1412.6980 [cs]},
  author = {Kingma, Diederik and Ba, Jimmy},
  month = dec,
  year = {2014},
  note = {00572},
  keywords = {Computer Science - Learning},
  annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
  file = {Kingma_Ba_2014_Adam.pdf:/Users/fergalcotter/Dropbox/Papers/Kingma_Ba_2014_Adam.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/33XDZ7EF/1412.html:text/html},
  groups = {network features,network features,network features,reading group,reading group,reading group}
}

@article{soatto_visual_2014,
  title = {Visual {{Representations}}: {{Defining Properties}} and {{Deep Approximations}}},
  shorttitle = {Visual {{Representations}}},
  abstract = {Visual representations are defined in terms of minimal sufficient statistics of visual data, for a class of tasks, that are also invariant to nuisance variability. Minimal sufficiency guarantees that we can store a representation in lieu of raw data with smallest complexity and no performance loss on the task at hand. Invariance guarantees that the statistic is constant with respect to uninformative transformations of the data. We derive analytical expressions for such representations and show they are related to feature descriptors commonly used in computer vision, as well as to convolutional neural networks. This link highlights the assumptions and approximations tacitly assumed by these methods and explains empirical practices such as clamping, pooling and joint normalization.},
  timestamp = {2016-08-06T15:59:00Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.7676},
  primaryClass = {cs},
  urldate = {2016-08-06},
  journal = {arXiv:1411.7676 [cs]},
  author = {Soatto, Stefano and Chiuso, Alessandro},
  month = nov,
  year = {2014},
  note = {00000},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote = {Comment: UCLA CSD TR140023, Nov. 12, 2014, revised April 13, 2015, November 13, 2015, February 28, 2016},
  file = {Soatto_Chiuso_2014_Visual Representations.pdf:/Users/fergalcotter/Dropbox/Papers/Soatto_Chiuso_2014_Visual Representations.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/A9CR6RD2/1411.html:text/html},
  groups = {advanced,ICVSS,advanced,advanced,ICVSS}
}

@incollection{lecun_efficient_2012,
  series = {Lecture Notes in Computer Science},
  title = {Efficient {{BackProp}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most ``classical'' second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
  language = {en},
  timestamp = {2016-08-09T23:21:49Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {LeCun, Yann A. and Bottou, L{\'e}on and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {01085},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {9--48},
  file = {LeCun et al_2012_Efficient BackProp.pdf:/Users/fergalcotter/Dropbox/Papers/LeCun et al_2012_Efficient BackProp.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/TPNIZZTW/978-3-642-35289-8_3.html:text/html},
  groups = {Tricks of the Trade,Generic Design,Tricks of the Trade,Generic Design,Generic Design},
  doi = {10.1007/978-3-642-35289-8_3}
}

@article{saxe_exact_2013,
  title = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
  timestamp = {2016-08-09T15:52:50Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.6120},
  primaryClass = {cond-mat, q-bio, stat},
  urldate = {2016-08-09},
  journal = {arXiv:1312.6120 [cond-mat, q-bio, stat]},
  author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  month = dec,
  year = {2013},
  note = {00087},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  annote = {uses orthogonal matrix initialization for the weights rather than gaussian noise, which is only approximately orthogonal.},
  file = {Saxe et al_2013_Exact solutions to the nonlinear dynamics of learning in deep linear neural.pdf:/Users/fergalcotter/Dropbox/Papers/Saxe et al_2013_Exact solutions to the nonlinear dynamics of learning in deep linear neural.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/BKC39HTK/1312.html:text/html},
  groups = {network features,network features,network features}
}

@inproceedings{zhang_image_2009,
  title = {Image Deconvolution Using a {{Gaussian Scale Mixtures}} Model to Approximate the Wavelet Sparseness Constraint},
  timestamp = {2015-11-19T14:13:21Z},
  urldate = {2015-11-03},
  booktitle = {Acoustics, {{Speech}} and {{Signal Processing}}, 2009. {{ICASSP}} 2009. {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  author = {Zhang, Yingsong and Kingsbury, Nick},
  year = {2009},
  note = {00009},
  pages = {681--684},
  file = {Zhang_Kingsbury_2009_Image deconvolution using a Gaussian Scale Mixtures model to approximate the.pdf:/Users/fergalcotter/Dropbox/Papers/Zhang_Kingsbury_2009_Image deconvolution using a Gaussian Scale Mixtures model to approximate the.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@techreport{radoslaw_mantiuk_-it-yourself_????,
  title = {Do-{{It}}-{{Yourself Eye Tracker}}: {{Low}}-{{Cost Pupil}}-{{Based Eye Tracker}} for {{Computer Graphics Applications}}},
  abstract = {Eye tracking technologies offer sophisticated methods for
capturing humans' gaze direction but their popularity in multimedia and
computer graphics systems is still low. One of the main reasons for this
are the high cost of commercial eye trackers that comes to 25,000 euros.
Interestingly, this price seems to stem from the costs incurred in research
rather than the value of used hardware components. In this work we show
that an eye tracker of a satisfactory precision can be built in the budget
of 30 euros. In the paper detailed instruction on how to construct a low
cost pupil-based eye tracker and utilise open source software to control
its behaviour is presented. We test the accuracy of our eye tracker and reveal
that its precision is comparable to commercial video-based devices.
We give an example of application in which our eye tracker is used to
control the depth-of-field rendering in real time virtual environment.},
  timestamp = {2016-08-07T17:20:49Z},
  institution = {West Pomeranian University of Technology in Szczecin, Faculty of Computer Science},
  author = {{Radoslaw Mantiuk} and {Michal Kowalik} and {Adam Nowosielski} and {Bartosz Bazyluk}},
  note = {00032},
  file = {Radoslaw Mantiuk et al_Do-It-Yourself Eye Tracker.pdf:/Users/fergalcotter/Dropbox/Papers/Radoslaw Mantiuk et al_Do-It-Yourself Eye Tracker.pdf:application/pdf},
  groups = {Eye Tracking,Eye Tracking}
}

@article{ghahramani_probabilistic_2015,
  title = {Probabilistic Machine Learning and Artificial Intelligence},
  volume = {521},
  copyright = {\textcopyright{} 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {0028-0836},
  doi = {10.1038/nature14541},
  abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
  language = {en},
  timestamp = {2016-10-25T17:03:45Z},
  number = {7553},
  urldate = {2016-10-25},
  journal = {Nature},
  author = {Ghahramani, Zoubin},
  month = may,
  year = {2015},
  keywords = {Computer science,Mathematics and computing,Neuroscience},
  pages = {452--459},
  file = {Ghahramani_2015_Probabilistic machine learning and artificial intelligence.pdf:/Users/fergalcotter/Dropbox/Papers/Ghahramani_2015_Probabilistic machine learning and artificial intelligence.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/2BDTM2GI/nature14541.html:text/html},
  groups = {Statistical Methods,Statistical Methods}
}

@article{shi_real-time_2016,
  title = {Real-{{Time Single Image}} and {{Video Super}}-{{Resolution Using}} an {{Efficient Sub}}-{{Pixel Convolutional Neural Network}}},
  abstract = {Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.},
  timestamp = {2016-10-18T11:12:43Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.05158},
  primaryClass = {cs, stat},
  urldate = {2016-10-18},
  journal = {arXiv:1609.05158 [cs, stat]},
  author = {Shi, Wenzhe and Caballero, Jose and Husz{\'a}r, Ferenc and Totz, Johannes and Aitken, Andrew P. and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
  month = sep,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  annote = {Comment: CVPR 2016 paper with updated affiliations and supplemental material, fixed typo in equation 4},
  file = {1609.07009.pdf:/Users/fergalcotter/Dropbox/Papers/1609.07009.pdf:application/pdf;Shi et al_2016_Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel.pdf:/Users/fergalcotter/Dropbox/Papers/Shi et al_2016_Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/E2HRVWZA/1609.html:text/html},
  groups = {Visualization \& Generative,Visualization \& Generative,Visualization \& Generative}
}

@article{sifre_rigid-motion_2014,
  title = {Rigid-{{Motion Scattering}} for {{Texture Classification}}},
  abstract = {A rigid-motion scattering computes adaptive invariants along translations and rotations, with a deep convolutional network. Convolutions are calculated on the rigid-motion group, with wavelets defined on the translation and rotation variables. It preserves joint rotation and translation information, while providing global invariants at any desired scale. Texture classification is studied, through the characterization of stationary processes from a single realization. State-of-the-art results are obtained on multiple texture data bases, with important rotation and scaling variabilities.},
  timestamp = {2016-01-25T16:43:35Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1403.1687},
  primaryClass = {cs},
  urldate = {2015-11-30},
  journal = {arXiv:1403.1687 [cs]},
  author = {Sifre, Laurent and Mallat, St{\'e}phane},
  month = mar,
  year = {2014},
  note = {00003},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Unread},
  annote = {Comment: 19 pages, submitted to International Journal of Computer Vision},
  file = {Sifre_Mallat_2014_Rigid-Motion Scattering for Texture Classification.pdf:/Users/fergalcotter/Dropbox/Papers/Sifre_Mallat_2014_Rigid-Motion Scattering for Texture Classification.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/BGFJPMFX/1403.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@article{srivastava_training_2015,
  title = {Training {{Very Deep Networks}}},
  abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.},
  timestamp = {2016-08-07T13:03:17Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.06228},
  primaryClass = {cs},
  urldate = {2016-08-07},
  journal = {arXiv:1507.06228 [cs]},
  author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"u}rgen},
  month = jul,
  year = {2015},
  note = {00064},
  keywords = {68T01,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,G.1.6,I.2.6},
  annote = {Comment: 11 pages. Extends arXiv:1505.00387. Project webpage is at http://people.idsia.ch/\textasciitilde{}rupesh/very\_deep\_learning/. in Advances in Neural Information Processing Systems 2015},
  file = {Srivastava et al_2015_Training Very Deep Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Srivastava et al_2015_Training Very Deep Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/3CX7XUPJ/1507.html:text/html},
  groups = {Generic Design,Generic Design,Generic Design}
}

@article{hull_database_1994,
  title = {A Database for Handwritten Text Recognition Research},
  volume = {16},
  issn = {0162-8828},
  doi = {10.1109/34.291440},
  abstract = {An image database for handwritten text recognition research is described. Digital images of approximately 5000 city names, 5000 state names, 10000 ZIP Codes, and 50000 alphanumeric characters are included. Each image was scanned from mail in a working post office at 300 pixels/in in 8-bit gray scale on a high-quality flat bed digitizer. The data were unconstrained for the writer, style, and method of preparation. These characteristics help overcome the limitations of earlier databases that contained only isolated characters or were prepared in a laboratory setting under prescribed circumstances. Also, the database is divided into explicit training and testing sets to facilitate the sharing of results among researchers as well as performance comparisons},
  timestamp = {2016-09-13T11:32:58Z},
  number = {5},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Hull, J. J.},
  month = may,
  year = {1994},
  note = {00905},
  keywords = {8 bit,alphanumeric characters,Character recognition,Cities and towns,digital images,flat bed digitizer,Gray-scale,gray scale,Handwriting recognition,handwritten text recognition,image database,Image databases,Performance analysis,performance comparisons,performance evaluation,Postal services,style,Testing,Text recognition,visual databases,writer,Writing},
  pages = {550--554},
  file = {Hull_1994_A database for handwritten text recognition research.pdf:/Users/fergalcotter/Dropbox/Papers/Hull_1994_A database for handwritten text recognition research.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/PG7BZX6P/abs_all.html:text/html},
  groups = {datasets,datasets,datasets}
}

@inproceedings{kingsbury_design_2003,
  title = {Design of {{Q}}-Shift Complex Wavelets for Image Processing Using Frequency Domain Energy Minimization},
  volume = {1},
  doi = {10.1109/ICIP.2003.1247137},
  abstract = {This paper proposes a new method of designing finite-support wavelet filters, based on minimization of energy in key parts of the frequency domain. In particular this technique is shown to be very effective for designing families of filters that are suitable for use in the shift-invariant dual-tree complex wavelet structure that has been developed by the author recently, and has been shown to be important for a range of image processing applications. The dual-tree structure requires most of the wavelet filters to have a well-controlled group delay, equivalent to one quarter of a sample period, in order to achieve optimal shift invariance. The proposed new design technique allows this requirement to be included along with the usual smoothness and perfect reconstruction properties to yield wavelet filters with a unique combination of features: linear phase, tight frame, compact spatial support, good frequency domain selectivity with low sidelobe levels, approximate shift invariance, and good directional selectivity in two or more dimensions.},
  timestamp = {2016-08-24T02:35:20Z},
  booktitle = {2003 {{International Conference}} on {{Image Processing}}, 2003. {{ICIP}} 2003. {{Proceedings}}},
  author = {Kingsbury, N.},
  month = sep,
  year = {2003},
  note = {00115},
  keywords = {Continuous wavelet transforms,Design engineering,Discrete wavelet transforms,Filters,Frequency domain analysis,frequency domain energy minimization,frequency domain selectivity,group delay,image processing,low sidelobe level,minimisation,Minimization methods,Multidimensional signal processing,optimal shift invariance,Propagation delay,Q-shift complex wavelet transform,shift-invariant dual-tree complex wavelet structure,Wavelet domain,wavelet filter,wavelet transforms},
  pages = {I--1013--16 vol.1},
  file = {Kingsbury_2003_Design of Q-shift complex wavelets for image processing using frequency domain.pdf:/Users/fergalcotter/Dropbox/Papers/Kingsbury_2003_Design of Q-shift complex wavelets for image processing using frequency domain.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/JHCS4WSW/1247137.html:text/html},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@phdthesis{waldspurger_wavelet_2012,
  address = {Paris, France},
  type = {PhD Thesis},
  title = {Wavelet Transform Modulus: Phase Retrieval and Scattering},
  abstract = {Automatically understanding the content of a natural signal, like a sound or an image, is in
general a difficult task. In their naive representation, signals are indeed complicated objects,
belonging to high-dimensional spaces. With a different representation, they can however be
easier to interpret.
This thesis considers a representation commonly used in these cases, in particular for the
analysis of audio signals: the modulus of the wavelet transform. To better understand the
behaviour of this operator, we study, from a theoretical as well as algorithmic point of view, the
corresponding inverse problem: the reconstruction of a signal from the modulus of its wavelet
transform.
This problem belongs to a wider class of inverse problems: phase retrieval problems. In a
first chapter, we describe a new algorithm, PhaseCut, which numerically solves a generic phase
retrieval problem. Like the similar algorithm PhaseLift, PhaseCut relies on a convex relaxation
of the phase retrieval problem, which happens to be of the same form as relaxations of the widely
studied problem MaxCut. We compare the performances of PhaseCut and PhaseLift, in terms
of precision and complexity.
In the next two chapters, we study the specific case of phase retrieval for the wavelet transform.
We show that any function with no negative frequencies is uniquely determined (up to
a global phase) by the modulus of its wavelet transform, but that the reconstruction from the
modulus is not stable to noise, for a strong notion of stability. However, we prove a local stability
property. We also present a new non-convex phase retrieval algorithm, which is specific to the
case of the wavelet transform, and we numerically study its performances.
Finally, in the last two chapters, we study a more sophisticated representation, built from
the modulus of the wavelet transform: the scattering transform. Our goal is to understand
which properties of a signal are characterized by its scattering transform. We first prove that
the energy of scattering coefficients of a signal, at a given order, is upper bounded by the energy
of the signal itself, convolved with a high-pass filter that depends on the order. We then study
a generalization of the scattering transform, for stationary processes. We show that, in finite
dimension, this generalized transform preserves the norm. In dimension one, we also show that
the generalized scattering coefficients of a process characterize the tail of its distribution.},
  timestamp = {2016-02-01T16:32:49Z},
  urldate = {2016-02-01},
  school = {{\'E}cole Normale Sup{\'e}rieure},
  author = {Waldspurger, Irene},
  month = nov,
  year = {2012},
  note = {00000},
  keywords = {Unread},
  file = {Waldspurger_2012_Wavelet transform modulus.pdf:/Users/fergalcotter/Dropbox/Papers/Waldspurger_2012_Wavelet transform modulus.pdf:application/pdf},
  groups = {advanced,advanced,advanced}
}

@article{selesnick_hilbert_2001,
  title = {Hilbert Transform Pairs of Wavelet Bases},
  volume = {8},
  issn = {1070-9908},
  doi = {10.1109/97.923042},
  abstract = {This paper considers the design of pairs of wavelet bases where the wavelets form a Hilbert transform pair. The derivation is based on the limit functions defined by the infinite product formula. It is found that the scaling filters should be offset from one another by a half sample. This gives an alternative derivation and explanation for the result by Kingsbury (1999), that the dual-tree DWT is (nearly) shift-invariant when the scaling filters satisfy the same offset.},
  timestamp = {2016-07-31T19:43:55Z},
  number = {6},
  journal = {IEEE Signal Processing Letters},
  author = {Selesnick, I.W.},
  month = jun,
  year = {2001},
  note = {00351},
  keywords = {Delay,Discrete transforms,Discrete wavelet transforms,dual-tree DWT,Encoding,filter bank,filtering theory,Fourier transforms,half sample,Hilbert transform pairs,Hilbert transforms,infinite product formula,limit functions,scaling filters,shift-invariant,signal processing,Transient analysis,Wavelet analysis,wavelet bases,wavelet transforms},
  pages = {170--173},
  annote = {Simple mathsy paper. Proves the half-sample delay condition},
  file = {IEEE Xplore Full Text PDF:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/GW5DTFXB/Selesnick_2001_Hilbert transform pairs of wavelet bases.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/XE22KAT7/abs_all.html:text/html},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@incollection{kingsbury_shift_1999,
  title = {Shift Invariant Properties of the Dual-Tree Complex Wavelet Transform},
  abstract = {We discuss the shift invariant properties of a new implementation of the Discrete Wavelet Transform, which employs a dual tree of wavelet filters to obtain the real and imaginary parts of complex: wavelet coefficients. This introduces limited redundancy (2(m):1 for m-dimensional signals) and allows the transform to provide approximate shift invariance and directionally selective filters (properties lacking in the traditional wavelet transform) while preserving the usual properties of perfect reconstruction and computational efficiency with good well-balanced frequency responses.},
  timestamp = {2016-01-20T12:28:37Z},
  booktitle = {Icassp '99: 1999 {{Ieee International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}, {{Proceedings Vols I}}-{{Vi}}},
  author = {Kingsbury, N.},
  year = {1999},
  note = {00234},
  pages = {1221--1224},
  file = {Kingsbury_1999_Shift invariant properties of the dual-tree complex wavelet transform.pdf:/Users/fergalcotter/Dropbox/Papers/Kingsbury_1999_Shift invariant properties of the dual-tree complex wavelet transform.pdf:application/pdf},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@article{gatys_neural_2015,
  title = {A {{Neural Algorithm}} of {{Artistic Style}}},
  abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
  timestamp = {2016-02-02T20:13:50Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1508.06576},
  primaryClass = {cs, q-bio},
  urldate = {2016-02-02},
  journal = {arXiv:1508.06576 [cs, q-bio]},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  month = aug,
  year = {2015},
  note = {00007},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {Gatys et al_2015_A Neural Algorithm of Artistic Style.pdf:/Users/fergalcotter/Dropbox/Papers/Gatys et al_2015_A Neural Algorithm of Artistic Style.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/I3FS73XK/1508.html:text/html},
  groups = {Visualization \& Generative,Visualization \& Generative,Visualization \& Generative,style,style,style}
}

@inproceedings{szegedy_going_2015,
  title = {Going {{Deeper With Convolutions}}},
  timestamp = {2016-01-14T14:02:04Z},
  urldate = {2015-11-29},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  note = {00569},
  keywords = {Key Paper,Unread},
  pages = {1--9},
  file = {Szegedy et al_2015_Going Deeper With Convolutions.pdf:/Users/fergalcotter/Dropbox/Papers/Szegedy et al_2015_Going Deeper With Convolutions.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/E7T97XTB/Szegedy_Going_Deeper_With_2015_CVPR_paper.html:text/html},
  groups = {state of the art,state of the art,state of the art}
}

@book{cormen_introduction_2009,
  address = {Cambridge, Mass},
  edition = {3rd edition},
  title = {Introduction to {{Algorithms}}, 3rd {{Edition}}},
  isbn = {978-0-262-03384-8},
  abstract = {Some books on algorithms are rigorous but incomplete; others cover masses                 of material but lack rigor. Introduction to Algorithms uniquely                 combines rigor and comprehensiveness. The book covers a broad range of algorithms in                 depth, yet makes their design and analysis accessible to all levels of readers. Each                 chapter is relatively self-contained and can be used as a unit of study. The                 algorithms are described in English and in a pseudocode designed to be readable by                 anyone who has done a little programming. The explanations have been kept elementary                 without sacrificing depth of coverage or mathematical rigor.The                 first edition became a widely used text in universities worldwide as well as the                 standard reference for professionals. The second edition featured new chapters on                 the role of algorithms, probabilistic analysis and randomized algorithms, and linear                 programming. The third edition has been revised and updated throughout. It includes                 two completely new chapters, on van Emde Boas trees and multithreaded algorithms,                 substantial additions to the chapter on recurrence (now called                 "Divide-and-Conquer"), and an appendix on matrices. It features improved                 treatment of dynamic programming and greedy algorithms and a new notion of                 edge-based flow in the material on flow networks. Many new exercises and problems                 have been added for this edition. As of the third edition, this textbook is                 published exclusively by the MIT Press.},
  language = {English},
  timestamp = {2016-01-20T14:04:43Z},
  publisher = {{The MIT Press}},
  author = {Cormen, Thomas and Leiserson, Charles and Rivest, Ronald and Stein, Clifford},
  month = jul,
  year = {2009},
  note = {00000},
  groups = {Books}
}

@inproceedings{kingsbury_dual-tree_1998,
  address = {Utah},
  title = {The {{Dual}}-{{Tree Complex Wavelet Transform}}: {{A New Technique For Shift Invariance And Directional Filters}}},
  shorttitle = {The {{Dual}}-{{Tree Complex Wavelet Transform}}},
  abstract = {A new implementation of the Discrete Wavelet Transform is presented, suitable for a range of signal and image processing applications. It employs a dual tree of wavelet filters to obtain the real and imaginary parts of complex wavelet coefficients. This introduces limited redundancy (4:1 for 2-dimensional signals) and allows the transform to provide approximate shift invariance and directionally selective filters (properties lacking in the traditional wavelet transform) while preserving the usual properties of perfect reconstruction and computational efficiency. An application to texture synthesis is presented.  1. INTRODUCTION  Although the Discrete Wavelet Transform (DWT) in its maximally decimated form (Mallat's dyadic filter tree [1]) has established an impressive reputation as a tool for image compression, its use for other signal analysis and reconstruction tasks has been hampered by two main disadvantages:  ffl Lack of shift invariance, which means that small shifts in the input...},
  timestamp = {2016-02-24T18:41:31Z},
  booktitle = {1998 8th {{International Conference}} on {{Digital Signal Processing}} ({{DSP}})},
  author = {Kingsbury, Nick},
  month = aug,
  year = {1998},
  note = {00586},
  pages = {319--322},
  file = {Kingsbury_1998_The Dual-Tree Complex Wavelet Transform.pdf:/Users/fergalcotter/Dropbox/Papers/Kingsbury_1998_The Dual-Tree Complex Wavelet Transform.pdf:application/pdf;Citeseer - Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/ZXPH2C6I/summary.html:text/html},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@incollection{yaeger_combining_2012,
  series = {Lecture Notes in Computer Science},
  title = {Combining {{Neural Networks}} and {{Context}}-{{Driven Search}} for {{On}}-Line, {{Printed Handwriting Recognition}} in the {{Newton}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {While on-line handwriting recognition is an area of long-standing and ongoing research, the recent emergence of portable, pen-based computers has focused urgent attention on usable, practical solutions. We discuss a combination and improvement of classical methods to produce robust recognition of hand-printed English text, for a recognizer shipping in new models of Apple Computer's Newton MessagePad\textregistered{} and eMate\textregistered. Combining an artificial neural network (ANN), as a character classifier, with a context-driven search over segmentation and word recognition hypotheses provides an effective recognition system. Long-standing issues relative to training, generalization, segmentation, models of context, probabilistic formalisms, etc., need to be resolved, however, to get excellent performance. We present a number of recent innovations in the application of ANNs as character classifiers for word recognition, including integrated multiple representations, normalized output error, negative training, stroke warping, frequency balancing, error emphasis, and quantized weights. User-adaptation and extension to cursive recognition pose continuing challenges.},
  language = {en},
  timestamp = {2016-08-09T23:21:50Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Yaeger, Larry S. and Webb, Brandyn J. and Lyon, Richard F.},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00000},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {271--293},
  file = {Yaeger et al_2012_Combining Neural Networks and Context-Driven Search for On-line, Printed.pdf:/Users/fergalcotter/Dropbox/Papers/Yaeger et al_2012_Combining Neural Networks and Context-Driven Search for On-line, Printed.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/IZ5CECMA/978-3-642-35289-8_18.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_18}
}

@article{shuman_emerging_2013,
  title = {The Emerging Field of Signal Processing on Graphs: {{Extending}} High-Dimensional Data Analysis to Networks and Other Irregular Domains},
  volume = {30},
  issn = {1053-5888},
  shorttitle = {The Emerging Field of Signal Processing on Graphs},
  doi = {10.1109/MSP.2012.2235192},
  abstract = {In applications such as social, energy, transportation, sensor, and neuronal networks, high-dimensional data naturally reside on the vertices of weighted graphs. The emerging field of signal processing on graphs merges algebraic and spectral graph theoretic concepts with computational harmonic analysis to process such signals on graphs. In this tutorial overview, we outline the main challenges of the area, discuss different ways to define graph spectral domains, which are the analogs to the classical frequency domain, and highlight the importance of incorporating the irregular structures of graph data domains when processing signals on graphs. We then review methods to generalize fundamental operations such as filtering, translation, modulation, dilation, and downsampling to the graph setting and survey the localized, multiscale transforms that have been proposed to efficiently extract information from high-dimensional data on graphs. We conclude with a brief discussion of open issues and possible extensions.},
  timestamp = {2016-09-30T13:07:25Z},
  number = {3},
  journal = {IEEE Signal Processing Magazine},
  author = {Shuman, D. I. and Narang, S. K. and Frossard, P. and Ortega, A. and Vandergheynst, P.},
  month = may,
  year = {2013},
  note = {00414},
  keywords = {Biological neural networks,classical frequency domain,computational harmonic analysis,data analysis,data structures,Feature extraction,Frequency domain analysis,graph spectral domains,graph theory,Harmonic analysis,high-dimensional data analysis,high-dimensional graph data,information extraction,irregular graph data structures,open issues,signal processing,Spectral analysis,spectral graph theoretic concepts,Tutorials,weighted graphs},
  pages = {83--98},
  file = {Shuman et al_2013_The emerging field of signal processing on graphs.pdf:/Users/fergalcotter/Dropbox/Papers/Shuman et al_2013_The emerging field of signal processing on graphs.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/3FAS2I8W/6494675.html:text/html},
  groups = {Graphs,Graphs}
}

@article{bruna_invariant_2013,
  title = {Invariant {{Scattering Convolution Networks}}},
  volume = {35},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2012.230},
  abstract = {A wavelet scattering network computes a translation invariant image representation which is stable to deformations and preserves high-frequency information for classification. It cascades wavelet transform convolutions with nonlinear modulus and averaging operators. The first network layer outputs SIFT-type descriptors, whereas the next layers provide complementary invariant information that improves classification. The mathematical analysis of wavelet scattering networks explains important properties of deep convolution networks for classification. A scattering representation of stationary processes incorporates higher order moments and can thus discriminate textures having the same Fourier power spectrum. State-of-the-art classification results are obtained for handwritten digits and texture discrimination, with a Gaussian kernel SVM and a generative PCA classifier.},
  timestamp = {2016-01-14T14:02:05Z},
  number = {8},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Bruna, J. and Mallat, S.},
  month = aug,
  year = {2013},
  note = {00150},
  keywords = {averaging operators,Classification,complementary invariant information,Computer architecture,Convolution,convolution networks,deep convolution networks,deformations,Fourier power spectrum,Fourier transforms,Gaussian kernel SVM,Gaussian processes,generative PCA classifier,handwritten character recognition,handwritten digits,high-frequency information,image classification,Image representation,image texture,invariants,invariant scattering convolution networks,Key Paper,mathematical analysis,network layer,nonlinear modulus,principal component analysis,Scattering,scattering representation,SIFT-type descriptors,Similar Work,state-of-the-art classification,stationary process,support vector machines,texture discrimination,translation invariant image representation,Wavelet coefficients,wavelets,wavelet scattering network,wavelet transform convolutions,wavelet transforms},
  pages = {1872--1886},
  file = {Bruna_Mallat_2013_Invariant Scattering Convolution Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Bruna_Mallat_2013_Invariant Scattering Convolution Networks.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/RCVIVNU4/articleDetails.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted,mlsp cites}
}

@inproceedings{lecun_convolutional_2010,
  title = {Convolutional Networks and Applications in Vision},
  doi = {10.1109/ISCAS.2010.5537907},
  abstract = {Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or "features")? which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologically-inspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some nonlinearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples. Applications to visual object recognition and vision navigation for off-road mobile robots are described.},
  timestamp = {2016-01-14T14:02:06Z},
  booktitle = {Proceedings of 2010 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {LeCun, Y. and Kavukcuoglu, K. and Farabet, C.},
  month = may,
  year = {2010},
  note = {00261},
  keywords = {_tablet,biologically-inspired architecture,ConvNets,convolutional networks,feature pooling layers,filter bank,intelligent tasks,internal representations,Key Paper,labeled training samples,Learning systems,machine learning,mobile robots,multilevel hierarchies,Navigation,object recognition,off-road mobile robots,Optical character recognition software,robot vision,Unread,unsupervised learning,Video surveillance,vision navigation,visual object recognition,Visual perception},
  pages = {253--256},
  file = {LeCun et al_2010_Convolutional networks and applications in vision.pdf:/Users/fergalcotter/Dropbox/Papers/LeCun et al_2010_Convolutional networks and applications in vision.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/JM4MPRJC/abs_all.html:text/html},
  groups = {CNNs,Generic Design,CNNs,Generic Design,Generic Design}
}

@article{lake_human-level_2015,
  title = {Human-Level Concept Learning through Probabilistic Program Induction},
  volume = {350},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab3050},
  abstract = {Handwritten characters drawn by a model
Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look ``right'' as judged by Turing-like tests of the model's output in comparison to what real humans produce.
Science, this issue p. 1332
People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms\textemdash{}for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several ``visual Turing tests'' probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.
Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model.
Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model.},
  language = {en},
  timestamp = {2016-03-02T00:10:23Z},
  number = {6266},
  urldate = {2016-02-25},
  journal = {Science},
  author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
  month = dec,
  year = {2015},
  note = {00006},
  keywords = {Read Now},
  pages = {1332--1338},
  file = {Lake et al_2015_Human-level concept learning through probabilistic program induction.pdf:/Users/fergalcotter/Dropbox/Papers/Lake et al_2015_Human-level concept learning through probabilistic program induction.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/J46EU8T2/1332.html:text/html},
  groups = {Bayesian Models,Bayesian Models,Bayesian Models},
  pmid = {26659050}
}

@article{porat_localized_1989,
  title = {Localized Texture Processing in Vision: Analysis and Synthesis in the {{Gaborian}} Space},
  volume = {36},
  issn = {0018-9294},
  shorttitle = {Localized Texture Processing in Vision},
  doi = {10.1109/10.16457},
  abstract = {Recent studies of cortical simple cell function suggest that the primitives of image representation in vision have a wavelet form similar to Gabor elementary functions (EFs). It is shown that textures and fully textured images can be practically decomposed into, and synthesized from, a finite set of EFs. Textured-images can be synthesized from a set of EFs using an image coefficient library. Alternatively, texturing of contoured (cartoonlike) images is analogous to adding chromaticity information to contoured images. A method for texture discrimination and image segmentation using local features based on the Gabor approach is introduced. Features related to the EF's parameters provide efficient means for texture discrimination and classification. This method is invariant under rotation and translation. The performance of the classification appears to be robust with respect to noisy conditions. The results show the insensitivity of the discrimination to relatively high noise levels, comparable to the performances of the human observer.$<$$>$},
  timestamp = {2016-07-31T17:00:59Z},
  number = {1},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Porat, M. and Zeevi, Y. Y.},
  month = jan,
  year = {1989},
  note = {00223},
  keywords = {Artificial Intelligence,Bandwidth,cartoonlike images,chromaticity information,Computer Simulation,contoured images,cortical simple cell function,Depth Perception,Frequency,Gaborian space,Humans,image coefficient library,Image edge detection,Image Processing; Computer-Assisted,Image representation,image representation primitives,image segmentation,Image texture analysis,Libraries,localized texture processing,Models; Neurological,Noise level,noise robustness,noisy conditions,texture discrimination,vision,visual analysis,Visual perception,visual processing,wavelet form},
  pages = {115--129},
  file = {Porat_Zeevi_1989_Localized texture processing in vision.pdf:/Users/fergalcotter/Dropbox/Papers/Porat_Zeevi_1989_Localized texture processing in vision.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/2EMBC5J6/articleDetails.html:text/html},
  groups = {Background,Background,Background}
}

@article{serre_robust_2007,
  title = {Robust {{Object Recognition}} with {{Cortex}}-{{Like Mechanisms}}},
  volume = {29},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2007.56},
  abstract = {We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex},
  timestamp = {2016-01-25T15:41:21Z},
  number = {3},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Serre, T. and Wolf, L. and Bileschi, S. and Riesenhuber, M. and Poggio, T.},
  month = mar,
  year = {2007},
  note = {01215},
  keywords = {Algorithms,Artificial Intelligence,Biomimetics,Brain modeling,complex visual scenes,Computer Simulation,computer vision,cortex-like mechanisms,Face detection,Gabor filters,Humans,Image Enhancement,Image Interpretation; Computer-Assisted,image matching,Layout,model,Models; Biological,multiclass categorization,neural network.,Neuroscience,object recognition,Pattern Recognition; Automated,Pattern Recognition; Visual,Reproducibility of Results,Robustness,robust object recognition,scene understanding,Sensitivity and Specificity,Streaming media,template matching,Unread,visual cortex},
  pages = {411--426},
  file = {Serre et al_2007_Robust Object Recognition with Cortex-Like Mechanisms.pdf:/Users/fergalcotter/Dropbox/Papers/Serre et al_2007_Robust Object Recognition with Cortex-Like Mechanisms.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/UB4KQ2DT/abs_all.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@article{lecun_deep_2015,
  title = {Deep Learning},
  volume = {521},
  copyright = {\textcopyright{} 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {0028-0836},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  language = {en},
  timestamp = {2016-01-14T14:02:08Z},
  number = {7553},
  urldate = {2015-11-19},
  journal = {Nature},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  month = may,
  year = {2015},
  note = {00049},
  keywords = {Computer science,Key Paper,Mathematics and computing},
  pages = {436--444},
  file = {Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/EDFAJTE5/nature14539.html:text/html},
  groups = {Other Networks,Other Networks}
}

@article{hinton_fast_2006,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  volume = {18},
  issn = {0899-7667},
  doi = {10.1162/neco.2006.18.7.1527},
  abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
  timestamp = {2016-07-29T14:44:15Z},
  number = {7},
  urldate = {2016-07-28},
  journal = {Neural Comput.},
  author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  month = jul,
  year = {2006},
  note = {04328},
  pages = {1527--1554},
  file = {Hinton et al_2006_A Fast Learning Algorithm for Deep Belief Nets.pdf:/Users/fergalcotter/Dropbox/Papers/Hinton et al_2006_A Fast Learning Algorithm for Deep Belief Nets.pdf:application/pdf},
  groups = {Unsupervised Methods,Unsupervised Methods}
}

@article{porter_robust_1997,
  title = {Robust Rotation-Invariant Texture Classification: Wavelet, {{Gabor}} Filter and {{GMRF}} Based Schemes},
  volume = {144},
  issn = {1350-245X},
  shorttitle = {Robust Rotation-Invariant Texture Classification},
  doi = {10.1049/ip-vis:19971182},
  abstract = {Three novel feature extraction schemes for texture classification are proposed. The schemes employ the wavelet transform, a circularly symmetric Gabor filter or a Gaussian Markov random field with a circular neighbour set to achieve rotation-invariant texture classification. The schemes are shown to give a high level of classification accuracy compared to most existing schemes, using both fewer features (four) and a smaller area of analysis (16\texttimes{}16). Furthermore, unlike most existing schemes, the proposed schemes are shown to be rotation invariant demonstrate a high level of robustness noise. The performances of the three schemes are compared, indicating that the wavelet-based approach is the most accurate, exhibits the best noise performance and has the lowest computational complexity},
  timestamp = {2016-07-31T16:57:43Z},
  number = {3},
  journal = {IEE Proceedings - Vision, Image and Signal Processing},
  author = {Porter, R. and Canagarajah, N.},
  month = jun,
  year = {1997},
  note = {00210},
  keywords = {circularly symmetric Gabor filter,circular neighbour set,classification accuracy,computational complexity,Feature extraction,filtering theory,Gaussian Markov random field,Gaussian processes,GMRF,image classification,image texture,Markov processes,noise,noise performance,noise robustness,random processes,robust rotation invariant texture classification,wavelet transform,wavelet transforms},
  pages = {180--188},
  file = {Porter_Canagarajah_1997_Robust rotation-invariant texture classification.pdf:/Users/fergalcotter/Dropbox/Papers/Porter_Canagarajah_1997_Robust rotation-invariant texture classification.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/ASMUBX96/articleDetails.html:text/html},
  groups = {Background,Background,Background}
}

@article{pickering_object_2011,
  title = {Object Search Using Wavelet-Based Polar Matching for Aerial Imagery},
  timestamp = {2015-11-19T14:13:25Z},
  urldate = {2015-11-03},
  author = {Pickering, Andy and Kingsbury, Nick},
  year = {2011},
  note = {00000},
  file = {Pickering_Kingsbury_2011_Object search using wavelet-based polar matching for aerial imagery.pdf:/Users/fergalcotter/Dropbox/Papers/Pickering_Kingsbury_2011_Object search using wavelet-based polar matching for aerial imagery.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@article{simonyan_deep_2014,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  timestamp = {2016-01-20T12:28:11Z},
  urldate = {2015-12-01},
  journal = {Proceedings of the International Conference on Learning Representations (ICLR)},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2014},
  note = {00092},
  keywords = {_tablet,Computer Science - Computer Vision and Pattern Recognition,Unread},
  file = {Simonyan et al_2014_Deep Inside Convolutional Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Simonyan et al_2014_Deep Inside Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/7GECRCNB/1312.html:text/html},
  groups = {Cords Papers,Visualization \& Generative,Cords Papers,Visualization \& Generative,Visualization \& Generative}
}

@incollection{larsen_adaptive_2012,
  series = {Lecture Notes in Computer Science},
  title = {Adaptive {{Regularization}} in {{Neural Network Modeling}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {In this paper we address the important problem of optimizing regularization parameters in neural network modeling. The suggested optimization scheme is an extended version of the recently presented algorithm [25]. The idea is to minimize an empirical estimate - like the cross-validation estimate - of the generalization error with respect to regularization parameters. This is done by employing a simple iterative gradient descent scheme using virtually no additional programming overhead compared to standard training. Experiments with feed-forward neural network models for time series prediction and classification tasks showed the viability and robustness of the algorithm. Moreover, we provided some simple theoretical examples in order to illustrate the potential and limitations of the proposed regularization framework.},
  language = {en},
  timestamp = {2016-08-09T23:23:34Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Larsen, Jan and Svarer, Claus and Andersen, Lars Nonboe and Hansen, Lars Kai},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00056},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {111--130},
  file = {Larsen et al_2012_Adaptive Regularization in Neural Network Modeling.pdf:/Users/fergalcotter/Dropbox/Papers/Larsen et al_2012_Adaptive Regularization in Neural Network Modeling.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/EBC28MWN/978-3-642-35289-8_8.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_8}
}

@article{autor_why_2015,
  title = {Why {{Are There Still So Many Jobs}}? {{The History}} and {{Future}} of {{Workplace Automation}}},
  volume = {29},
  issn = {0895-3309},
  shorttitle = {Why {{Are There Still So Many Jobs}}?},
  doi = {10.1257/jep.29.3.3},
  abstract = {In this essay, I begin by identifying the reasons that automation has not wiped out a majority of jobs over the decades and centuries. 
Automation does indeed substitute for labor\textemdash{}as it is typically intended to do. 
However, automation also complements labor, raises output in ways that leads to higher demand for labor, and interacts with adjustments in labor supply. 
Journalists and even expert commentators tend to overstate the extent of machine substitution for human labor and ignore the strong complementarities between automation and labor that increase productivity, raise earnings, and augment demand for labor. 
Changes in technology do alter the types of jobs available and what those jobs pay. 
In the last few decades, one noticeable change has been a "polarization" of the labor market, in which wage gains went disproportionately to those at the top and at the bottom of the income and skill distribution, not to those in the middle; however, I also argue, this polarization is unlikely to continue very far into future. 
The final section of this paper reflects on how recent and future advances in artificial intelligence and robotics should shape our thinking about the likely trajectory of occupational change and employment growth. 
I argue that the interplay between machine and human comparative advantage allows computers to substitute for workers in performing routine, codifiable tasks while amplifying the comparative advantage of workers in supplying problem-solving skills, adaptability, and creativity.},
  timestamp = {2016-07-20T23:56:47Z},
  number = {3},
  urldate = {2016-07-20},
  journal = {Journal of Economic Perspectives},
  author = {Autor, David H.},
  month = aug,
  year = {2015},
  note = {00000},
  pages = {3--30},
  file = {Autor_2015_Why Are There Still So Many Jobs.pdf:/Users/fergalcotter/Dropbox/Papers/Autor_2015_Why Are There Still So Many Jobs.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/VAATQBII/articles.html:text/html},
  groups = {ICVSS,ICVSS}
}

@article{bovik_multichannel_1990,
  title = {Multichannel Texture Analysis Using Localized Spatial Filters},
  volume = {12},
  issn = {0162-8828},
  doi = {10.1109/34.41384},
  abstract = {A computational approach for analyzing visible textures is described. Textures are modeled as irradiance patterns containing a limited range of spatial frequencies, where mutually distinct textures differ significantly in their dominant characterizing frequencies. By encoding images into multiple narrow spatial frequency and orientation channels, the slowly varying channel envelopes (amplitude and phase) are used to segregate textural regions of different spatial frequency, orientation, or phase characteristics. Thus, an interpretation of image texture as a region code, or carrier of region information, is emphasized. The channel filters used, known as the two-dimensional Gabor functions, are useful for these purposes in several senses: they have tunable orientation and radial frequency bandwidths and tunable center frequencies, and they optimally achieve joint resolution in space and in spatial frequency. By comparing the channel amplitude responses, one can detect boundaries between textures. Locating large variations in the channel phase responses allows discontinuities in the texture phase to be detected. Examples are given of both types of texture processing using a variety of real and synthetic textures},
  timestamp = {2016-07-31T17:05:01Z},
  number = {1},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Bovik, A. C. and Clark, M. and Geisler, W. S.},
  month = jan,
  year = {1990},
  note = {01720},
  keywords = {channel amplitude responses,Demodulation,discontinuities,Encoding,filtering and prediction theory,Frequency,Image analysis,image segmentation,image texture,Image texture analysis,irradiance patterns,joint resolution,Layout,localized spatial filters,multichannel texture analysis,pattern recognition,radial frequency bandwidths,region code,region information,Shape,Spatial filters,Surface texture,tunable center frequencies,tunable orientation,two-dimensional Gabor functions,visible textures},
  pages = {55--73},
  file = {Bovik et al_1990_Multichannel texture analysis using localized spatial filters.pdf:/Users/fergalcotter/Dropbox/Papers/Bovik et al_1990_Multichannel texture analysis using localized spatial filters.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/U83GQ6MT/abs_all.html:text/html},
  groups = {Background,Background,Background}
}

@inproceedings{glorot_understanding_2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include},
  timestamp = {2016-08-09T15:49:03Z},
  booktitle = {In {{Proceedings}} of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}}'10). {{Society}} for {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  note = {00649},
  annote = {pre alexnet look at initialization and activations},
  file = {Glorot_Bengio_2010_Understanding the difficulty of training deep feedforward neural networks.pdf:/Users/fergalcotter/Dropbox/Papers/Glorot_Bengio_2010_Understanding the difficulty of training deep feedforward neural networks.pdf:application/pdf;Citeseer - Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/TQFGSP6G/summary.html:text/html},
  groups = {Generic Design,Generic Design,Generic Design}
}

@incollection{mairal_convolutional_2014,
  title = {Convolutional {{Kernel Networks}}},
  timestamp = {2016-02-01T17:27:56Z},
  urldate = {2016-02-01},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  publisher = {{Curran Associates, Inc.}},
  author = {Mairal, Julien and Koniusz, Piotr and Harchaoui, Zaid and Schmid, Cordelia},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  note = {00030},
  keywords = {Unread},
  pages = {2627--2635},
  file = {Mairal et al_2014_Convolutional Kernel Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Mairal et al_2014_Convolutional Kernel Networks.pdf:application/pdf;NIPS Snapshort:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/6R3EB42S/5348-convolutional-kernel-networks.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@article{hyeonwoo_noh_learning_????,
  title = {Learning {{Deconvolution Network}} for {{Semantic Segmentation}}},
  timestamp = {2016-07-04T03:54:52Z},
  author = {{Hyeonwoo Noh} and {Seunghoon Hong} and {Bohyung Han}},
  note = {00073},
  groups = {Visualization \& Generative,Visualization \& Generative,Visualization \& Generative}
}

@inproceedings{kingsbury_rotation-invariant_2006,
  title = {Rotation-Invariant Local Feature Matching with Complex Wavelets},
  timestamp = {2016-01-14T14:02:10Z},
  urldate = {2015-11-03},
  booktitle = {Proc. {{European Conference}} on {{Signal Processing}} ({{EUSIPCO}})},
  author = {Kingsbury, Nick},
  year = {2006},
  note = {00052},
  keywords = {Useful},
  pages = {901--904},
  annote = {Can show interpolation of angles between the set ones, and also talks about making the frequency components all the same.
~},
  file = {Kingsbury_2006_Rotation-invariant local feature matching with complex wavelets.pdf:/Users/fergalcotter/Dropbox/Papers/Kingsbury_2006_Rotation-invariant local feature matching with complex wavelets.pdf:application/pdf},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@misc{vedaldi_matconvnet_2016,
  title = {{{MatConvNet}}},
  abstract = {MatConvNet is an implementation of Convolutional Neural Networks (CNNs)
for MATLAB. The toolbox is designed with an emphasis on simplicity and flexibility.
It exposes the building blocks of CNNs as easy-to-use MATLAB functions, providing
routines for computing linear convolutions with filter banks, feature pooling, and many
more. In this manner, MatConvNet allows fast prototyping of new CNN architectures;
at the same time, it supports efficient computation on CPU and GPU allowing
to train complex models on large datasets such as ImageNet ILSVRC. This document
provides an overview of CNNs and how they are implemented in MatConvNet and
gives the technical details of each computational block in the toolbox.},
  timestamp = {2016-08-15T00:48:52Z},
  author = {Vedaldi, Andrea and Lenc, Karel and Gupta, Ankush},
  month = may,
  year = {2016},
  note = {00253},
  file = {Andrea Vedaldi et al_2016_MatConvNet.pdf:/Users/fergalcotter/Dropbox/Papers/Andrea Vedaldi et al_2016_MatConvNet.pdf:application/pdf},
  groups = {Software}
}

@incollection{orr_speeding_2012,
  series = {Lecture Notes in Computer Science},
  title = {Speeding {{Learning}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {There are those who argue that developing fast algorithms is no longer necessary because computers have become so fast. However, we believe that the complexity of our algorithms and the size of our problems will always expand to consume all cycles available, regardless of the speed of ourmachines.Thus, there will never come a time when computational efficiency can or should be ignored. Besides, in the quest to find solutions faster, we also often find better and more stable solutions as well. This section is devoted to techniques for making the learning process in backpropagation (BP) faster and more efficient. It contains a single chapter based on a workshop by Leon Bottou and Yann LeCun. While many alternative learning systems have emerged since the time BP was first introduced, BP is still the most widely used learning algorithm.The reason for this is its simplicity, efficiency, and its general effectiveness on a wide range of problems. Even so, there are many pitfalls in applying it, which is where all these tricks enter.},
  language = {en},
  timestamp = {2016-08-09T23:23:31Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00000},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {7--8},
  file = {Orr_Müller_2012_Speeding Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Orr_Müller_2012_Speeding Learning.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/7ARKTK9R/978-3-642-35289-8_2.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_2}
}

@incollection{prechelt_early_2012,
  series = {Lecture Notes in Computer Science},
  title = {Early {{Stopping}} \textemdash{} {{But When}}?},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (``early stopping''). The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeoff between training time and generalization: From the given mix of 1296 training runs using different 12 problems and 24 different network architectures I conclude slower stopping criteria allow for small improvements in generalization (here: about 4\% on average), but cost much more training time (here: about factor 4 longer on average).},
  language = {en},
  timestamp = {2016-08-09T23:23:29Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Prechelt, Lutz},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00206},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {53--67},
  file = {Prechelt_2012_Early Stopping — But When.pdf:/Users/fergalcotter/Dropbox/Papers/Prechelt_2012_Early Stopping — But When.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/WKZI47V7/978-3-642-35289-8_5.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_5}
}

@article{russakovsky_imagenet_2015,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  volume = {115},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-015-0816-y},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
  language = {en},
  timestamp = {2016-09-13T11:32:59Z},
  number = {3},
  urldate = {2016-08-13},
  journal = {International Journal of Computer Vision},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  month = apr,
  year = {2015},
  note = {01188},
  pages = {211--252},
  file = {Russakovsky et al_2015_ImageNet Large Scale Visual Recognition Challenge.pdf:/Users/fergalcotter/Dropbox/Papers/Russakovsky et al_2015_ImageNet Large Scale Visual Recognition Challenge.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/IXN6Q53D/s11263-015-0816-y.html:text/html},
  groups = {datasets,datasets,datasets}
}

@article{carandini_linearity_1997,
  title = {Linearity and {{Normalization}} in {{Simple Cells}} of the {{Macaque Primary Visual Cortex}}},
  volume = {17},
  issn = {0270-6474, 1529-2401},
  abstract = {Simple cells in the primary visual cortex often appear to compute a weighted sum of the light intensity distribution of the visual stimuli that fall on their receptive fields. A linear model of these cells has the advantage of simplicity and captures a number of basic aspects of cell function. It, however, fails to account for important response nonlinearities, such as the decrease in response gain and latency observed at high contrasts and the effects of masking by stimuli that fail to elicit responses when presented alone. To account for these nonlinearities we have proposed a normalization model, which extends the linear model to include mutual shunting inhibition among a large number of cortical cells. Shunting inhibition is divisive, and its effect in the model is to normalize the linear responses by a measure of stimulus energy. To test this model we performed extracellular recordings of simple cells in the primary visual cortex of anesthetized macaques. We presented large stimulus sets consisting of (1) drifting gratings of various orientations and spatiotemporal frequencies; (2) plaids composed of two drifting gratings; and (3) gratings masked by full-screen spatiotemporal white noise. We derived expressions for the model predictions and fitted them to the physiological data. Our results support the normalization model, which accounts for both the linear and the nonlinear properties of the cells. An alternative model, in which the linear responses are subject to a compressive nonlinearity, did not perform nearly as well.},
  language = {en},
  timestamp = {2016-07-27T13:15:31Z},
  number = {21},
  urldate = {2016-07-27},
  journal = {The Journal of Neuroscience},
  author = {Carandini, Matteo and Heeger, David J. and Movshon, J. Anthony},
  month = jan,
  year = {1997},
  note = {00741},
  keywords = {contrast,gain control,masking,noise,nonlinearity,normalization,visual cortex},
  pages = {8621--8644},
  file = {Carandini et al_1997_Linearity and Normalization in Simple Cells of the Macaque Primary Visual Cortex.pdf:/Users/fergalcotter/Dropbox/Papers/Carandini et al_1997_Linearity and Normalization in Simple Cells of the Macaque Primary Visual Cortex.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/GI5ZS2GS/8621.html:text/html},
  groups = {Biological Vision,Biological Vision},
  pmid = {9334433}
}

@article{grun_taxonomy_2016,
  title = {A {{Taxonomy}} and {{Library}} for {{Visualizing Learned Features}} in {{Convolutional Neural Networks}}},
  abstract = {Over the last decade, Convolutional Neural Networks (CNN) saw a tremendous surge in performance. However, understanding what a network has learned still proves to be a challenging task. To remedy this unsatisfactory situation, a number of groups have recently proposed different methods to visualize the learned models. In this work we suggest a general taxonomy to classify and compare these methods, subdividing the literature into three main categories and providing researchers with a terminology to base their works on. Furthermore, we introduce the FeatureVis library for MatConvNet: an extendable, easy to use open source library for visualizing CNNs. It contains implementations from each of the three main classes of visualization methods and serves as a useful tool for an enhanced understanding of the features learned by intermediate layers, as well as for the analysis of why a network might fail for certain examples.},
  timestamp = {2016-08-12T02:11:46Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.07757},
  primaryClass = {cs},
  urldate = {2016-07-27},
  journal = {arXiv:1606.07757 [cs]},
  author = {Grun, Felix and Rupprecht, Christian and Navab, Nassir and Tombari, Federico},
  month = jun,
  year = {2016},
  note = {00000},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Grün et al_2016_A Taxonomy and Library for Visualizing Learned Features in Convolutional Neural.pdf:/Users/fergalcotter/Dropbox/Papers/Grün et al_2016_A Taxonomy and Library for Visualizing Learned Features in Convolutional Neural.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/ABMFD94A/1606.html:text/html},
  groups = {ICML Viz,Visualization \& Generative,ICML Viz,Visualization \& Generative,Visualization \& Generative}
}

@article{lecun_tutorial_2006,
  title = {A {{Tutorial}} on {{Energy Based Learning}}},
  timestamp = {2016-09-16T19:06:53Z},
  urldate = {2016-05-06},
  journal = {Predicting Structured Data},
  author = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc' Aurelio and Huang, Fu Jie},
  month = aug,
  year = {2006},
  note = {00206},
  file = {LeCun et al_2006_A Tutorial on Energy Based Learning.pdf:/Users/fergalcotter/Dropbox/Papers/LeCun et al_2006_A Tutorial on Energy Based Learning.pdf:application/pdf;lecun-06.html:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/N6SCVTS8/lecun-06.html:text/html},
  groups = {Unsupervised Methods,Unsupervised Methods}
}

@article{tang_deep_2013,
  title = {Deep {{Learning}} Using {{Linear Support Vector Machines}}},
  abstract = {Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these "deep learning" models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.},
  timestamp = {2016-05-05T17:53:04Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1306.0239},
  primaryClass = {cs, stat},
  urldate = {2016-05-05},
  journal = {arXiv:1306.0239 [cs, stat]},
  author = {Tang, Yichuan},
  month = jun,
  year = {2013},
  note = {00065},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  annote = {Comment: Contribution to the ICML 2013 Challenges in Representation Learning Workshop},
  file = {arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/HC3BT85G/1306.html:text/html},
  groups = {CNNs,CNNs}
}

@misc{krizhevsky_cuda_2014,
  title = {Cuda {{Convnet}}},
  timestamp = {2016-08-10T01:22:35Z},
  author = {Krizhevsky, Alex},
  year = {2014},
  note = {00006},
  groups = {Software}
}

@article{mallat_group_2012,
  title = {Group {{Invariant Scattering}}},
  volume = {65},
  copyright = {Copyright \textcopyright{} 2012 Wiley Periodicals, Inc.},
  issn = {1097-0312},
  doi = {10.1002/cpa.21413},
  abstract = {This paper constructs translation-invariant operators on
\$$\backslash$font$\backslash$open=msbm10 at 10pt$\backslash$def$\backslash$R\{$\backslash$hbox\{$\backslash$open R\}\}\{$\backslash$bf L\}\^2(\{\{\{$\backslash$R\}\}\}\^d)\$, which are Lipschitz-continuous to the action of diffeomorphisms. A scattering propagator is a path-ordered product of nonlinear and noncommuting operators, each of which computes the modulus of a wavelet transform. A local integration defines a windowed scattering transform, which is proved to be Lipschitz-continuous to the action of C2 diffeomorphisms. As the window size increases, it converges to a wavelet scattering transform that is translation invariant. Scattering coefficients also provide representations of stationary processes. Expected values depend upon high-order moments and can discriminate processes having the same power spectrum. Scattering operators are extended on L2(G), where G is a compact Lie group, and are invariant under the action of G. Combining a scattering on \$$\backslash$font$\backslash$open=msbm10 at 10pt$\backslash$def$\backslash$R\{$\backslash$hbox\{$\backslash$open R\}\}\{$\backslash$bf L\}\^2(\{\{\{$\backslash$R\}\}\}\^d)\$
and on L2(SO(d)) defines a translation- and rotation-invariant scattering on \$$\backslash$font$\backslash$open=msbm10 at 10pt$\backslash$def$\backslash$R\{$\backslash$hbox\{$\backslash$open R\}\}\{$\backslash$bf L\}\^2(\{\{\{$\backslash$R\}\}\}\^d)\$. \textcopyright{} 2012 Wiley Periodicals, Inc.},
  language = {en},
  timestamp = {2016-01-14T14:02:11Z},
  number = {10},
  urldate = {2015-11-30},
  journal = {Communications on Pure and Applied Mathematics},
  author = {Mallat, St{\'e}phane},
  month = oct,
  year = {2012},
  note = {00135},
  keywords = {Key Paper,Similar Work,Unread},
  pages = {1331--1398},
  file = {Mallat - 2012 - Group Invariant Scattering.pdf:/Users/fergalcotter/Dropbox/Papers/Mallat - 2012 - Group Invariant Scattering.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/EXEEC2VT/abstract.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@incollection{rippel_spectral_2015,
  title = {Spectral {{Representations}} for {{Convolutional Neural Networks}}},
  timestamp = {2016-02-01T12:48:26Z},
  urldate = {2016-02-01},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  publisher = {{Curran Associates, Inc.}},
  author = {Rippel, Oren and Snoek, Jasper and Adams, Ryan P},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  note = {00002},
  keywords = {Similar Work},
  pages = {2440--2448},
  file = {Rippel et al_2015_Spectral Representations for Convolutional Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Rippel et al_2015_Spectral Representations for Convolutional Neural Networks.pdf:application/pdf;NIPS Snapshort:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/E4SI9UTI/5649-hardness-of-parameter-estimation-in-graphical-models.html:text/html},
  groups = {Other Networks,Other Networks}
}

@incollection{_practical_????,
  title = {A {{Practical Guide}} to {{Training Restricted Boltzmann Machines}}},
  timestamp = {2016-10-07T11:46:25Z},
  booktitle = {A {{Practical Guide}} to {{Training Restricted Boltzmann Machines}}},
  note = {00627},
  file = {A Practical Guide to Training Restricted Boltzmann Machines.pdf:/Users/fergalcotter/Dropbox/Papers/A Practical Guide to Training Restricted Boltzmann Machines.pdf:application/pdf},
  groups = {Unsupervised Methods,Unsupervised Methods}
}

@article{boscain_anthropomorphic_2010,
  title = {Anthropomorphic Image Reconstruction via Hypoelliptic Diffusion},
  abstract = {In this paper we study a model of geometry of vision due to Petitot, Citti and Sarti. One of the main features of this model is that the primary visual cortex V1 lifts an image from \$R\^2\$ to the bundle of directions of the plane. Neurons are grouped into orientation columns, each of them corresponding to a point of this bundle. In this model a corrupted image is reconstructed by minimizing the energy necessary for the activation of the orientation columns corresponding to regions in which the image is corrupted. The minimization process intrinsically defines an hypoelliptic heat equation on the bundle of directions of the plane. In the original model, directions are considered both with and without orientation, giving rise respectively to a problem on the group of rototranslations of the plane SE(2) or on the projective tangent bundle of the plane \$PTR\^2\$. We provide a mathematical proof of several important facts for this model. We first prove that the model is mathematically consistent only if directions are considered without orientation. We then prove that the convolution of a \$L\^2(R\^2,R)\$ function (e.g. an image) with a 2-D Gaussian is generically a Morse function. This fact is important since the lift of Morse functions to \$PTR\^2\$ is defined on a smooth manifold. We then provide the explicit expression of the hypoelliptic heat kernel on \$PTR\^2\$ in terms of Mathieu functions. Finally, we present the main ideas of an algorithm which allows to perform image reconstruction on real non-academic images. A very interesting point is that this algorithm is massively parallelizable and needs no information on where the image is corrupted.},
  timestamp = {2016-08-03T11:13:44Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1006.3735},
  primaryClass = {math},
  urldate = {2016-08-03},
  journal = {arXiv:1006.3735 [math]},
  author = {Boscain, Ugo and Duplaix, Jean and Gauthier, Jean-Paul and Rossi, Francesco},
  month = jun,
  year = {2010},
  note = {00032},
  keywords = {Mathematics - Analysis of PDEs,Mathematics - Classical Analysis and ODEs,Mathematics - Optimization and Control},
  file = {Boscain et al_2010_Anthropomorphic image reconstruction via hypoelliptic diffusion.pdf:/Users/fergalcotter/Dropbox/Papers/Boscain et al_2010_Anthropomorphic image reconstruction via hypoelliptic diffusion.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/WCWH9EH5/1006.html:text/html},
  groups = {Background,Background,Background}
}

@article{su_-noising_2005,
  title = {De-{{Noising}} of {{ECG}} Signal Using Translation- Invariant Wavelet de-Noising Method with Improved Thresholding},
  volume = {6},
  issn = {1557-170X},
  doi = {10.1109/IEMBS.2005.1615845},
  abstract = {The electrocardiogram (ECG) signal may mix various kinds of noises while gathering and recording. Wavelet thresholding de-noising method based on discrete wavelet transform (DWT) proposed by Donoho et al. is often used in de-noising of ECG signal. According to the defects of Donoho's method in de-noising of ECG signal, this paper proposes an improved thresholding de-noising method based on Donoho's method. The advantage of the improved de-noising method is that it may not only remain the geometrical characteristics of the original ECG signal and keep the amplitudes of various ECG waveforms efficiently, but also suppress impulsive noise to some extent. Furthermore, the traditional wavelet thresholding de-noising method causes Pseudo-Gibbs phenomena in Q and S waves of ECG signal due to DWT. In order to suppress Pseudo-Gibbs phenomena in Q and S waves, a new de-noising method combining above improved thresholding with the translation-invariant wavelet transform is proposed in this paper. The experimental results indicate that the proposed methods in the paper are better than traditional wavelet thresholding de-noising methods in aspects of remaining geometrical characteristics of ECG signal and the signal-to-noise ratio (SNR).},
  language = {eng},
  timestamp = {2016-09-13T11:33:00Z},
  journal = {Conference proceedings: ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference},
  author = {Su, Li and Zhao, Guoliang},
  year = {2005},
  note = {00046},
  pages = {5946--5949},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT},
  pmid = {17281615}
}

@article{srivastava_modeling_2013,
  title = {Modeling {{Documents}} with {{Deep Boltzmann Machines}}},
  abstract = {We introduce a Deep Boltzmann Machine model suitable for modeling and extracting latent semantic representations from a large unstructured collection of documents. We overcome the apparent difficulty of training a DBM with judicious parameter tying. This parameter tying enables an efficient pretraining algorithm and a state initialization scheme that aids inference. The model can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA, Replicated Softmax, and DocNADE models on document retrieval and document classification tasks.},
  timestamp = {2016-01-20T12:28:16Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1309.6865},
  primaryClass = {cs, stat},
  urldate = {2015-11-19},
  journal = {arXiv:1309.6865 [cs, stat]},
  author = {Srivastava, Nitish and Salakhutdinov, Ruslan R. and Hinton, Geoffrey E.},
  month = sep,
  year = {2013},
  note = {00045},
  keywords = {Computer Science - Information Retrieval,Computer Science - Learning,Statistics - Machine Learning},
  annote = {Comment: Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence (UAI2013)},
  file = {Srivastava et al_2013_Modeling Documents with Deep Boltzmann Machines.pdf:/Users/fergalcotter/Dropbox/Papers/Srivastava et al_2013_Modeling Documents with Deep Boltzmann Machines.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/37UD5UEZ/1309.html:text/html},
  groups = {Unsupervised Methods,Unsupervised Methods}
}

@article{kingsbury_image_1999,
  title = {Image Processing with Complex Wavelets},
  volume = {357},
  doi = {10.1098/rsta.1999.0447},
  abstract = {We first review how wavelets may be used for multi-resolution image processing, describing the filter-bank implementation of the discrete wavelet transform (DWT) and how it may be extended via separable filtering for processing images and other multi-dimensional signals. We then show that the condition for inversion of the DWT (perfect reconstruction) forces many commonly used wavelets to be similar in shape, and that this shape produces severe shift dependence (variation of DWT coefficient energy at any given scale with shift of the input signal). It is also shown that separable filtering with the DWT prevents the transform from providing directionally selective filters for diagonal image features. Complex wavelets can provide both shift invariance and good directional selectivity, with only modest increases in signal redundancy and computation load. However, development of a complex wavelet transform (CWT) with perfect reconstruction and good filter characteristics has proved difficult until recently. We now propose the dual-tree CWT as a solution to this problem, yielding a transform with attractive properties for a range of signal and image processing applications, including motion estimation, denoising, texture analysis and synthesis, and object segmentation.},
  timestamp = {2016-07-31T19:44:27Z},
  number = {1760},
  journal = {Philosophical Transactions of the Royal Society a-Mathematical Physical and Engineering Sciences},
  author = {Kingsbury, N.},
  month = sep,
  year = {1999},
  note = {00870},
  pages = {2543--2560},
  annote = {Good introduction to the DT CWT transform and it's properties. Doesn't make the implementation fully clear but does step through from the problems with real valued separable wavelet transforms to complex valued ones.},
  file = {Kingsbury_1999_Image processing with complex wavelets_2.pdf:/Users/fergalcotter/Dropbox/Papers/Kingsbury_1999_Image processing with complex wavelets_2.pdf:application/pdf},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@incollection{anderson_directed_2005,
  title = {Directed {{Visual Attention}} and the {{Dynamic Control}} of {{Information Flow}}},
  isbn = {978-0-12-375731-9},
  language = {en},
  timestamp = {2016-07-29T14:44:16Z},
  urldate = {2016-07-26},
  booktitle = {Neurobiology of {{Attention}}},
  publisher = {{Elsevier}},
  author = {Anderson, Charles H. and Van Essen, David C. and Olshausen, Bruno A.},
  year = {2005},
  note = {00038},
  pages = {11--17},
  file = {Anderson et al_2005_Directed Visual Attention and the Dynamic Control of Information Flow.pdf:/Users/fergalcotter/Dropbox/Papers/Anderson et al_2005_Directed Visual Attention and the Dynamic Control of Information Flow.pdf:application/pdf},
  groups = {Biological Vision,Biological Vision}
}

@book{lichman_uci_2013,
  title = {{{UCI Machine Learning Repository}}},
  timestamp = {2016-09-13T11:33:01Z},
  publisher = {{University of California, Irvine, School of Information and Computer Sciences}},
  author = {Lichman, M.},
  year = {2013},
  note = {01585},
  groups = {datasets,datasets,datasets}
}

@inproceedings{hamari_does_2014,
  title = {Does Gamification Work?\textendash{}a Literature Review of Empirical Studies on Gamification},
  shorttitle = {Does Gamification Work?},
  timestamp = {2016-06-27T23:40:45Z},
  urldate = {2016-06-19},
  booktitle = {2014 47th {{Hawaii International Conference}} on {{System Sciences}}},
  publisher = {{IEEE}},
  author = {Hamari, Juho and Koivisto, Jonna and Sarsa, Harri},
  year = {2014},
  note = {00535},
  pages = {3025--3034},
  file = {Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/5DBNHSW4/abs_all.html:text/html},
  groups = {Cords Papers,Cords Papers}
}

@article{nelson_enhanced_2011,
  title = {Enhanced {{Shift}} and {{Scale Tolerance}} for {{Rotation Invariant Polar Matching With Dual}}-{{Tree Wavelets}}},
  volume = {20},
  issn = {1057-7149},
  doi = {10.1109/TIP.2010.2069711},
  abstract = {Polar matching is a recently developed shift and rotation invariant object detection method that is based upon dual-tree complex wavelet transforms or equivalent multiscale directional filterbanks. It can be used to facilitate both keypoint matching, neighborhood search detection, or detection and tracking with particle filters. The theory is extended here to incorporate an allowance for local spatial and dilation perturbations. With experiments, we demonstrate that the robustness of the polar matching method is strengthened at modest computational cost.},
  timestamp = {2015-11-04T12:40:56Z},
  number = {3},
  journal = {IEEE Transactions on Image Processing},
  author = {Nelson, J.D.B. and Kingsbury, N.G.},
  month = mar,
  year = {2011},
  note = {00007},
  keywords = {channel bank filters,Correlation,dual-tree complex wavelet transforms,dual-tree wavelets,equivalent multiscale directional filterbanks,Feature extraction,keypoint matching,neighborhood search detection,object detection,particle filtering (numerical methods),particle filters,Pixel,Robustness,rotation invariant object detection,rotation invariant polar matching,scale tolerance,shift tolerance,Tin,wavelet transforms},
  pages = {814--821},
  file = {Enhanced shift and scale tolerance for rotation invariant polar matching with dual-tree wavelets.pdf:/Users/fergalcotter/Dropbox/Papers/Enhanced shift and scale tolerance for rotation invariant polar matching with dual-tree wavelets.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/5WIDUWN5/abs_all.html:text/html},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@article{chan_pcanet:_2015,
  title = {{{PCANet}}: {{A Simple Deep Learning Baseline}} for {{Image Classification}}?},
  volume = {24},
  issn = {1057-7149, 1941-0042},
  shorttitle = {{{PCANet}}},
  doi = {10.1109/TIP.2015.2475625},
  abstract = {In this work, we propose a very simple deep learning network for image classification which comprises only the very basic data processing components: cascaded principal component analysis (PCA), binary hashing, and block-wise histograms. In the proposed architecture, PCA is employed to learn multistage filter banks. It is followed by simple binary hashing and block histograms for indexing and pooling. This architecture is thus named as a PCA network (PCANet) and can be designed and learned extremely easily and efficiently. For comparison and better understanding, we also introduce and study two simple variations to the PCANet, namely the RandNet and LDANet. They share the same topology of PCANet but their cascaded filters are either selected randomly or learned from LDA. We have tested these basic networks extensively on many benchmark visual datasets for different tasks, such as LFW for face verification, MultiPIE, Extended Yale B, AR, FERET datasets for face recognition, as well as MNIST for hand-written digits recognition. Surprisingly, for all tasks, such a seemingly naive PCANet model is on par with the state of the art features, either prefixed, highly hand-crafted or carefully learned (by DNNs). Even more surprisingly, it sets new records for many classification tasks in Extended Yale B, AR, FERET datasets, and MNIST variations. Additional experiments on other public datasets also demonstrate the potential of the PCANet serving as a simple but highly competitive baseline for texture classification and object recognition.},
  timestamp = {2016-02-01T17:08:38Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1404.3606},
  number = {12},
  urldate = {2016-02-01},
  journal = {IEEE Transactions on Image Processing},
  author = {Chan, Tsung-Han and Jia, Kui and Gao, Shenghua and Lu, Jiwen and Zeng, Zinan and Ma, Yi},
  month = dec,
  year = {2015},
  note = {00057},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Similar Work,Unread},
  pages = {5017--5032},
  file = {Chan et al_2015_PCANet.pdf:/Users/fergalcotter/Dropbox/Papers/Chan et al_2015_PCANet.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/NB8TX77A/1404.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@incollection{ruderman_statistics_1994,
  title = {Statistics of {{Natural Images}}: {{Scaling}} in the {{Woods}}},
  shorttitle = {Statistics of {{Natural Images}}},
  timestamp = {2016-10-25T17:08:14Z},
  urldate = {2016-10-25},
  booktitle = {Advances in {{Neural Information Processing Systems}} 6},
  publisher = {{Morgan-Kaufmann}},
  author = {Ruderman, Daniel L. and Bialek, William},
  editor = {Cowan, J. D. and Tesauro, G. and Alspector, J.},
  year = {1994},
  pages = {551--558},
  file = {Ruderman_Bialek_1994_Statistics of Natural Images.pdf:/Users/fergalcotter/Dropbox/Papers/Ruderman_Bialek_1994_Statistics of Natural Images.pdf:application/pdf;NIPS Snapshort:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/SHZZ7Q9R/835-statistics-of-natural-images-scaling-in-the-woods.html:text/html},
  groups = {Statistical Methods,Statistical Methods}
}

@misc{kingsbury_scattering_2015,
  address = {University of Adelaide},
  title = {Scattering {{Convolution Networks}} and {{Dual}}-Tree {{Wavelets}}},
  timestamp = {2015-11-04T17:21:13Z},
  author = {Kingsbury, Nick},
  month = feb,
  year = {2015},
  note = {00000},
  keywords = {Unread},
  annote = {Good introduction to the desirable properties of nets. Invariant R(gx) = R(x) vs. Covariant R(gx) = g.R(x).
He talks about why fourier coeffs are bad, and then steps in with wavelets. Wavelet modulus operator.
He then gives a great diagram of what the spectrum of a scatternet looks like, and how to build one.},
  file = {ScatterNetsTalk3.pdf:/Users/fergalcotter/Dropbox/Papers/ScatterNetsTalk3.pdf:application/pdf},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@incollection{horn_large_2012,
  series = {Lecture Notes in Computer Science},
  title = {Large {{Ensemble Averaging}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {Averaging over many predictors leads to a reduction of the variance portion of the error. We present a method for evaluating the mean squared error of an infinite ensemble of predictors from finite (small size) ensemble information. We demonstrate it on ensembles of networks with different initial choices of synaptic weights. We find that the optimal stopping criterion for large ensembles occurs later in training time than for single networks. We test our method on the suspots data set and obtain excellent results.},
  language = {en},
  timestamp = {2016-08-09T23:23:37Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Horn, David and Naftaly, Ury and Intrator, Nathan},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00002},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {131--137},
  file = {Horn et al_2012_Large Ensemble Averaging.pdf:/Users/fergalcotter/Dropbox/Papers/Horn et al_2012_Large Ensemble Averaging.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/HUAIQZZG/978-3-642-35289-8_9.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_9}
}

@techreport{nick_kingsbury_4f8_2015,
  title = {{{4F8 Image Coding Course}}},
  timestamp = {2016-08-07T17:20:52Z},
  institution = {University of Cambridge},
  author = {{Nick Kingsbury}},
  year = {2015},
  note = {00004},
  file = {4F8CODING.pdf:/Users/fergalcotter/Google Drive/Courses/Image Processing/4F8CODING.pdf:application/pdf},
  groups = {Notes,Wavelets and DTCWT,Notes,Wavelets and DTCWT}
}

@inproceedings{krizhevsky_imagenet_2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  timestamp = {2017-09-05T12:13:48Z},
  urldate = {2015-11-29},
  booktitle = {{{NIPS}}},
  publisher = {{Curran Associates, Inc.}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  note = {03570},
  keywords = {Key Paper},
  pages = {1097--1105},
  file = {Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf:application/pdf;NIPS Snapshort:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/MMZSEFVF/4824-imagenet-classification-with-deep-convolutional-neural-networks.html:text/html},
  groups = {state of the art,state of the art,state of the art,mlsp cites}
}

@inproceedings{loo_motion-estimation-based_2001,
  title = {Motion-Estimation-Based Registration of Geometrically Distorted Images for Watermark Recovery},
  doi = {10.1117/12.435445},
  timestamp = {2016-08-07T17:20:53Z},
  urldate = {2016-08-05},
  author = {Loo, Patrick and Kingsbury, Nick G.},
  editor = {Wong, Ping W. and Delp III, Edward J.},
  month = aug,
  year = {2001},
  note = {00035},
  pages = {606--617},
  file = {Loo_Kingsbury_2001_Motion-estimation-based registration of geometrically distorted images for.pdf:/Users/fergalcotter/Dropbox/Papers/Loo_Kingsbury_2001_Motion-estimation-based registration of geometrically distorted images for.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@incollection{simard_transformation_2012,
  series = {Lecture Notes in Computer Science},
  title = {Transformation {{Invariance}} in {{Pattern Recognition}} \textendash{} {{Tangent Distance}} and {{Tangent Propagation}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {In pattern recognition, statistical modeling, or regression, the amount of data is a critical factor affecting the performance. If the amount of data and computational resources are unlimited, even trivial algorithms will converge to the optimal solution. However, in the practical case, given limited data and other resources, satisfactory performance requires sophisticated methods to regularize the problem by introducing a priori knowledge. Invariance of the output with respect to certain transformations of the input is a typical example of such a priori knowledge. In this chapter, we introduce the concept of tangent vectors, which compactly represent the essence of these transformation invariances, and two classes of algorithms, ``tangent distance'' and ``tangent propagation'', which make use of these invariances to improve performance.},
  language = {en},
  timestamp = {2016-08-09T23:23:30Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Simard, Patrice Y. and LeCun, Yann A. and Denker, John S. and Victorri, Bernard},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00279},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {235--269},
  file = {Simard et al_2012_Transformation Invariance in Pattern Recognition – Tangent Distance and Tangent.pdf:/Users/fergalcotter/Dropbox/Papers/Simard et al_2012_Transformation Invariance in Pattern Recognition – Tangent Distance and Tangent.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/SD7IJ9VI/978-3-642-35289-8_17.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_17}
}

@inproceedings{sifre_combined_2012,
  title = {Combined Scattering for Rotation Invariant Texture Analysis},
  timestamp = {2016-07-31T16:07:08Z},
  urldate = {2016-07-31},
  booktitle = {European {{Symposium}} on {{Artificial Neural Networks}} ({{ESANN}}) 2012},
  author = {Sifre, Laurent and Mallat, St{\'e}phane},
  year = {2012},
  note = {00032},
  file = {Sifre_Mallat_2012_Combined scattering for rotation invariant texture analysis.pdf:/Users/fergalcotter/Dropbox/Papers/Sifre_Mallat_2012_Combined scattering for rotation invariant texture analysis.pdf:application/pdf},
  groups = {Background,Background,Background}
}

@article{tompson_efficient_2015,
  title = {Efficient {{Object Localization Using Convolutional Networks}}},
  abstract = {Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC dataset and outperforms all existing approaches on the MPII-human-pose dataset.},
  timestamp = {2016-01-14T14:02:16Z},
  urldate = {2015-12-01},
  journal = {CVPR 2015},
  author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christopher},
  year = {2015},
  note = {00009},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Unread},
  annote = {Comment: 8 pages with 1 page of citations},
  file = {Tompson et al_2014_Efficient Object Localization Using Convolutional Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Tompson et al_2014_Efficient Object Localization Using Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/2BA2I69U/1411.html:text/html},
  groups = {other,other,other}
}

@incollection{hatipoglu_texture_1999,
  title = {Texture Classification Using Dual-Tree Complex Wavelet Transform},
  abstract = {A new texture feature extraction method utilizing dual tree complex wavelet transform (DT-CWT) is introduced. The complex wavelet transform is a recently developed tool that uses a dual tree of wavelet filters to find the real and imaginary parts of complex wavelet coefficients (Ij. Approximate shift invariance, good directional selectivity, computational efficiency properties of DT-CWT make it a good candidate for representing the texture features. In this paper, we propose a method for efficiently using the properties of DT-CWT in finding the directional and spatial/frequency characteristics of the patterns and classifying different texture patterns in terms of these characteristics. Experimental results show that the proposed feature extraction and classification method is efficient in terms of computational speed and retrieval accuracy.},
  timestamp = {2016-01-20T12:29:11Z},
  booktitle = {Seventh {{International Conference}} on {{Image Processing}} and {{Its Applications}}},
  author = {Hatipoglu, S. and Mitra, S. K. and Kingsbury, N.},
  year = {1999},
  note = {00064},
  pages = {344--347},
  file = {Texture classification using dual-tree complex wavelet transform.pdf:/Users/fergalcotter/Dropbox/Papers/Texture classification using dual-tree complex wavelet transform.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@inproceedings{zhu_semi-supervised_2003,
  title = {Semi-{{Supervised Learning Using Gaussian Fields}} and {{Harmonic Functions}}},
  abstract = {An approach to semi-supervised learning is proposed  that is based on a Gaussian random field  model. Labeled and unlabeled data are represented  as vertices in a weighted graph, with  edge weights encoding the similarity between instances. The learning},
  timestamp = {2016-10-25T17:03:20Z},
  booktitle = {In {{Icml}}},
  author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
  year = {2003},
  pages = {912--919},
  file = {Zhu et al_2003_Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions.pdf:/Users/fergalcotter/Dropbox/Papers/Zhu et al_2003_Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions.pdf:application/pdf;Citeseer - Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/GHJJ6GCK/summary.html:text/html},
  groups = {Statistical Methods,Statistical Methods}
}

@book{winkler_image_2003,
  address = {Berlin, Heidelberg},
  title = {Image {{Analysis}}, {{Random Fields}} and {{Markov Chain Monte Carlo Methods}}},
  isbn = {978-3-642-62911-2 978-3-642-55760-6},
  timestamp = {2016-09-30T13:07:26Z},
  urldate = {2016-09-29},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Winkler, Gerhard},
  year = {2003},
  note = {01278},
  groups = {Miscellaneous,Miscellaneous}
}

@article{springenberg_striving_2014,
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  shorttitle = {Striving for {{Simplicity}}},
  abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
  timestamp = {2016-09-30T13:07:27Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6806},
  primaryClass = {cs},
  urldate = {2016-09-26},
  journal = {arXiv:1412.6806 [cs]},
  author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  month = dec,
  year = {2014},
  note = {00109},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: accepted to ICLR-2015 workshop track; no changes other than style},
  file = {Springenberg et al_2014_Striving for Simplicity.pdf:/Users/fergalcotter/Dropbox/Papers/Springenberg et al_2014_Striving for Simplicity_2.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/3Q6Q3V64/1412.html:text/html},
  groups = {Visualization \& Generative,Visualization \& Generative,Visualization \& Generative}
}

@incollection{van_rooyen_learning_2015,
  title = {Learning with {{Symmetric Label Noise}}: {{The Importance}} of {{Being Unhinged}}},
  shorttitle = {Learning with {{Symmetric Label Noise}}},
  timestamp = {2016-01-14T14:02:43Z},
  urldate = {2016-01-14},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  publisher = {{Curran Associates, Inc.}},
  author = {{van Rooyen}, Brendan and Menon, Aditya and Williamson, Robert C},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  note = {00003},
  pages = {10--18},
  file = {NIPS Snapshort:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/WE47WVUD/5941-learning-with-symmetric-label-noise-the-importance-of-being-unhinged.html:text/html},
  groups = {Miscellaneous,Miscellaneous}
}

@misc{sifre_scatnet_2013,
  address = {{\'E}cole normale sup{\'e}rieure},
  title = {{{ScatNet}}},
  timestamp = {2016-08-15T00:47:59Z},
  author = {Sifre, L. and Anden, J.},
  month = nov,
  year = {2013},
  note = {00004},
  file = {L. Sifre_J. Anden_2013_ScatNet.pdf:/Users/fergalcotter/Dropbox/Papers/L. Sifre_J. Anden_2013_ScatNet.pdf:application/pdf},
  groups = {Software}
}

@article{vedaldi_matconvnet_2014,
  title = {{{MatConvNet}} - {{Convolutional Neural Networks}} for {{MATLAB}}},
  abstract = {MatConvNet is an implementation of Convolutional Neural Networks (CNNs) for MATLAB. The toolbox is designed with an emphasis on simplicity and flexibility. It exposes the building blocks of CNNs as easy-to-use MATLAB functions, providing routines for computing linear convolutions with filter banks, feature pooling, and many more. In this manner, MatConvNet allows fast prototyping of new CNN architectures; at the same time, it supports efficient computation on CPU and GPU allowing to train complex models on large datasets such as ImageNet ILSVRC. This document provides an overview of CNNs and how they are implemented in MatConvNet and gives the technical details of each computational block in the toolbox.},
  timestamp = {2016-01-22T01:48:35Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.4564},
  primaryClass = {cs},
  urldate = {2016-01-20},
  journal = {arXiv:1412.4564 [cs]},
  author = {Vedaldi, Andrea and Lenc, Karel},
  month = dec,
  year = {2014},
  note = {00107},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Mathematical Software,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: Updated for release v1.0-beta12},
  file = {matconvnet-manual.pdf:/Users/fergalcotter/Dropbox/Papers/Notes/matconvnet-manual.pdf:application/pdf;Vedaldi_Lenc_2014_MatConvNet - Convolutional Neural Networks for MATLAB.pdf:/Users/fergalcotter/Dropbox/Papers/Notes/Vedaldi_Lenc_2014_MatConvNet - Convolutional Neural Networks for MATLAB.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/3U2WXPVC/1412.html:text/html},
  groups = {Notes,Notes}
}

@article{ren_object_2015,
  title = {Object {{Detection Networks}} on {{Convolutional Feature Maps}}},
  abstract = {Most object detectors contain two important components: a feature extractor and an object classifier. The feature extractor has rapidly evolved with significant research efforts leading to better deep ConvNet architectures. The object classifier, however, has not received much attention and most state-of-the-art systems (like R-CNN) use simple multi-layer perceptrons. This paper demonstrates that carefully designing deep networks for object classification is just as important. We take inspiration from traditional object classifiers, such as DPM, and experiment with deep networks that have part-like filters and reason over latent variables. We discover that on pre-trained convolutional feature maps, even randomly initialized deep classifiers produce excellent results, while the improvement due to fine-tuning is secondary; on HOG features, deep classifiers outperform DPMs and produce the best HOG-only results without external data. We believe these findings provide new insight for developing object detection systems. Our framework, called Networks on Convolutional feature maps (NoC), achieves outstanding results on the PASCAL VOC 2007 (73.3\% mAP) and 2012 (68.8\% mAP) benchmarks.},
  timestamp = {2016-01-20T12:28:03Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1504.06066},
  primaryClass = {cs},
  urldate = {2015-11-29},
  journal = {arXiv:1504.06066 [cs]},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Zhang, Xiangyu and Sun, Jian},
  month = apr,
  year = {2015},
  note = {00008},
  keywords = {_tablet,Computer Science - Computer Vision and Pattern Recognition,Unread},
  annote = {Comment: Technical report},
  file = {Ren et al_2015_Object Detection Networks on Convolutional Feature Maps.pdf:/Users/fergalcotter/Dropbox/Papers/Ren et al_2015_Object Detection Networks on Convolutional Feature Maps.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/7PWKB2V4/1504.html:text/html},
  groups = {other,other,other}
}

@misc{griffin_caltech-256_2007,
  type = {Report or Paper},
  title = {Caltech-256 {{Object Category Dataset}}},
  abstract = {We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.},
  timestamp = {2016-09-13T11:33:02Z},
  urldate = {2016-08-13},
  howpublished = {\url{http://resolver.caltech.edu/CaltechAUTHORS:CNS-TR-2007-001}},
  author = {Griffin, Gregory and Holub, Alex and Perona, Pietro},
  month = mar,
  year = {2007},
  note = {01302},
  file = {Griffin et al_2007_Caltech-256 Object Category Dataset.pdf:/Users/fergalcotter/Dropbox/Papers/Griffin et al_2007_Caltech-256 Object Category Dataset.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/HHHMUH6Z/7694.html:text/html},
  groups = {CNNs,CNNs}
}

@article{ribeiro_why_2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  timestamp = {2016-09-13T11:33:03Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.04938},
  primaryClass = {cs, stat},
  urldate = {2016-08-12},
  journal = {arXiv:1602.04938 [cs, stat]},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  month = feb,
  year = {2016},
  note = {00007},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Statistics - Machine Learning},
  file = {Ribeiro et al_2016_Why Should I Trust You.pdf:/Users/fergalcotter/Dropbox/Papers/Ribeiro et al_2016_Why Should I Trust You.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/BFG66CKC/1602.html:text/html},
  groups = {criticisms,criticisms,criticisms}
}

@phdthesis{yingsong_zhang_sparse_2011,
  address = {Department of Engineering, Signal Processing},
  type = {PhD Thesis},
  title = {Sparse {{Reconstruction Algorithms}} and the {{Application}} in {{Image Processing}}},
  abstract = {This dissertation investigates the sparse-regularized linear inverse problem and its applications in image deconvolution, interpolation and denoising problems. Wavelets provide sparse representations of a wide range of natural images and data. For this reason, we are interested in applying the sparse regularization onto the wavelet coefficients of images, particularly large images and 3D datasets which presents se-
rious challenges to the design of algorithms for signal recovery. The conventional sparse-signal-recovery methods, have been traditionally based on greedy heuristics (e.g. matching-pursuit based methods) or convex relaxation of l0 (l1 minimization). Such methods become very computational expensive when the dimensionality of the problem is large. 

In this dissertation, we proposed two algorithms to perform the sparse-signal recovery efficiently and accurately on large images and 3D datasets. SAIWave is specially designed for deconvolution. It is a Subband-Adaptive and generalized version of the popular Iterative thresholding algorithm that takes different update steps and thresholds for each subband, which is shown to accelerate the convergence. The SAIWave algorithm runs in parallel, updating all of the subbands at the same time. We also give an algorithm for selecting the parameter for each subband that decides the update steps and thresholds. The other al-
gorithm is L0RL2 . L0RL2 is developed for the purpose of general sparse-signal recovery. We introduce a new penalty function, which has some useful geometric properties with regard to the continuation parameter ǫ. These properties are then utilized to develop algorithms (IRLS-ǫ and L0RL2 ). Both algorithms are shown to recover sparse signals with fewer measurements than the conventional methods, while being efficient and accurate. L0RL2 is then combined with the devel-
opment of SAIWave to incorporate typical signal structures (group and tree).

Finally, we consider two image applications of the L0RL2 algorithm. The first one is to recover an image to a higher resolution than the observation (super-resolution). The second proposes a simplified image model to work with the L0RL2 algorithm on image denoising problems. The proposed model adopts a hierarchical structure to describe the multi-scale properties of wavelet coefficients. This is able to model the non-Gaussian features of the wavelet coefficients' marginal distributions. In our experiments, the denoising result based on the proposed model has the visual effect of improving the image sharpness.},
  timestamp = {2016-08-06T14:21:01Z},
  school = {University of Cambridge},
  author = {{Yingsong Zhang}},
  year = {2011},
  note = {00000},
  file = {Yingsong Zhang_2011_Sparse Reconstruction Algorithms and the Application in Image Processing.pdf:/Users/fergalcotter/Dropbox/Papers/Yingsong Zhang_2011_Sparse Reconstruction Algorithms and the Application in Image Processing.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@article{dinh_fast_2016,
  title = {Fast Learning Rates with Heavy-Tailed Losses},
  abstract = {We study fast learning rates when the losses are not necessarily bounded and may have a distribution with heavy tails. To enable such analyses, we introduce two new conditions: (i) the envelope function \$$\backslash$sup\_\{f $\backslash$in $\backslash$mathcal\{F\}\}|$\backslash$ell $\backslash$circ f|\$, where \$$\backslash$ell\$ is the loss function and \$$\backslash$mathcal\{F\}\$ is the hypothesis class, exists and is \$L\^r\$-integrable, and (ii) \$$\backslash$ell\$ satisfies the multi-scale Bernstein's condition on \$$\backslash$mathcal\{F\}\$. Under these assumptions, we prove that learning rate faster than \$O(n\^\{-1/2\})\$ can be obtained and, depending on \$r\$ and the multi-scale Bernstein's powers, can be arbitrarily close to \$O(n\^\{-1\})\$. We then verify these assumptions and derive fast learning rates for the problem of vector quantization by \$k\$-means clustering with heavy-tailed distributions. The analyses enable us to obtain novel learning rates that extend and complement existing results in the literature from both theoretical and practical viewpoints.},
  timestamp = {2016-09-30T15:08:16Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.09481},
  primaryClass = {cs, stat},
  urldate = {2016-09-30},
  journal = {arXiv:1609.09481 [cs, stat]},
  author = {Dinh, Vu and Ho, Lam Si Tung and Nguyen, Duy and Nguyen, Binh T.},
  month = sep,
  year = {2016},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  annote = {Comment: Advances in Neural Information Processing Systems (NIPS 2016): 11 pages},
  file = {Dinh et al_2016_Fast learning rates with heavy-tailed losses.pdf:/Users/fergalcotter/Dropbox/Papers/Dinh et al_2016_Fast learning rates with heavy-tailed losses.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/92V76SG9/1609.html:text/html},
  groups = {Miscellaneous,Miscellaneous}
}

@article{chen_efficient_2012,
  title = {Efficient {{Registration}} of {{Nonrigid}} 3-{{D Bodies}}},
  volume = {21},
  issn = {1057-7149},
  doi = {10.1109/TIP.2011.2160958},
  abstract = {We present a novel method to perform an accurate registration of 3-D nonrigid bodies by using phase-shift properties of the dual-tree complex wavelet transform (DT-BBCWT). Since the phases of DT-BBCWT coefficients change approximately linearly with the amount of feature displacement in the spatial domain, motion can be estimated using the phase information from these coefficients. The motion estimation is performed iteratively: first by using coarser level complex coefficients to determine large motion components and then by employing finer level coefficients to refine the motion field. We use a parametric affine model to describe the motion, where the affine parameters are found locally by substituting into an optical flow model and by solving the resulting overdetermined set of equations. From the estimated affine parameters, the motion field between the sensed and the reference data sets can be generated, and the sensed data set then can be shifted and interpolated spatially to align with the reference datafeature displacement set.},
  timestamp = {2016-05-06T12:38:30Z},
  number = {1},
  journal = {IEEE Transactions on Image Processing},
  author = {Chen, H. and Kingsbury, N.},
  month = jan,
  year = {2012},
  note = {00011},
  keywords = {3-D MRI,affine parameters,affine transforms,Algorithms,biomedical MRI,coarser level complex coefficients,dual-tree complex wavelet transform,Dual-tree complex wavelet transform $(hboxDT-BBChboxWT)$,Equations,feature displacement,Feature extraction,finer level coefficients,Image Enhancement,Image Interpretation; Computer-Assisted,image registration,image sequences,Imaging; Three-Dimensional,Mathematical model,medical image processing,motion estimation,nonrigid 3-D bodies,optical flow,optical flow model,parametric affine model,Pattern Recognition; Automated,phase-shift properties,registration,Reproducibility of Results,Sensitivity and Specificity,Subtraction Technique,wavelet transforms},
  pages = {262--272},
  file = {Chen_Kingsbury_2012_Efficient Registration of Nonrigid 3-D Bodies.pdf:/Users/fergalcotter/Dropbox/Papers/Chen_Kingsbury_2012_Efficient Registration of Nonrigid 3-D Bodies.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/CB3HRJSQ/abs_all.html:text/html},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@article{donahue_decaf:_2013,
  title = {{{DeCAF}}: {{A Deep Convolutional Activation Feature}} for {{Generic Visual Recognition}}},
  shorttitle = {{{DeCAF}}},
  abstract = {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.},
  timestamp = {2016-07-29T14:44:18Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1310.1531},
  primaryClass = {cs},
  urldate = {2016-07-15},
  journal = {arXiv:1310.1531 [cs]},
  author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
  month = oct,
  year = {2013},
  note = {00798},
  keywords = {_tablet,Computer Science - Computer Vision and Pattern Recognition},
  file = {Donahue et al_2013_DeCAF.pdf:/Users/fergalcotter/Dropbox/Papers/Donahue et al_2013_DeCAF.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/UPDHAKFS/1310.html:text/html},
  groups = {Transfer Learning,Transfer Learning,Transfer Learning}
}

@inproceedings{rivaz_complex_1999,
  title = {Complex Wavelet Features for Fast Texture Image Retrieval},
  volume = {1},
  doi = {10.1109/ICIP.1999.821576},
  abstract = {Digital libraries and multimedia databases are being rapidly developed and efficient search algorithms must now be developed. Gabor features have been experimentally shown to be the most accurate but have the disadvantage of slow computation. This paper shows how a new complex wavelet transform can be used to approximate the Gabor features and derives a distance metric based on statistical hypothesis testing that gives a better performance than the usual metric. The new features are experimentally compared with both Gabor and standard wavelet techniques},
  timestamp = {2016-08-05T01:32:15Z},
  booktitle = {1999 {{International Conference}} on {{Image Processing}}, 1999. {{ICIP}} 99. {{Proceedings}}},
  author = {de Rivaz, P. and Kingsbury, N.},
  year = {1999},
  note = {00086},
  keywords = {Bandwidth,complex wavelet features,complex wavelet transform,Data engineering,digital libraries,Discrete wavelet transforms,distance metric,efficient search algorithms,fast texture image retrieval,Frequency,Gabor features,Gabor filters,Humans,image retrieval,image texture,multimedia databases,Software libraries,statistical hypothesis testing,Testing,wavelet transforms},
  pages = {109--113 vol.1},
  file = {Rivaz_Kingsbury_1999_Complex wavelet features for fast texture image retrieval.pdf:/Users/fergalcotter/Dropbox/Papers/Rivaz_Kingsbury_1999_Complex wavelet features for fast texture image retrieval.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/T8G3MK9A/abs_all.html:text/html},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@phdthesis{pierre_garrigues_sparse_2009,
  title = {Sparse {{Coding Models}} of {{Natural Images}}: {{Algorithms}} for {{Efficient Inference}} and {{Learning}} of {{Higher}}-{{Order Structure}} | {{EECS}} at {{UC Berkeley}}},
  abstract = {The concept of sparsity is widely used in the signal processing, machine learning,
and statistics communities for model fitting and solving inverse problems. It is also
important in neuroscience as it is thought to underlie the neural representations used
in the brain. In this thesis, I derive new algorithms for learning higher-order structure
in sparse coding models of images, and I present an improved algorithm for inferring
sparse representations with sequential observations.
It has been shown that adapting a dictionary of basis functions to the statistics
of natural images so as to maximize sparsity in the coefficients results in a set of
dictionary elements whose spatial properties resemble those of primary visual cortex
receptive fields. The operation to compute the sparse coefficients can be implemented
via an `1-penalized least-square problem commonly referred to as Basis Pursuit Denoising
or Lasso. However, the resulting sparse coefficients still exhibit pronounced
statistical dependencies, thus violating the independence assumption of the sparse
coding model. I propose in this thesis two models that attempt to capture the dependencies
among the basis function coefficients. The first model includes a pairwise
coupling term in the prior over the coefficient activity states. When adapted to the statistics of natural images, the coupling terms converge to a solution involving
facilitatory and inhibitory interactions among neighboring basis functions. In the
second model, the prior is a mixture of Laplacian distributions, where the statistical
dependencies among the basis function coefficients are modeled through the scale
parameters. I show that I can leverage the efficient algorithms developed for Basis
Pursuit Denoising to derive improved inference algorithms with the Laplacian scale
mixture prior.
I also propose in this thesis a new algorithm, RecLasso, to solve the Lasso with
online observations. I introduce an optimization problem that allows us to compute an
homotopy from the current solution to the solution after observing a new data point.
I compare RecLasso to Lars and Coordinate Descent, and present an application
to compressive sensing with sequential observations. I also propose an algorithm to
automatically update the regularization parameter after observing a new data point.},
  timestamp = {2016-07-29T14:44:19Z},
  urldate = {2016-07-28},
  school = {University of California, Berkeley},
  author = {{Pierre Garrigues}},
  year = {2009},
  note = {00000},
  file = {Sparse Coding Models of Natural Images.pdf:/Users/fergalcotter/Dropbox/Papers/Sparse Coding Models of Natural Images.pdf:application/pdf;Sparse Coding Models of Natural Images\: Algorithms for Efficient Inference and Learning of Higher-Order Structure | EECS at UC Berkeley:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/UEP8FHMW/EECS-2009-71.html:text/html},
  groups = {Sparsity for Vision,Sparsity for Vision}
}

@incollection{orr_regularization_2012,
  series = {Lecture Notes in Computer Science},
  title = {Regularization {{Techniques}} to {{Improve Generalization}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {Preface Good tricks for regularization are extremely important for improving the generalization ability of neural networks. The first and most commonly used trick is early stopping, which was originally described in [11]. In its simplest version, the trick is as follows: Take an independent validation set, e.g. take out a part of the training set, and monitor the error on this set during training. The error on the training set will decrease, whereas the error on the validation set will first decrease and then increase. The early stopping point occurs where the error on the validation set is the lowest. It is here that the network weights provide the best generalization.},
  language = {en},
  timestamp = {2016-08-09T23:23:28Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00000},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {49--51},
  file = {Orr_Müller_2012_Regularization Techniques to Improve Generalization.pdf:/Users/fergalcotter/Dropbox/Papers/Orr_Müller_2012_Regularization Techniques to Improve Generalization.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/VWVMQ5NA/978-3-642-35289-8_4.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_4}
}

@article{ioannou_training_2015,
  title = {Training {{CNNs}} with {{Low}}-{{Rank Filters}} for {{Efficient Image Classification}}},
  abstract = {We propose a new method for creating computationally efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41\% less compute and only 24\% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7\% while reducing computation by 16\% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26\% less compute and 41\% fewer model parameters. Applying our method to a near state-of-the-art network for CIFAR, we achieved comparable accuracy with 46\% less compute and 55\% fewer parameters.},
  timestamp = {2016-01-20T12:28:49Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.06744},
  primaryClass = {cs},
  urldate = {2015-11-26},
  journal = {arXiv:1511.06744 [cs]},
  author = {Ioannou, Yani and Robertson, Duncan and Shotton, Jamie and Cipolla, Roberto and Criminisi, Antonio},
  month = nov,
  year = {2015},
  note = {00000},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: Under review as a conference paper at ICLR 2016},
  file = {Ioannou et al. - 2015 - Training CNNs with Low-Rank Filters for Efficient .pdf:/Users/fergalcotter/Dropbox/Papers/Ioannou et al. - 2015 - Training CNNs with Low-Rank Filters for Efficient .pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/2TNEDUCW/1511.html:text/html},
  groups = {Specific Design,Specific Design,Specific Design}
}

@techreport{komlos_has_2014,
  type = {Working Paper},
  title = {Has {{Creative Destruction Become More Destructive}}?},
  abstract = {Schumpeter's concept of creative destruction as the engine of capitalist development is well-known. However, that the destructive part of creative destruction is a social cost and therefore biases our estimate of the impact of the innovation on NNP and on welfare is hardly acknowledged, with the exception of Witt (1996). Admittedly, during the First and Second Industrial Revolutions the magnitude of the destructive component of innovation was probably small compared to the net value added to employment, NNP or to welfare. However, we conjecture that recently the new technologies are often creating products which are close substitutes for the ones they replace whose value depreciates substantially in the process of destruction. Consequently, the contribution of recent innovations to NNP is likely biased upward. This note calls for a research agenda to estimate innovations into their creative and destructive components in order to provide improved estimates of their contribution to NNP, welfare, and employment.},
  timestamp = {2016-07-20T23:57:34Z},
  number = {20379},
  urldate = {2016-07-20},
  institution = {National Bureau of Economic Research},
  author = {Komlos, John},
  month = aug,
  year = {2014},
  note = {00002},
  groups = {ICVSS,ICVSS}
}

@inproceedings{ng_matching_2010,
  title = {Matching of Interest Point Groups with Pairwise Spatial Constraints},
  timestamp = {2016-01-14T14:02:18Z},
  urldate = {2015-11-03},
  booktitle = {Image {{Processing}} ({{ICIP}}), 2010 17th {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  author = {Ng, Ee Sin and Kingsbury, Nick G.},
  year = {2010},
  note = {00016},
  pages = {2693--2696},
  file = {Ng_Kingsbury_2010_Matching of interest point groups with pairwise spatial constraints.pdf:/Users/fergalcotter/Dropbox/Papers/Ng_Kingsbury_2010_Matching of interest point groups with pairwise spatial constraints.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@article{daniely_toward_2016,
  title = {Toward {{Deeper Understanding}} of {{Neural Networks}}: {{The Power}} of {{Initialization}} and a {{Dual View}} on {{Expressivity}}},
  shorttitle = {Toward {{Deeper Understanding}} of {{Neural Networks}}},
  abstract = {We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power.},
  timestamp = {2016-09-16T18:54:50Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.05897},
  primaryClass = {cs, stat},
  urldate = {2016-03-02},
  journal = {arXiv:1602.05897 [cs, stat]},
  author = {Daniely, Amit and Frostig, Roy and Singer, Yoram},
  month = feb,
  year = {2016},
  note = {00001},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Computer Science - Learning,Statistics - Machine Learning},
  file = {Daniely et al_2016_Toward Deeper Understanding of Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Daniely et al_2016_Toward Deeper Understanding of Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/V5W2JMUW/1602.html:text/html},
  groups = {Other Networks,Other Networks}
}

@article{szegedy_inception-v4_2016,
  title = {Inception-v4, {{Inception}}-{{ResNet}} and the {{Impact}} of {{Residual Connections}} on {{Learning}}},
  abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
  timestamp = {2016-02-24T19:12:48Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.07261},
  primaryClass = {cs},
  urldate = {2016-02-24},
  journal = {arXiv:1602.07261 [cs]},
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent},
  month = feb,
  year = {2016},
  note = {00000},
  keywords = {_tablet,Computer Science - Computer Vision and Pattern Recognition},
  file = {Szegedy et al_2016_Inception-v4, Inception-ResNet and the Impact of Residual Connections on.pdf:/Users/fergalcotter/Dropbox/Papers/Szegedy et al_2016_Inception-v4, Inception-ResNet and the Impact of Residual Connections on.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/VMAEQ44C/1602.html:text/html},
  groups = {state of the art,state of the art,state of the art}
}

@inproceedings{serre_object_2005,
  title = {Object Recognition with Features Inspired by Visual Cortex},
  volume = {2},
  doi = {10.1109/CVPR.2005.254},
  abstract = {We introduce a novel set of features for robust object recognition. Each element of this set is a complex feature obtained by combining position- and scale-tolerant edge-detectors over neighboring positions and multiple orientations. Our system's architecture is motivated by a quantitative model of visual cortex. We show that our approach exhibits excellent recognition performance and outperforms several state-of-the-art systems on a variety of image datasets including many different object categories. We also demonstrate that our system is able to learn from very few examples. The performance of the approach constitutes a suggestive plausibility proof for a class of feedforward models of object recognition in cortex.},
  timestamp = {2016-01-25T15:41:09Z},
  booktitle = {{{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}, 2005. {{CVPR}} 2005},
  author = {Serre, T. and Wolf, L. and Poggio, T.},
  month = jun,
  year = {2005},
  note = {00773},
  keywords = {Biology computing,Brain modeling,edge detection,Face detection,Feature extraction,Geometry,image dataset,Image recognition,object detection,object recognition,position-tolerant edge detector,Robustness,scale-tolerant edge detector,Shape,Target recognition,Unread,visual cortex},
  pages = {994--1000 vol. 2},
  file = {Serre et al_2005_Object recognition with features inspired by visual cortex.pdf:/Users/fergalcotter/Dropbox/Papers/Serre et al_2005_Object recognition with features inspired by visual cortex.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/4H5ZGJ77/abs_all.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@article{zagoruyko_wide_2016,
  title = {Wide {{Residual Networks}}},
  abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN.},
  timestamp = {2016-08-09T22:22:39Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.07146},
  primaryClass = {cs},
  urldate = {2016-08-09},
  journal = {arXiv:1605.07146 [cs]},
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  month = may,
  year = {2016},
  note = {00006},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Zagoruyko_Komodakis_2016_Wide Residual Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Zagoruyko_Komodakis_2016_Wide Residual Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/VHCVNTIF/1605.html:text/html},
  groups = {network features,network features,network features}
}

@phdthesis{bruna_scattering_2013,
  type = {Theses},
  title = {Scattering {{Representations}} for {{Recognition}}},
  timestamp = {2016-02-01T15:37:00Z},
  urldate = {2016-02-01},
  school = {Ecole Polytechnique X},
  author = {Bruna, Joan},
  month = feb,
  year = {2013},
  note = {00010 
D{\'e}pos{\'e}e Novembre 2012.},
  keywords = {classifcation,Classification,invariance,multifractales,multifractals,ondelettes,recognition,reconnaissance,Similar Work,Unread,wavelets},
  file = {Bruna_2013_Scattering Representations for Recognition.pdf:/Users/fergalcotter/Dropbox/Papers/Bruna_2013_Scattering Representations for Recognition.pdf:application/pdf;HAL Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/UBPSK9TK/pastel-00905109.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@incollection{caruana_dozen_2012,
  series = {Lecture Notes in Computer Science},
  title = {A {{Dozen Tricks}} with {{Multitask Learning}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {Multitask Learning is an inductive transfer method that improves generalization accuracy on a main task by using the information contained in the training signals of other related tasks. It does this by learning the extra tasks in parallel with the main task while using a shared representation; what is learned for each task can help other tasks be learned better. This chapter describes a dozen opportunities for applying multitask learning in real problems. At the end of the chapter we also make several suggestions for how to get the most our of multitask learning on real-world problems.},
  language = {en},
  timestamp = {2016-08-09T23:23:38Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Caruana, Rich},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00012},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {163--189},
  file = {Caruana_2012_A Dozen Tricks with Multitask Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Caruana_2012_A Dozen Tricks with Multitask Learning.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/F4AAVGT9/978-3-642-35289-8_12.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_12}
}

@inproceedings{nair_rectified_2010,
  title = {Rectified {{Linear Units Improve Restricted Boltzmann Machines}}},
  timestamp = {2016-08-08T19:33:12Z},
  urldate = {2016-08-08},
  author = {Nair, Vinod and Hinton, Geoffrey E.},
  year = {2010},
  note = {00861},
  pages = {807--814},
  file = {Nair_Hinton_2010_Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:/Users/fergalcotter/Dropbox/Papers/Nair_Hinton_2010_Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/RV6MAZ7D/icml2010_NairH10.html:text/html},
  groups = {network features,network features,network features}
}

@inproceedings{simard_best_2003,
  title = {Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis},
  doi = {10.1109/ICDAR.2003.1227801},
  abstract = {Not Available},
  timestamp = {2016-08-07T16:27:08Z},
  booktitle = {Seventh {{International Conference}} on {{Document Analysis}} and {{Recognition}}, 2003. {{Proceedings}}},
  author = {Simard, P. Y. and Steinkraus, D. and Platt, J. C.},
  month = aug,
  year = {2003},
  note = {00716},
  keywords = {Best practices,Concrete,Convolution,Handwriting recognition,Industrial training,Information processing,Neural networks,Performance analysis,support vector machines,Text analysis},
  pages = {958--963},
  file = {Simard et al_2003_Best practices for convolutional neural networks applied to visual document.pdf:/Users/fergalcotter/Dropbox/Papers/Simard et al_2003_Best practices for convolutional neural networks applied to visual document.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/JAFAVD6I/abs_all.html:text/html},
  groups = {Generic Design,Generic Design,Generic Design}
}

@article{olshausen_emergence_1996,
  title = {Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images},
  volume = {381},
  copyright = {\textcopyright{} 1996 Nature Publishing Group},
  doi = {10.1038/381607a0},
  language = {en},
  timestamp = {2016-07-29T14:44:20Z},
  number = {6583},
  urldate = {2016-07-27},
  journal = {Nature},
  author = {Olshausen, Bruno A. and Field, David J.},
  month = jun,
  year = {1996},
  note = {04099},
  pages = {607--609},
  file = {Olshausen_Field_1996_Emergence of simple-cell receptive field properties by learning a sparse code.pdf:/Users/fergalcotter/Dropbox/Papers/Olshausen_Field_1996_Emergence of simple-cell receptive field properties by learning a sparse code.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/42QIZ9IX/381607a0.html:text/html},
  groups = {ICVSS,Sparsity for Vision,ICVSS,Sparsity for Vision}
}

@inproceedings{yingsong_zhang_restoration_????,
  title = {Restoration of Images and {{3D}} Data to Higher Resolution by Deconvolution with Sparsity Regularization - {{Semantic Scholar}}},
  abstract = {Image convolution is conventionally approximated by the LTI discrete model. It is well recognized that the higher the sampling rate, the better is the approximation. However sometimes images or 3D data are only available at a lower sampling rate due to physical constraints of the imaging system. In this paper, we model the under-sampled observation as the result of combining convolution and subsampling. Because the wavelet coefficients of piecewise smooth images tend to be sparse and well modelled by tree-like structures, we propose the L0 reweighted-L2 minimization (L 0 RL 2) algorithm to solve this problem. This promotes sparsity by minimizing the reweighted L2 norm, which approximates the L0 norm, and by enforcing a tree model through bivariate shrinkage. We test the algorithm on 3 examples: a simple ring, the cameraman image and a 3D microscope dataset; and show that good results can be obtained.},
  timestamp = {2016-08-07T17:20:55Z},
  urldate = {2016-08-05},
  author = {{Yingsong Zhang} and {Nick Kingsbury}},
  note = {00000},
  file = {Yingsong Zhang_Nick Kingsbury_Restoration of images and 3D data to higher resolution by deconvolution with.pdf:/Users/fergalcotter/Dropbox/Papers/Yingsong Zhang_Nick Kingsbury_Restoration of images and 3D data to higher resolution by deconvolution with.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/8Q8SVWME/af042e8dd846aa7a9d891419afcae4e3653bc4d7.html:text/html},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@article{he_deep_2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  timestamp = {2016-03-02T00:10:24Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.03385},
  primaryClass = {cs},
  urldate = {2016-02-24},
  journal = {arXiv:1512.03385 [cs]},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = dec,
  year = {2015},
  note = {00013},
  keywords = {_tablet,Computer Science - Computer Vision and Pattern Recognition},
  annote = {2015 ILSVRC winner},
  file = {He et al_2015_Deep Residual Learning for Image Recognition.pdf:/Users/fergalcotter/Dropbox/Papers/He et al_2015_Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/SMMQ7KH4/1512.html:text/html},
  groups = {state of the art,state of the art,state of the art,Reading Groups,Reading Groups}
}

@techreport{scott_pakin_comprehensive_2015,
  title = {The {{Comprehensive Latex Symbol List}}},
  timestamp = {2016-08-08T19:33:13Z},
  author = {{Scott Pakin}},
  month = nov,
  year = {2015},
  note = {00048},
  file = {Scott Pakin_2015_The Comprehensive Latex Symbol List.pdf:/Users/fergalcotter/Dropbox/Papers/Scott Pakin_2015_The Comprehensive Latex Symbol List.pdf:application/pdf},
  groups = {Latex,Latex}
}

@techreport{till_tantau_pgfplots_2013,
  title = {{{PGFplots}}},
  timestamp = {2016-08-08T18:54:03Z},
  number = {Version 3.0.1a},
  author = {{Till Tantau}},
  year = {2013},
  note = {00000},
  groups = {Latex,Latex}
}

@inproceedings{hatipoglu_image_2000,
  title = {Image Texture Description Using Complex Wavelet Transform},
  abstract = {Texture description is important for image retrieval from various large image databases. A method for representing texture Information in images utilizing dual tree complex wavelet transform (DT-CWT) is introduced. Approximate shift invariance, good directional selectivity and computational efficiency properties of DT-CWT are effective in describing texture patterns. Perfect reconstruction property of DT-CWT enables us to store the image data using DT-CWT coefficients. Experiments are performed with Brodatz texture database [1] and nature images that have significant amount of texture information to investigate the efficiency of our texture description algorithm.},
  timestamp = {2016-01-20T12:29:08Z},
  author = {Hatipoglu, S. and Mitra, S. K. and Kingsbury, N.},
  year = {2000},
  note = {00025},
  file = {Hatipoglu et al_2000_Image texture description using complex wavelet transform.pdf:/Users/fergalcotter/Dropbox/Papers/Hatipoglu et al_2000_Image texture description using complex wavelet transform.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@inproceedings{glorot_deep_2011,
  title = {Deep {{Sparse Rectifier Neural Networks}}},
  timestamp = {2016-08-09T21:31:48Z},
  urldate = {2016-08-09},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year = {2011},
  note = {00526},
  pages = {315--323},
  file = {Glorot et al_2011_Deep Sparse Rectifier Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Glorot et al_2011_Deep Sparse Rectifier Neural Networks.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/UX324FKQ/AISTATS2011_GlorotBB11.html:text/html},
  groups = {network features,network features,network features}
}

@techreport{krizhevsky_learning_2009,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  timestamp = {2016-05-27T16:10:48Z},
  author = {Krizhevsky, Alex},
  month = apr,
  year = {2009},
  note = {00757},
  file = {Krizhevsky2009_Learning_Multiple_Layers_of_Features_from_Tiny_Images.pdf:/Users/fergalcotter/Dropbox/Papers/Krizhevsky2009_Learning_Multiple_Layers_of_Features_from_Tiny_Images.pdf:application/pdf},
  groups = {Unsupervised Methods,datasets,Unsupervised Methods,datasets,datasets}
}

@article{ng_robust_2012,
  title = {Robust Pairwise Matching of Interest Points with Complex Wavelets},
  volume = {21},
  timestamp = {2015-11-19T14:13:31Z},
  number = {8},
  urldate = {2015-11-03},
  journal = {Image Processing, IEEE Transactions on},
  author = {Ng, Ee Sin and Kingsbury, Nick G.},
  year = {2012},
  note = {00006},
  pages = {3429--3442},
  file = {Ng_Kingsbury_2012_Robust pairwise matching of interest points with complex wavelets.pdf:/Users/fergalcotter/Dropbox/Papers/Ng_Kingsbury_2012_Robust pairwise matching of interest points with complex wavelets.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@inproceedings{bendale_multiscale_2010,
  title = {Multiscale Keypoint Analysis Based on Complex Wavelets},
  timestamp = {2016-01-14T14:02:21Z},
  urldate = {2015-11-03},
  booktitle = {{{BMVC}} 2010-{{British Machine Vision Conference}}},
  publisher = {{BMVA Press}},
  author = {Bendale, Pashmina and Triggs, William and Kingsbury, Nick},
  year = {2010},
  note = {00009},
  pages = {49--1},
  file = {Bendale et al_2010_Multiscale keypoint analysis based on complex wavelets.pdf:/Users/fergalcotter/Dropbox/Papers/Bendale et al_2010_Multiscale keypoint analysis based on complex wavelets.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@book{mallat_wavelet_1998,
  title = {A {{Wavelet Tour}} of {{Signal Processing}}},
  timestamp = {2016-09-13T11:33:04Z},
  publisher = {{Academic Press}},
  author = {Mallat, Stephane},
  year = {1998},
  note = {17547},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@incollection{casazza_physical_2006,
  title = {A Physical Interpretation of Tight Frames},
  timestamp = {2016-01-14T17:37:44Z},
  urldate = {2016-01-14},
  booktitle = {Harmonic Analysis and Applications},
  publisher = {{Birkh{\"a}user Boston}},
  author = {Casazza, Peter G. and Fickus, Matthew and Kova{\v c}evi{\'c}, Jelena and Leon, Manuel T. and Tremain, Janet C.},
  year = {2006},
  note = {00095},
  keywords = {_tablet},
  pages = {51--76},
  annote = {This was all about the frame force. Went well with Chapter 4 of "An Introduction to Frames" by Kovacevic and Chebira.},
  file = {Casazza et al_2006_A physical interpretation of tight frames.pdf:/Users/fergalcotter/Dropbox/Papers/Casazza et al_2006_A physical interpretation of tight frames.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/673VMDTF/0-8176-4504-7_4.html:text/html},
  groups = {Miscellaneous,Miscellaneous}
}

@article{unser_texture_1995,
  title = {Texture Classification and Segmentation Using Wavelet Frames},
  volume = {4},
  issn = {1057-7149},
  doi = {10.1109/83.469936},
  abstract = {This paper describes a new approach to the characterization of texture properties at multiple scales using the wavelet transform. The analysis uses an overcomplete wavelet decomposition, which yields a description that is translation invariant. It is shown that this representation constitutes a tight frame of l2 and that it has a fast iterative algorithm. A texture is characterized by a set of channel variances estimated at the output of the corresponding filter bank. Classification experiments with l2 Brodatz textures indicate that the discrete wavelet frame (DWF) approach is superior to a standard (critically sampled) wavelet transform feature extraction. These results also suggest that this approach should perform better than most traditional single resolution techniques (co-occurrences, local linear transform, and the like). A detailed comparison of the classification performance of various orthogonal and biorthogonal wavelet transforms is also provided. Finally, the DWF feature extraction technique is incorporated into a simple multicomponent texture segmentation algorithm, and some illustrative examples are presented},
  timestamp = {2016-07-31T16:55:56Z},
  number = {11},
  journal = {IEEE Transactions on Image Processing},
  author = {Unser, M.},
  month = nov,
  year = {1995},
  note = {01566},
  keywords = {biorthogonal wavelet transforms,channel variances,discrete wavelet frame approach,Discrete wavelet transforms,fast iterative algorithm,Feature extraction,filter bank,Frequency,Gabor filters,image classification,image segmentation,image texture,Iterative algorithms,l2 Brodatz textures,multicomponent texture segmentation algorithm,orthogonal wavelet transforms,overcomplete wavelet decomposition,Spline,texture classification,tight frame,transforms,translation invariant description,Wavelet analysis,Wavelet packets,wavelet transforms},
  pages = {1549--1560},
  file = {Unser_1995_Texture classification and segmentation using wavelet frames.pdf:/Users/fergalcotter/Dropbox/Papers/Unser_1995_Texture classification and segmentation using wavelet frames.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/AQD3JN5J/abs_all.html:text/html},
  groups = {Background,Background,Background}
}

@article{jaderberg_speeding_2014,
  title = {Speeding up {{Convolutional Neural Networks}} with {{Low Rank Expansions}}},
  abstract = {The focus of this paper is speeding up the evaluation of convolutional neural networks. While delivering impressive results across a range of computer vision and machine learning tasks, these networks are computationally demanding, limiting their deployability. Convolutional layers generally consume the bulk of the processing time, and so in this work we present two simple schemes for drastically speeding up these layers. This is achieved by exploiting cross-channel or filter redundancy to construct a low rank basis of filters that are rank-1 in the spatial domain. Our methods are architecture agnostic, and can be easily applied to existing CPU and GPU convolutional frameworks for tuneable speedup performance. We demonstrate this with a real world network designed for scene text character recognition, showing a possible 2.5x speedup with no loss in accuracy, and 4.5x speedup with less than 1\% drop in accuracy, still achieving state-of-the-art on standard benchmarks.},
  timestamp = {2016-01-20T12:28:44Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1405.3866},
  primaryClass = {cs},
  urldate = {2015-11-29},
  journal = {arXiv:1405.3866 [cs]},
  author = {Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  month = may,
  year = {2014},
  note = {00043},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Unread},
  file = {Jaderberg et al_2014_Speeding up Convolutional Neural Networks with Low Rank Expansions.pdf:/Users/fergalcotter/Dropbox/Papers/Jaderberg et al_2014_Speeding up Convolutional Neural Networks with Low Rank Expansions.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/TW8GH5VH/1405.html:text/html},
  groups = {Specific Design,Specific Design,Specific Design}
}

@article{bengio_early_2015,
  title = {Early {{Inference}} in {{Energy}}-{{Based Models Approximates Back}}-{{Propagation}}},
  abstract = {We show that Langevin MCMC inference in an energy-based model with latent variables has the property that the early steps of inference, starting from a stationary point, correspond to propagating error gradients into internal layers, similarly to back-propagation. The error that is back-propagated is with respect to visible units that have received an outside driving force pushing them away from the stationary point. Back-propagated error gradients correspond to temporal derivatives of the activation of hidden units. This observation could be an element of a theory for explaining how brains perform credit assignment in deep hierarchies as efficiently as back-propagation does. In this theory, the continuous-valued latent variables correspond to averaged voltage potential (across time, spikes, and possibly neurons in the same minicolumn), and neural computation corresponds to approximate inference and error back-propagation at the same time.},
  timestamp = {2016-02-22T17:45:58Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1510.02777},
  primaryClass = {cs},
  urldate = {2016-02-22},
  journal = {arXiv:1510.02777 [cs]},
  author = {Bengio, Yoshua and Fischer, Asja},
  month = oct,
  year = {2015},
  note = {00001},
  keywords = {Computer Science - Learning},
  annote = {Comment: arXiv admin note: text overlap with arXiv:1509.05936},
  file = {Bengio_Fischer_2015_Early Inference in Energy-Based Models Approximates Back-Propagation.pdf:/Users/fergalcotter/Dropbox/Papers/Bengio_Fischer_2015_Early Inference in Energy-Based Models Approximates Back-Propagation.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/5SHC5DNV/1510.html:text/html},
  groups = {Biological Vision,Biological Vision}
}

@article{szegedy_rethinking_2015,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
  timestamp = {2016-09-13T11:33:05Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.00567},
  primaryClass = {cs},
  urldate = {2016-08-26},
  journal = {arXiv:1512.00567 [cs]},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  month = dec,
  year = {2015},
  note = {00047},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Szegedy et al_2015_Rethinking the Inception Architecture for Computer Vision.pdf:/Users/fergalcotter/Dropbox/Papers/Szegedy et al_2015_Rethinking the Inception Architecture for Computer Vision.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/6T6TCT3J/1512.html:text/html},
  groups = {network features,network features,network features}
}

@inproceedings{zeiler_visualizing_2014,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  copyright = {\textcopyright{}2014 Springer International Publishing Switzerland},
  isbn = {978-3-319-10589-5 978-3-319-10590-1},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  language = {en},
  timestamp = {2017-09-05T12:13:40Z},
  urldate = {2015-11-18},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  month = sep,
  year = {2014},
  note = {00000},
  keywords = {Artificial Intelligence (incl. Robotics),Computer Graphics,Image Processing and Computer Vision,Key Paper,pattern recognition},
  pages = {818--833},
  file = {Zeiler_Fergus_2014_Visualizing and Understanding Convolutional Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Zeiler_Fergus_2014_Visualizing and Understanding Convolutional Networks.pdf:application/pdf},
  groups = {Visualization \& Generative,Visualization \& Generative,Visualization \& Generative,mlsp cites}
}

@article{scellier_towards_2016,
  title = {Towards a {{Biologically Plausible Backprop}}},
  abstract = {This work follows Bengio and Fischer (2015) in which theoretical foundations were laid to show how iterative inference can backpropagate error signals. Neurons move their activations towards configurations corresponding to lower energy and smaller prediction error: a new observation creates a perturbation at visible neurons that propagates into hidden layers, with these propagated perturbations corresponding to the back-propagated gradient. This avoids the need for a lengthy relaxation in the positive phase of training (when both inputs and targets are observed), as was believed with previous work on fixed-point recurrent networks. We show experimentally that energy-based neural networks with several hidden layers can be trained at discriminative tasks by using iterative inference and an STDP-like learning rule. The main result of this paper is that we can train neural networks with 1, 2 and 3 hidden layers on the permutation-invariant MNIST task and get the training error down to 0.00\%. The results presented here make it more biologically plausible that a mechanism similar to back-propagation may take place in brains in order to achieve credit assignment in deep networks. The paper also discusses some of the remaining open problems to achieve a biologically plausible implementation of backprop in brains.},
  timestamp = {2016-02-22T11:12:36Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.05179},
  primaryClass = {cs},
  urldate = {2016-02-22},
  journal = {arXiv:1602.05179 [cs]},
  author = {Scellier, Benjamin and Bengio, Yoshua},
  month = feb,
  year = {2016},
  note = {00000},
  keywords = {Computer Science - Learning},
  file = {Scellier_Bengio_2016_Towards a Biologically Plausible Backprop.pdf:/Users/fergalcotter/Dropbox/Papers/Scellier_Bengio_2016_Towards a Biologically Plausible Backprop.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/2DSXT6KF/1602.html:text/html},
  groups = {Biological Vision,Biological Vision}
}

@article{simonyan_very_2014,
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  timestamp = {2016-08-08T21:51:15Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.1556},
  primaryClass = {cs},
  urldate = {2016-08-08},
  journal = {arXiv:1409.1556 [cs]},
  author = {Simonyan, Karen and Zisserman, Andrew},
  month = sep,
  year = {2014},
  note = {01664},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Simonyan_Zisserman_2014_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:/Users/fergalcotter/Dropbox/Papers/Simonyan_Zisserman_2014_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/MNVDM3MC/1409.html:text/html},
  groups = {state of the art,state of the art,state of the art,mlsp cites}
}

@inproceedings{choi_hidden_2000,
  title = {Hidden {{Markov}} Tree Modeling of Complex Wavelet Transforms},
  volume = {1},
  timestamp = {2016-01-14T14:02:24Z},
  urldate = {2015-11-03},
  booktitle = {Acoustics, {{Speech}}, and {{Signal Processing}}, 2000. {{ICASSP}}'00. {{Proceedings}}. 2000 {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  author = {Choi, Hyeokho and Romberg, Justin and Baraniuk, Richard and Kingsbury, Nick},
  year = {2000},
  note = {00114},
  pages = {133--136},
  file = {Choi et al_2000_Hidden Markov tree modeling of complex wavelet transforms.pdf:/Users/fergalcotter/Dropbox/Papers/Choi et al_2000_Hidden Markov tree modeling of complex wavelet transforms.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@article{lee_image_1996,
  title = {Image Representation Using {{2D Gabor}} Wavelets},
  volume = {18},
  issn = {0162-8828},
  doi = {10.1109/34.541406},
  abstract = {This paper extends to two dimensions the frame criterion developed by Daubechies for one-dimensional wavelets, and it computes the frame bounds for the particular case of 2D Gabor wavelets. Completeness criteria for 2D Gabor image representations are important because of their increasing role in many computer vision applications and also in modeling biological vision, since recent neurophysiological evidence from the visual cortex of mammalian brains suggests that the filter response profiles of the main class of linearly-responding cortical neurons (called simple cells) are best modeled as a family of self-similar 2D Gabor wavelets. We therefore derive the conditions under which a set of continuous 2D Gabor wavelets will provide a complete representation of any image, and we also find self-similar wavelet parametrization which allow stable reconstruction by summation as though the wavelets formed an orthonormal basis. Approximating a ``tight frame'' generates redundancy which allows low-resolution neural responses to represent high-resolution images},
  timestamp = {2016-02-11T12:38:59Z},
  number = {10},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Lee, Tai Sing},
  month = oct,
  year = {1996},
  note = {01551},
  keywords = {2D Gabor wavelets,Application software,Biological system modeling,Biology computing,Brain modeling,Cells (biology),coarse coding,computer vision,Continuous wavelet transforms,frame bounds,frame criterion,Gabor filters,Image coding,Image reconstruction,Image representation,Neurons,neurophysiology,self-similar wavelet parametrization,Useful,visual cortex,wavelet transforms},
  pages = {959--971},
  file = {Lee_1996_Image representation using 2D Gabor wavelets.pdf:/Users/fergalcotter/Dropbox/Papers/Lee_1996_Image representation using 2D Gabor wavelets.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/F5F3K2Q3/articleDetails.html:text/html},
  groups = {Biological Vision,Biological Vision}
}

@article{he_delving_2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human}}-{{Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  timestamp = {2016-08-07T12:44:01Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.01852},
  primaryClass = {cs},
  urldate = {2016-08-07},
  journal = {arXiv:1502.01852 [cs]},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = feb,
  year = {2015},
  note = {00401},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  file = {He et al_2015_Delving Deep into Rectifiers.pdf:/Users/fergalcotter/Dropbox/Papers/He et al_2015_Delving Deep into Rectifiers.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/U7WUXFQD/1502.html:text/html},
  groups = {network features,network features,network features}
}

@incollection{orr_representing_2012,
  series = {Lecture Notes in Computer Science},
  title = {Representing and {{Incorporating Prior Knowledge}} in {{Neural Network Training}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {Preface The present section focuses on tricks for four important aspects in learning: (1) incorporation of prior knowledge, (2) choice of representation for the learning task, (3) unequal class prior distributions, and finally (4) large network training.},
  language = {en},
  timestamp = {2016-08-09T23:23:26Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00000},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {231--233},
  file = {Orr_Müller_2012_Representing and Incorporating Prior Knowledge in Neural Network Training.pdf:/Users/fergalcotter/Dropbox/Papers/Orr_Müller_2012_Representing and Incorporating Prior Knowledge in Neural Network Training.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/ST58X4FD/978-3-642-35289-8_16.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_16}
}

@phdthesis{anderson_phase-based_2007,
  title = {Phase-{{Based Object Matching}} Using {{Complex Wavelets}}},
  timestamp = {2016-02-19T15:42:52Z},
  school = {Cambridge},
  author = {Anderson, Ryan},
  year = {2007},
  note = {00000},
  annote = {This is supposed to be important for looking at joining up edges},
  file = {Anderson_2007_Phase-Based Object Matching using Complex Wavelets.pdf:/Users/fergalcotter/Dropbox/Papers/Anderson_2007_Phase-Based Object Matching using Complex Wavelets.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@book{abadi_tensorflow:_2015,
  title = {{{TensorFlow}}: {{Large}}-{{Scale Machine Learning}} on {{Heterogeneous Systems}}},
  timestamp = {2016-08-15T00:48:49Z},
  author = {Abadi, Mart{\'\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dan Man{\'e}} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Vi{\'e}gas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
  year = {2015},
  note = {00002 
Software available from tensorflow.org},
  groups = {Software}
}

@article{lowe_distinctive_2004,
  title = {Distinctive {{Image Features}} from {{Scale}}-{{Invariant Keypoints}}},
  volume = {60},
  abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination.},
  timestamp = {2015-11-30T19:21:32Z},
  journal = {International Journal of Computer Vision},
  author = {Lowe, David G.},
  year = {2004},
  note = {32503},
  pages = {91--110},
  file = {Lowe - 2004 - Distinctive Image Features from Scale-Invariant Ke.pdf:/Users/fergalcotter/Dropbox/Papers/Lowe - 2004 - Distinctive Image Features from Scale-Invariant Ke.pdf:application/pdf;Citeseer - Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/5N8PKKU6/summary.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@article{smith_gradual_2015,
  title = {Gradual {{DropIn}} of {{Layers}} to {{Train Very Deep Neural Networks}}},
  abstract = {We introduce the concept of dynamically growing a neural network during training. In particular, an untrainable deep network starts as a trainable shallow network and newly added layers are slowly, organically added during training, thereby increasing the network's depth. This is accomplished by a new layer, which we call DropIn. The DropIn layer starts by passing the output from a previous layer (effectively skipping over the newly added layers), then increasingly including units from the new layers for both feedforward and backpropagation. We show that deep networks, which are untrainable with conventional methods, will converge with DropIn layers interspersed in the architecture. In addition, we demonstrate that DropIn provides regularization during training in an analogous way as dropout. Experiments are described with the MNIST dataset and various expanded LeNet architectures, CIFAR-10 dataset with its architecture expanded from 3 to 11 layers, and on the ImageNet dataset with the AlexNet architecture expanded to 13 layers and the VGG 16-layer architecture.},
  timestamp = {2016-01-25T17:09:04Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.06951},
  primaryClass = {cs},
  urldate = {2016-01-25},
  journal = {arXiv:1511.06951 [cs]},
  author = {Smith, Leslie N. and Hand, Emily M. and Doster, Timothy},
  month = nov,
  year = {2015},
  note = {00000},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Smith et al_2015_Gradual DropIn of Layers to Train Very Deep Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Smith et al_2015_Gradual DropIn of Layers to Train Very Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/VXT8CHCE/1511.html:text/html},
  groups = {Other Networks,Other Networks}
}

@inproceedings{sifre_rotation_2013,
  title = {Rotation, {{Scaling}} and {{Deformation Invariant Scattering}} for {{Texture Discrimination}}},
  doi = {10.1109/CVPR.2013.163},
  abstract = {An affine invariant representation is constructed with a cascade of invariants, which preserves information for classification. A joint translation and rotation invariant representation of image patches is calculated with a scattering transform. It is implemented with a deep convolution network, which computes successive wavelet transforms and modulus non-linearities. Invariants to scaling, shearing and small deformations are calculated with linear operators in the scattering domain. State-of-the-art classification results are obtained over texture databases with uncontrolled viewing conditions.},
  timestamp = {2016-01-14T14:02:25Z},
  booktitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Sifre, L. and Mallat, S.},
  month = jun,
  year = {2013},
  note = {00053},
  keywords = {affine,affine invariant representation,Classification,Computer architecture,Convolution,deep,deep convolution network,deformation,deformations,image classification,image patches,Image representation,image texture,information preservation,invariant,Joints,joint translation-rotation invariant representation,Key Paper,linear operators,modulus nonlinearities,network,neural,rotation,roto-translation,scaling,Scattering,scattering domain,scattering transform,shearing,Similar Work,state-of-the-art classification,successive wavelet transforms,texture,texture databases,texture discrimination,translation,uncontrolled viewing conditions,Unread,visual databases,wavelet,wavelet transforms},
  pages = {1233--1240},
  annote = {Rotation, Shift Invariance and Deformation covariance introduced},
  file = {Sifre_Mallat_2013_Rotation, Scaling and Deformation Invariant Scattering for Texture.pdf:/Users/fergalcotter/Dropbox/Papers/Sifre_Mallat_2013_Rotation, Scaling and Deformation Invariant Scattering for Texture.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/SU39JVZ2/abs_all.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted,mlsp cites}
}

@article{olshausen_sparse_1997,
  title = {Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by {{V1}}?},
  volume = {37},
  issn = {0042-6989},
  shorttitle = {Sparse Coding with an Overcomplete Basis Set},
  abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
  language = {eng},
  timestamp = {2016-07-29T14:44:22Z},
  number = {23},
  journal = {Vision Research},
  author = {Olshausen, B. A. and Field, D. J.},
  month = dec,
  year = {1997},
  note = {02433},
  keywords = {Algorithms,Animals,Mammals,Models; Psychological,visual cortex,Visual perception},
  pages = {3311--3325},
  file = {olshausen_field_1997 sparse coding with an overcomplete basis set.pdf:/Users/fergalcotter/Dropbox/Papers/olshausen_field_1997 sparse coding with an overcomplete basis set.pdf:application/pdf},
  groups = {ICVSS,Sparsity for Vision,Unsupervised Methods,ICVSS,Sparsity for Vision,Unsupervised Methods},
  pmid = {9425546}
}

@article{oyallon_generic_2013,
  title = {Generic {{Deep Networks}} with {{Wavelet Scattering}}},
  abstract = {We introduce a two-layer wavelet scattering network, for object classification. This scattering transform computes a spatial wavelet transform on the first layer and a new joint wavelet transform along spatial, angular and scale variables in the second layer. Numerical experiments demonstrate that this two layer convolution network, which involves no learning and no max pooling, performs efficiently on complex image data sets such as CalTech, with structural objects variability and clutter. It opens the possibility to simplify deep neural network learning by initializing the first layers with wavelet filters.},
  timestamp = {2016-01-14T14:02:26Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.5940},
  primaryClass = {cs},
  urldate = {2015-11-30},
  journal = {arXiv:1312.5940 [cs]},
  author = {Oyallon, Edouard and Mallat, St{\'e}phane and Sifre, Laurent},
  month = dec,
  year = {2013},
  note = {00006},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Key Paper,Similar Work,Unread},
  annote = {Scattering Networks applied to Image classification datasets like Caltech 101 and Caltech 256. Only had a 2 layer version of the scattering and then given to a linear SVM. Gets better than 2 layers of a convnet, but not yet at 7 layers of deep network used.},
  file = {Oyallon et al. - 2013 - Generic Deep Networks with Wavelet Scattering.pdf:/Users/fergalcotter/Dropbox/Papers/Oyallon et al. - 2013 - Generic Deep Networks with Wavelet Scattering.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/JCHJ7A4C/1312.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@article{hinton_reducing_2006,
  title = {Reducing the {{Dimensionality}} of {{Data}} with {{Neural Networks}}},
  volume = {313},
  copyright = {American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1127647},
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such ``autoencoder'' networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.},
  language = {en},
  timestamp = {2016-07-29T14:44:23Z},
  number = {5786},
  urldate = {2016-07-11},
  journal = {Science},
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  month = jul,
  year = {2006},
  note = {03626},
  pages = {504--507},
  file = {Hinton_Salakhutdinov_2006_Reducing the Dimensionality of Data with Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Hinton_Salakhutdinov_2006_Reducing the Dimensionality of Data with Neural Networks.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/SEZSPW79/504.html:text/html},
  groups = {ICVSS,ICVSS},
  pmid = {16873662}
}

@inproceedings{rivaz_bayesian_2001,
  title = {Bayesian Image Deconvolution and Denoising Using Complex Wavelets},
  volume = {2},
  doi = {10.1109/ICIP.2001.958477},
  abstract = {This paper proposes a new algorithm for image restoration (deconvolution and denoising) which employs the recently developed dual-tree complex wavelet transform in an iterative Bayesian framework. Complex wavelets are selected for their key features: shift invariance, directional selectivity and efficiency. The aim is to find an optimal description of the restored image in the complex wavelet domain, which minimises a quadratic energy function of the wavelet coefficients. The algorithm searches for this minimum using an efficient conjugate gradient method. We show that this can improve the SNR performance of a good minimax deconvolution method, WaRD, which is used to initialise the iterations, by typically 1.2 dB. Convergence is quite rapid, achieving 80\% of the ultimate performance gain in about 20 iterations. Each iteration takes around 5 seconds using MatLab on a 400 MHz Pentium computer with 256\texttimes{}256 pixel images},
  timestamp = {2016-08-05T00:58:01Z},
  booktitle = {2001 {{International Conference}} on {{Image Processing}}, 2001. {{Proceedings}}},
  author = {de Rivaz, P. and Kingsbury, N.},
  month = oct,
  year = {2001},
  note = {00044},
  keywords = {5 sec,256 pixel,Bayesian methods,Bayes methods,conjugate gradient method,conjugate gradient methods,convergence of numerical methods,deconvolution,directional selectivity,dual-tree complex wavelet transform,Gradient methods,image deconvolution,image denoising,image restoration,interference suppression,Iterative algorithms,iterative Bayesian framework,minimax deconvolution method,minimax techniques,Noise reduction,quadratic energy function,shift invariance,SNR performance,trees (mathematics),wavelet-based regularised deconvolution,Wavelet coefficients,Wavelet domain,wavelet transforms},
  pages = {273--276 vol.2},
  file = {Rivaz_Kingsbury_2001_Bayesian image deconvolution and denoising using complex wavelets.pdf:/Users/fergalcotter/Dropbox/Papers/Rivaz_Kingsbury_2001_Bayesian image deconvolution and denoising using complex wavelets.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/8H7X669X/abs_all.html:text/html},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@inproceedings{regli_scattering_2016,
  title = {Scattering Convolutional Hidden {{Markov}} Trees},
  doi = {10.1109/ICIP.2016.7532685},
  abstract = {We here combine the rich, overcomplete signal representation afforded by the scattering transform together with a probabilistic graphical model which captures hierarchical dependencies between coefficients at different layers. The wavelet scattering network result in a high-dimensional representation which is translation invariant and stable to deformations whilst preserving informative content. Such properties are achieved by cascading wavelet transform convolutions with non-linear modulus and averaging operators. The network structure and its distributions are described using a Hidden Markov Tree. This yields a generative model for high-dimensional inference and offers a means to perform various inference tasks such as prediction. Our proposed scattering convolutional hidden Markov tree displays promising results on classification tasks of complex images in the challenging case where the number of training examples is extremely small.},
  timestamp = {2016-09-30T13:07:28Z},
  booktitle = {2016 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Regli, J. B. and Nelson, J. D. B.},
  month = sep,
  year = {2016},
  note = {00000},
  keywords = {Classification,Convolution,Deep network,Hidden Markov Model,Hidden Markov models,Scattering,Scattering network,Standards,Training,wavelet transforms},
  pages = {1883--1887},
  file = {Regli_Nelson_2016_Scattering convolutional hidden Markov trees.pdf:/Users/fergalcotter/Dropbox/Papers/Regli_Nelson_2016_Scattering convolutional hidden Markov trees.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/6X73VACF/7532685.html:text/html},
  groups = {advanced,advanced,advanced}
}

@incollection{jaderberg_spatial_2015,
  title = {Spatial {{Transformer Networks}}},
  timestamp = {2016-02-01T17:31:26Z},
  urldate = {2016-02-01},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  publisher = {{Curran Associates, Inc.}},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and {kavukcuoglu}, koray},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  note = {00022},
  pages = {2008--2016},
  file = {Jaderberg et al_2015_Spatial Transformer Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Jaderberg et al_2015_Spatial Transformer Networks.pdf:application/pdf;NIPS Snapshort:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/GH2CU4V8/5854-spatial-transformer-networks.html:text/html},
  groups = {Specific Design,Specific Design,Specific Design}
}

@incollection{coifman_translation-invariant_1995,
  series = {Lecture Notes in Statistics},
  title = {Translation-{{Invariant De}}-{{Noising}}},
  copyright = {\textcopyright{}1995 Springer-Verlag New York},
  isbn = {978-0-387-94564-4 978-1-4612-2544-7},
  abstract = {De-Noising with the traditional (orthogonal, maximally-decimated) wavelet transform sometimes exhibits visual artifacts; we attribute some of these\textemdash{}for example, Gibbs phenomena in the neighborhood of discontinuities\textemdash{}to the lack of translation invariance of the wavelet basis. One method to suppress such artifacts, termed ``cycle spinning'' by Coifman, is to ``average out'' the translation dependence. For a range of shifts, one shifts the data (right or left as the case may be), De-Noises the shifted data, and then unshifts the de-noised data. Doing this for each of a range of shifts, and averaging the several results so obtained, produces a reconstruction subject to far weaker Gibbs phenomena than thresholding based De-Noising using the traditional orthogonal wavelet transform. Cycle-Spinning over the range ofall circulant shifts can be accomplished in ordernlog2(n) time; it is equivalent to de-noising using the undecimated or stationary wavelet transform. Cycle-spinning exhibits benefits outside of wavelet de-noising, for example in cosine packet denoising, where it helps suppress `clicks'. It also has a counterpart in frequency domain de-noising, where the goal of translation-invariance is replaced by modulation invariance, and the central shift-De-Noise-unshift operation is replaced by modulate-De-Noise-demodulate. We illustrate these concepts with extensive computational examples; all figures presented here are reproducible using the WaveLab software},
  language = {en},
  timestamp = {2016-09-13T11:33:06Z},
  number = {103},
  urldate = {2016-08-14},
  booktitle = {Wavelets and {{Statistics}}},
  publisher = {{Springer New York}},
  author = {Coifman, R. R. and Donoho, D. L.},
  editor = {Antoniadis, Anestis and Oppenheim, Georges},
  year = {1995},
  note = {02529},
  keywords = {Probability Theory and Stochastic Processes},
  pages = {125--150},
  file = {Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/HRX5A8X8/10.html:text/html},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT},
  doi = {10.1007/978-1-4612-2544-7_9}
}

@incollection{rognvaldsson_simple_2012,
  series = {Lecture Notes in Computer Science},
  title = {A {{Simple Trick}} for {{Estimating}} the {{Weight Decay Parameter}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {We present a simple trick to get an approximate estimate of the weight decay parameter $\lambda$. The method combines early stopping and weight decay, into the estimate ˆ$\lambda$=$\parallel\nabla$E(Wes)$\parallel$/$\parallel$2Wes$\parallel$,$\lambda$\^ =$\parallel\nabla$E(Wes)$\parallel$/$\parallel$2Wes$\parallel$, $\backslash$hat$\backslash$lambda = $\backslash$parallel $\backslash$nabla E(W\_\{es\})$\backslash$parallel /$\backslash$parallel 2W\_\{es\}$\backslash$parallel, where W es is the set of weights at the early stopping point, and E(W) is the training data fit error. The estimate is demonstrated and compared to the standard cross-validation procedure for $\lambda$ selection on one synthetic and four real life data sets. The result is that ˆ$\lambda\backslash$hat$\backslash$lambda is as good an estimator for the optimal weight decay parameter value as the standard search estimate, but orders of magnitude quicker to compute. The results also show that weight decay can produce solutions that are significantly superior to committees of networks trained with early stopping.},
  language = {en},
  timestamp = {2016-08-09T23:23:03Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {R{\"o}gnvaldsson, Thorsteinn S.},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00000},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {69--89},
  file = {Rögnvaldsson_2012_A Simple Trick for Estimating the Weight Decay Parameter.pdf:/Users/fergalcotter/Dropbox/Papers/Rögnvaldsson_2012_A Simple Trick for Estimating the Weight Decay Parameter.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/VCF74W8V/978-3-642-35289-8_6.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_6}
}

@article{mullen_contrast_1985,
  title = {The Contrast Sensitivity of Human Colour Vision to Red-Green and Blue-Yellow Chromatic Gratings.},
  volume = {359},
  issn = {1469-7793},
  doi = {10.1113/jphysiol.1985.sp015591},
  abstract = {A method of producing red-green and blue-yellow sinusoidal chromatic gratings is used which permits the correction of all chromatic aberrations. A quantitative criterion is adopted to choose the intensity match of the two colours in the stimulus: this is the intensity ratio at which contrast sensitivity for the chromatic grating differs most from the contrast sensitivity for a monochromatic luminance grating. Results show that this intensity match varies with spatial frequency and does not necessarily correspond to a luminance match between the colours. Contrast sensitivities to the chromatic gratings at the criterion intensity match are measured as a function of spatial frequency, using field sizes ranging from 2 to 23 deg. Both blue-yellow and red-green contrast sensitivity functions have similar low-pass characteristics, with no low-frequency attenuation even at low frequencies below 0.1 cycles/deg. These functions indicate that the limiting acuities based on red-green and blue-yellow colour discriminations are similar at 11 or 12 cycles/deg. Comparisons between contrast sensitivity functions for the chromatic and monochromatic gratings are made at the same mean luminances. Results show that, at low spatial frequencies below 0.5 cycles/deg, contrast sensitivity is greater to the chromatic gratings, consisting of two monochromatic gratings added in antiphase, than to either monochromatic grating alone. Above 0.5 cycles/deg, contrast sensitivity is greater to monochromatic than to chromatic gratings.},
  language = {en},
  timestamp = {2016-08-06T19:08:37Z},
  number = {1},
  urldate = {2016-08-06},
  journal = {The Journal of Physiology},
  author = {Mullen, K T},
  month = feb,
  year = {1985},
  note = {00868},
  pages = {381--400},
  file = {Mullen_1985_The contrast sensitivity of human colour vision to red-green and blue-yellow.pdf:/Users/fergalcotter/Dropbox/Papers/Mullen_1985_The contrast sensitivity of human colour vision to red-green and blue-yellow.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/IU6IBISP/abstract.html:text/html},
  groups = {Biological Vision,Biological Vision}
}

@incollection{anderson_determining_2005,
  series = {Lecture Notes in Computer Science},
  title = {Determining {{Multiscale Image Feature Angles}} from {{Complex Wavelet Phases}}},
  copyright = {\textcopyright{}2005 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-29069-8 978-3-540-31938-2},
  abstract = {In this paper, we introduce a new multiscale representation for 2-D images named the Inter-Coefficient Product (ICP). The ICP is a decimated pyramid of complex values based on the Dual-Tree Complex Wavelet Transform (DT-CWT). The complex phases of its coefficients correspond to the angles of dominant directional features in their support regions. As a sparse representation of this information, the ICP is relatively simple to calculate and is a computationally efficient representation for subsequent analysis in computer vision activities or large data set analysis. Examples of ICP decomposition show its ability to provide an intuitive representation of multiscale features (such as edges and ridges). Its potential uses are then discussed.},
  language = {en},
  timestamp = {2016-01-20T12:29:15Z},
  number = {3656},
  urldate = {2015-11-04},
  booktitle = {Image {{Analysis}} and {{Recognition}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Anderson, Ryan and Kingsbury, Nick and Fauqueur, Julien},
  editor = {Kamel, Mohamed and Campilho, Aur{\'e}lio},
  month = sep,
  year = {2005},
  note = {00026},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Computer Graphics,Computer Imaging; Vision; Pattern Recognition and Graphics,Image Processing and Computer Vision,pattern recognition},
  pages = {490--498},
  file = {Anderson et al_2005_Determining Multiscale Image Feature Angles from Complex Wavelet Phases_2.pdf:/Users/fergalcotter/Dropbox/Papers/Anderson et al_2005_Determining Multiscale Image Feature Angles from Complex Wavelet Phases_2.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/IHCRWXEP/10.html:text/html},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@inproceedings{kingsbury_wavelet_1997,
  title = {Wavelet Transforms in Image Processing},
  abstract = {This paper is designed to be partly tutorial ill nature and partly a summary of recent work by the authors in applying wavelets to various image processing problems, The tutorial part describes the filter-bank implementation of the discrete wavelet transform (DWT) and shows that most wavelets which permit perfect reconstruction are similar in shape and scale, We then discuss an important drawback of these wavelet transforms, which is that the distribution of energy between coefficients at different scales is very sensitive to shifts in the input data, We propose the Complex Wavelet Transform (CWT) as a solution to this problem and show how it may be applied in two dimensions. Finally we give brief details of applications of the CWT to motion estimation and image de-noising.},
  timestamp = {2016-01-20T12:28:41Z},
  author = {Kingsbury, N. and Magarey, J.},
  editor = {Prochazka, A. and Uhlir, J. and Sovka, P.},
  year = {1997},
  note = {00076},
  file = {Kingsbury_Magarey_1997_Wavelet transforms in image processing.pdf:/Users/fergalcotter/Dropbox/Papers/Kingsbury_Magarey_1997_Wavelet transforms in image processing.pdf:application/pdf},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@article{hubel_receptive_1959,
  title = {Receptive Fields of Single Neurones in the Cat's Striate Cortex},
  volume = {148},
  issn = {0022-3751},
  timestamp = {2016-07-29T14:44:24Z},
  number = {3},
  urldate = {2016-07-26},
  journal = {The Journal of Physiology},
  author = {Hubel, D. H. and Wiesel, T. N.},
  month = oct,
  year = {1959},
  note = {03157},
  pages = {574--591},
  file = {Hubel_Wiesel_1959_Receptive fields of single neurones in the cat's striate cortex.pdf:/Users/fergalcotter/Dropbox/Papers/Hubel_Wiesel_1959_Receptive fields of single neurones in the cat's striate cortex.pdf:application/pdf},
  groups = {Biological Vision,Biological Vision},
  pmid = {14403679},
  pmcid = {PMC1363130}
}

@phdthesis{matthew_zeiler_hierarchical_2014,
  address = {Department of Computer Science},
  title = {Hierarchical {{Convolutional Deep Learning}} in {{Computer Vision}}},
  abstract = {It has long been the goal in computer vision to learn a hierarchy of features useful for object recognition. Spanning the two traditional paradigms of machine learning, unsupervised and supervised learning, we investigate the application of deep learning methods to tackle this challenging task and to learn robust representations of images. 

We begin our investigation with the introduction of a novel unsupervised learning technique called deconvolutional networks. Based on convolutional sparse coding, we show this model learns interesting decompositions of images into parts without object label information. This method, which easily scales to large images, becomes increasingly invariant by learning multiple layers of feature extraction coupled with pooling layers. We
introduce a novel pooling method called Gaussian pooling to enable these layers to store continuous location information while being differentiable, creating a unified objective function to optimize.

In the supervised learning domain, a well-established model for recognition of objects is the convolutional network. We introduce a new regularization method for convolutional networks called stochastic pooling which relies on sampling noise to prevent these powerful models from overfitting. Additionally, we show novel visualizations of these complex models to better understand what they learn and to provide insight on how to develop state-of-the-art architectures for large-scale classification of 1,000 different
object categories.

We also investigate related problems in deep learning. First, we introduce a model for the task of mapping one high dimensional time series sequence onto another. Second, we address the choice of nonlinearity in neural networks, showing evidence that rectified linear units outperform others types in automatic speech recognition. Finally, we introduce a novel optimization method called ADADELTA which shows promising convergence speeds in practice whilst being robust to hyper-parameter selection.},
  timestamp = {2016-08-08T22:42:00Z},
  school = {New York University},
  author = {{Matthew Zeiler}},
  month = jan,
  year = {2014},
  note = {00000},
  file = {Matthew Zeiler_2014_Hierarchical Convolutional Deep Learning in Computer Vision.pdf:/Users/fergalcotter/Dropbox/Papers/Matthew Zeiler_2014_Hierarchical Convolutional Deep Learning in Computer Vision.pdf:application/pdf},
  groups = {Visualization \& Generative,Visualization \& Generative,Visualization \& Generative}
}

@inproceedings{osuna_improved_1997,
  title = {An Improved Training Algorithm for Support Vector Machines},
  doi = {10.1109/NNSP.1997.622408},
  abstract = {We investigate the problem of training a support vector machine (SVM) on a very large database in the case in which the number of support vectors is also very large. Training a SVM is equivalent to solving a linearly constrained quadratic programming (QP) problem in a number of variables equal to the number of data points. This optimization problem is known to be challenging when the number of data points exceeds few thousands. In previous work done by us as well as by other researchers, the strategy used to solve the large scale QP problem takes advantage of the fact that the expected number of support vectors is small ($<$3,000). Therefore, the existing algorithms cannot deal with more than a few thousand support vectors. In this paper we present a decomposition algorithm that is guaranteed to solve the QP problem and that does not make assumptions on the expected number of support vectors. In order to present the feasibility of our approach we consider a foreign exchange rate time series database with 110,000 data points that generates 100,000 support vectors},
  timestamp = {2016-02-04T14:57:01Z},
  booktitle = {Neural {{Networks}} for {{Signal Processing}} [1997] {{VII}}. {{Proceedings}} of the 1997 {{IEEE Workshop}}},
  author = {Osuna, E. and Freund, R. and Girosi, F.},
  month = sep,
  year = {1997},
  note = {01320},
  keywords = {Classification algorithms,decomposition algorithm,Exchange rates,financial data processing,foreign exchange rate,Large-scale systems,Learning systems,Minimization methods,Neural networks,optimization,pattern classification,Polynomials,quadratic programming,support vector machine,Support vector machine classification,support vector machines,time series,training algorithm,very large database,very large databases},
  pages = {276--285},
  file = {Osuna et al_1997_An improved training algorithm for support vector machines.pdf:/Users/fergalcotter/Dropbox/Papers/Osuna et al_1997_An improved training algorithm for support vector machines.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/E7VFH9MG/abs_all.html:text/html},
  groups = {Other Networks,Other Networks}
}

@incollection{orr_improving_2012,
  series = {Lecture Notes in Computer Science},
  title = {Improving {{Network Models}} and {{Algorithmic Tricks}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {Preface This section contains 5 chapters presenting easy to implement tricks which modify either the architecture and/or the learning algorithm so as to enhance the network's modeling ability. Better modeling means better solutions in less time.},
  language = {en},
  timestamp = {2016-08-09T23:22:48Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00000},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {139--141},
  file = {Orr_Müller_2012_Improving Network Models and Algorithmic Tricks.pdf:/Users/fergalcotter/Dropbox/Papers/Orr_Müller_2012_Improving Network Models and Algorithmic Tricks.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/T3TDNS8C/978-3-642-35289-8_10.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_10}
}

@article{waldspurger_phase_2015,
  title = {Phase Retrieval for Wavelet Transforms},
  abstract = {We describe a new algorithm to solve a particular phase retrieval problem, that has wide applications in audio processing: the reconstruction of a function from its scalogram, that is from the modulus of its wavelet transform. It is a multiscale iterative algorithm. To efficiently propagate phase information from low to high frequencies, it uses an equivalent formulation of the phase retrieval problem that involves the holomorphic extension of the wavelet transform. Our numerical results, on audio and non-audio signals, show that reconstruction is precise and stable to noise. The algorithm has a linear complexity in the size of the signal, up to logarithmic factors, and can thus be used on large signals.},
  timestamp = {2016-02-01T16:27:02Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.07024},
  primaryClass = {math},
  urldate = {2016-02-01},
  journal = {arXiv:1512.07024 [math]},
  author = {Waldspurger, Ir{\`e}ne},
  month = dec,
  year = {2015},
  note = {00000},
  keywords = {Mathematics - Optimization and Control,Unread},
  file = {Waldspurger_2015_Phase retrieval for wavelet transforms.pdf:/Users/fergalcotter/Dropbox/Papers/Waldspurger_2015_Phase retrieval for wavelet transforms.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/7GK68JXR/1512.html:text/html},
  groups = {advanced,advanced,advanced}
}

@misc{vedaldi_image_2014,
  title = {Image Representations from Shallow to Deep},
  timestamp = {2016-07-01T16:42:37Z},
  author = {Vedaldi, Andrea},
  year = {2014},
  note = {00000},
  groups = {Visualization \& Generative,Visualization \& Generative,Visualization \& Generative}
}

@article{agostinelli_learning_2014,
  title = {Learning {{Activation Functions}} to {{Improve Deep Neural Networks}}},
  abstract = {Artificial neural networks typically have a fixed, non-linear activation function at each neuron. We have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51\%), CIFAR-100 (30.83\%), and a benchmark from high-energy physics involving Higgs boson decay modes.},
  timestamp = {2016-08-08T14:12:55Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6830},
  primaryClass = {cs, stat},
  urldate = {2016-08-08},
  journal = {arXiv:1412.6830 [cs, stat]},
  author = {Agostinelli, Forest and Hoffman, Matthew and Sadowski, Peter and Baldi, Pierre},
  month = dec,
  year = {2014},
  note = {00023},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annote = {Comment: Accepted as a workshop paper contribution at the International Conference on Learning Representations (ICLR) 2015},
  file = {Agostinelli et al_2014_Learning Activation Functions to Improve Deep Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Agostinelli et al_2014_Learning Activation Functions to Improve Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/57FFSQAC/1412.html:text/html},
  groups = {network features,network features,network features}
}

@article{mallat_theory_1989,
  title = {A Theory for Multiresolution Signal Decomposition: The Wavelet Representation},
  volume = {11},
  issn = {0162-8828},
  shorttitle = {A Theory for Multiresolution Signal Decomposition},
  doi = {10.1109/34.192463},
  abstract = {Multiresolution representations are effective for analyzing the information content of images. The properties of the operator which approximates a signal at a given resolution were studied. It is shown that the difference of information between the approximation of a signal at the resolutions 2j+1 and 2j (where j is an integer) can be extracted by decomposing this signal on a wavelet orthonormal basis of L2(Rn), the vector space of measurable, square-integrable n-dimensional functions. In L2(R), a wavelet orthonormal basis is a family of functions which is built by dilating and translating a unique function $\psi$(x). This decomposition defines an orthogonal multiresolution representation called a wavelet representation. It is computed with a pyramidal algorithm based on convolutions with quadrature mirror filters. Wavelet representation lies between the spatial and Fourier domains. For images, the wavelet representation differentiates several spatial orientations. The application of this representation to data compression in image coding, texture discrimination and fractal analysis is discussed},
  timestamp = {2015-11-04T12:36:43Z},
  number = {7},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Mallat, S.G.},
  month = jul,
  year = {1989},
  note = {20748},
  keywords = {Convolution,Convolutional codes,convolutions,data compression,Data mining,Encoding,Filters,fractal analysis,Image analysis,Image coding,Image resolution,Information analysis,Mirrors,multiresolution signal decomposition,pattern recognition,picture processing,pyramidal algorithm,quadrature mirror filters,Signal resolution,Spatial resolution,texture discrimination,wavelet representation},
  pages = {674--693},
  file = {Mallat_1989_A theory for multiresolution signal decomposition_2.pdf:/Users/fergalcotter/Dropbox/Papers/Mallat_1989_A theory for multiresolution signal decomposition_2.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/8Q5NEGEV/abs_all.html:text/html},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@inproceedings{felix_grun_taxonomy_2016,
  address = {Ney York, NY, USA},
  title = {A {{Taxonomy}} and {{Library}} for {{Visualizing Learned Features}} in {{Convolutional Neural Networks}}},
  volume = {48},
  abstract = {Over the last decade, Convolutional Neural Networks
(CNN) saw a tremendous surge in performance.
However, understanding what a network
has learned still proves to be a challenging task.
To remedy this unsatisfactory situation, a number
of groups have recently proposed different methods
to visualize the learned models. In this work
we suggest a general taxonomy to classify and
compare these methods, subdividing the literature
into three main categories and providing researchers
with a terminology to base their works
on. Furthermore, we introduce the FeatureVis
library for MatConvNet: an extendable, easy to
use open source library for visualizing CNNs. It
contains implementations from each of the three
main classes of visualization methods and serves
as a useful tool for an enhanced understanding
of the features learned by intermediate layers, as
well as for the analysis of why a network might
fail for certain examples.},
  timestamp = {2016-07-27T13:03:07Z},
  author = {{Felix Gr{\"u}n} and {Christian Rupprecht} and {Nassir Navab} and {Federico Tombari}},
  month = jun,
  year = {2016},
  note = {00000},
  file = {Felix Grün et al_2016_A Taxonomy and Library for Visualizing Learned Features in Convolutional Neural.pdf:/Users/fergalcotter/Dropbox/Papers/Felix Grün et al_2016_A Taxonomy and Library for Visualizing Learned Features in Convolutional Neural.pdf:application/pdf},
  groups = {ICML Viz,ICML Viz}
}

@article{keysers_deformation_2007,
  title = {Deformation {{Models}} for {{Image Recognition}}},
  volume = {29},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2007.1153},
  abstract = {We present the application of different nonlinear image deformation models to the task of image recognition. The deformation models are especially suited for local changes as they often occur in the presence of image object variability. We show that, among the discussed models, there is one approach that combines simplicity of implementation, low-computational complexity, and highly competitive performance across various real-world image recognition tasks. We show experimentally that the model performs very well for four different handwritten digit recognition tasks and for the classification of medical images, thus showing high generalization capacity. In particular, an error rate of 0.54 percent on the MNIST benchmark is achieved, as well as the lowest reported error rate, specifically 12.6 percent, in the 2005 international ImageCLEF evaluation of medical image specifically categorization.},
  timestamp = {2016-02-11T00:07:29Z},
  number = {8},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Keysers, D. and Deselaers, T. and Gollan, C. and Ney, H.},
  month = aug,
  year = {2007},
  note = {00152},
  keywords = {Algorithms,Artificial Intelligence,Biomedical imaging,Character recognition,Classification algorithms,Computer Simulation,Context modeling,Deformable models,Error analysis,Handwriting recognition,handwritten character recognition,handwritten digit recognition,Humans,image alignment,Image Interpretation; Computer-Assisted,image matching,image object variability,Image Processing; Computer-Assisted,Image recognition,low-computational complexity,medical image categorization,medical image categorization.,medical image processing,medical images classification,MNIST benchmark,Nonlinear Dynamics,nonlinear image deformation,Pattern Recognition; Automated,Pixel,Useful},
  pages = {1422--1435},
  file = {Keysers et al_2007_Deformation Models for Image Recognition.pdf:/Users/fergalcotter/Dropbox/Papers/Keysers et al_2007_Deformation Models for Image Recognition.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/2ZAQTWHQ/abs_all.html:text/html},
  groups = {Miscellaneous,Miscellaneous}
}

@article{lin_microsoft_2014,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  timestamp = {2016-09-13T11:33:07Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1405.0312},
  primaryClass = {cs},
  urldate = {2016-08-13},
  journal = {arXiv:1405.0312 [cs]},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll{\'a}r, Piotr},
  month = may,
  year = {2014},
  note = {00500},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote = {Comment: 1) updated annotation pipeline description and figures; 2) added new section describing datasets splits; 3) updated author list},
  file = {Lin et al_2014_Microsoft COCO.pdf:/Users/fergalcotter/Dropbox/Papers/Lin et al_2014_Microsoft COCO.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/ABP85MW6/1405.html:text/html},
  groups = {CNNs,CNNs}
}

@techreport{rahtz_hyperref_2012,
  title = {Hyperref - {{Hypertext}} Marks in {{Latex}}},
  timestamp = {2016-09-13T11:33:08Z},
  author = {Rahtz, Sebastian and Oberdiek, Heiko},
  month = nov,
  year = {2012},
  note = {00013},
  file = {Rahtz_Oberdiek_2012_Hyperref - Hypertext marks in Latex_3.pdf:/Users/fergalcotter/Dropbox/Papers/Rahtz_Oberdiek_2012_Hyperref - Hypertext marks in Latex_3.pdf:application/pdf;Rahtz_Oberdiek_2012_Hyperref - Hypertext marks in Latex_4.pdf:/Users/fergalcotter/Dropbox/Papers/Rahtz_Oberdiek_2012_Hyperref - Hypertext marks in Latex_4.pdf:application/pdf;Rahtz_Oberdiek_2012_Hyperref - Hypertext marks in Latex.pdf:/Users/fergalcotter/Dropbox/Papers/Rahtz_Oberdiek_2012_Hyperref - Hypertext marks in Latex.pdf:application/pdf},
  groups = {Latex,Latex}
}

@article{zhou_learning_2015,
  title = {Learning {{Deep Features}} for {{Discriminative Localization}}},
  abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2\% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them},
  timestamp = {2016-09-30T13:07:30Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.04150},
  primaryClass = {cs},
  urldate = {2016-09-26},
  journal = {arXiv:1512.04150 [cs]},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  month = dec,
  year = {2015},
  note = {00012},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Zhou et al_2015_Learning Deep Features for Discriminative Localization.pdf:/Users/fergalcotter/Dropbox/Papers/Zhou et al_2015_Learning Deep Features for Discriminative Localization.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/377G7ZCJ/1512.html:text/html},
  groups = {Visualization \& Generative,Visualization \& Generative,Visualization \& Generative}
}

@incollection{orr_introduction_2012,
  series = {Lecture Notes in Computer Science},
  title = {Introduction},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {It is our belief that researchers and practitioners acquire, through experience and word-of-mouth, techniques and heuristics that help them successfully apply neural networks to difficult real world problems. Often these ``tricks'' are theoretically well motivated. Sometimes they are the result of trial and error. However, their most common link is that they are usually hidden in people's heads or in the back pages of space-constrained conference papers. As a result newcomers to the field waste much time wondering why their networks train so slowly and perform so poorly. This book is an outgrowth of a 1996 NIPS workshop called Tricks of the Trade whose goal was to begin the process of gathering and documenting these tricks. The interest that the workshop generated, motivated us to expand our collection and compile it into this book. Although we have no doubt that there are many tricks we have missed, we hope that what we have included will prove to be useful, particularly to those who are relatively new to the field. Each chapter contains one or more tricks presented by a given author (or authors). We have attempted to group related chapters into sections, though we recognize that the different sections are far from disjoint. Some of the chapters (e.g. 1,13,17) contain entire systems of tricks that are far more general than the category they have been placed in. Before each section we provide the reader with a summary of the tricks contained within, to serve as a quick overview and reference. However, we do not recommend applying tricks before having read the accompanying chapter. Each trick may only work in a particular context that is not fully explained in the summary. This is particularly true for the chapters that present systems where combinations of tricks must be applied together for them to be effective. Below we give a coarse roadmap of the contents of the individual chapters.},
  language = {en},
  timestamp = {2016-08-09T23:22:58Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00801},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {1--5},
  file = {Orr_Müller_2012_Introduction.pdf:/Users/fergalcotter/Dropbox/Papers/Orr_Müller_2012_Introduction.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/TISNCPPH/978-3-642-35289-8_1.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_1}
}

@article{mallat_understanding_2016,
  title = {Understanding {{Deep Convolutional Networks}}},
  abstract = {Deep convolutional networks provide state of the art classifications and regressions results over many high-dimensional problems. We review their architecture, which scatters data with a cascade of linear filter weights and non-linearities. A mathematical framework is introduced to analyze their properties. Computations of invariants involve multiscale contractions, the linearization of hierarchical symmetries, and sparse separations. Applications are discussed.},
  timestamp = {2016-05-06T12:38:33Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1601.04920},
  primaryClass = {cs, stat},
  urldate = {2016-03-02},
  journal = {arXiv:1601.04920 [cs, stat]},
  author = {Mallat, St{\'e}phane},
  month = jan,
  year = {2016},
  note = {00003},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Statistics - Machine Learning},
  annote = {Comment: 17 pages, 4 Figures},
  file = {Mallat_2016_Understanding Deep Convolutional Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Mallat_2016_Understanding Deep Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/DPE3QHF2/1601.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@article{tariyal_greedy_2016,
  title = {Greedy {{Deep Dictionary Learning}}},
  abstract = {In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVD. Our method yields better results than all.},
  timestamp = {2016-07-29T14:44:27Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.00203},
  primaryClass = {cs, stat},
  urldate = {2016-07-15},
  journal = {arXiv:1602.00203 [cs, stat]},
  author = {Tariyal, Snigdha and Majumdar, Angshul and Singh, Richa and Vatsa, Mayank},
  month = jan,
  year = {2016},
  note = {00002},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Statistics - Machine Learning},
  file = {Tariyal et al_2016_Greedy Deep Dictionary Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Tariyal et al_2016_Greedy Deep Dictionary Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/RGXUMD48/1602.html:text/html},
  groups = {Unsupervised Methods,Unsupervised Methods}
}

@article{ruderman_statistics_1994-1,
  title = {The Statistics of Natural Images},
  volume = {5},
  issn = {0954-898X},
  doi = {10.1088/0954-898X_5_4_006},
  abstract = {Recently there has been a resurgence of interest in the properties of natural images. Their statistics are important not only in image compression but also for the study of sensory processing in biology, which can be viewed as satisfying certain `design criteria'. This review summarizes previous work on image statistics and presents our own data. Perhaps the most notable property of natural images is an invariance to scale. We present data to support this claim as well as evidence for a hierarchical invariance in natural scenes. These symmetries provide a powerful description of natural images as they greatly restrict the class of allowed distributions.},
  timestamp = {2016-10-25T17:10:36Z},
  number = {4},
  urldate = {2016-10-25},
  journal = {Network: Computation in Neural Systems},
  author = {Ruderman, Daniel L},
  month = jan,
  year = {1994},
  pages = {517--548},
  file = {Ruderman_1994_The statistics of natural images.pdf:/Users/fergalcotter/Dropbox/Papers/Ruderman_1994_The statistics of natural images.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/JP673H8P/0954-898X_5_4_006.html:text/html},
  groups = {Statistical Methods,Statistical Methods}
}

@inproceedings{geoff_hinton_what_2005,
  title = {What Kind of a Graphical Model Is the Brain?},
  abstract = {If neurons are treated as latent variables, our visual
systems are non-linear, densely-connected
graphical models containing billions of variables
and thousands of billions of parameters. Current
algorithms would have difficulty learning a
graphical model of this scale. Starting with an
algorithm that has difficulty learning more than
a few thousand parameters, I describe a series
of progressively better learning algorithms all of
which are designed to run on neuron-like hardware.
The latest member of this series can learn
deep, multi-layer belief nets quite rapidly. It turns
a generic network with three hidden layers and
1.7 million connections into a very good generative
model of handwritten digits. After learning,
the model gives classification performance that is
comparable to the best discriminative methods.},
  timestamp = {2016-07-28T17:50:43Z},
  author = {{Geoff Hinton}},
  year = {2005},
  note = {00000},
  pages = {1765--1775},
  groups = {Unsupervised Methods,Unsupervised Methods}
}

@incollection{shah_double_2015,
  title = {Double or {{Nothing}}: {{Multiplicative Incentive Mechanisms}} for {{Crowdsourcing}}},
  shorttitle = {Double or {{Nothing}}},
  timestamp = {2016-01-14T13:57:12Z},
  urldate = {2016-01-14},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  publisher = {{Curran Associates, Inc.}},
  author = {Shah, Nihar Bhadresh and Zhou, Denny},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  note = {00004},
  pages = {1--9},
  file = {Shah_Zhou_2015_Double or Nothing.pdf:/Users/fergalcotter/Dropbox/Papers/Shah_Zhou_2015_Double or Nothing.pdf:application/pdf;NIPS Snapshort:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/2BIS2H9F/5677-double-or-nothing-multiplicative-incentive-mechanisms-for-crowdsourcing.html:text/html},
  groups = {Miscellaneous,Miscellaneous}
}

@article{hsu_comparison_2002,
  title = {A Comparison of Methods for Multiclass Support Vector Machines},
  volume = {13},
  issn = {1045-9227},
  doi = {10.1109/72.991427},
  abstract = {Support vector machines (SVMs) were originally designed for binary classification. How to effectively extend it for multiclass classification is still an ongoing research issue. Several methods have been proposed where typically we construct a multiclass classifier by combining several binary classifiers. Some authors also proposed methods that consider all classes at once. As it is computationally more expensive to solve multiclass problems, comparisons of these methods using large-scale problems have not been seriously conducted. Especially for methods solving multiclass SVM in one step, a much larger optimization problem is required so up to now experiments are limited to small data sets. In this paper we give decomposition implementations for two such "all-together" methods. We then compare their performance with three methods based on binary classifications: "one-against-all," "one-against-one," and directed acyclic graph SVM (DAGSVM). Our experiments indicate that the "one-against-one" and DAG methods are more suitable for practical use than the other methods. Results also show that for large problems methods by considering all data at once in general need fewer support vectors},
  timestamp = {2016-09-30T13:07:31Z},
  number = {2},
  journal = {IEEE Transactions on Neural Networks},
  author = {Hsu, Chih-Wei and Lin, Chih-Jen},
  month = mar,
  year = {2002},
  note = {06170},
  keywords = {binary classifiers,Computer science,DAGSVM,decomposition,directed acyclic graph SVM,Large-scale systems,learning automata,multiclass classification,optimization,Optimization methods,pattern classification,Support vector machine classification,support vector machines,SVMs,Training data},
  pages = {415--425},
  file = {Hsu_Lin_2002_A comparison of methods for multiclass support vector machines.pdf:/Users/fergalcotter/Dropbox/Papers/Hsu_Lin_2002_A comparison of methods for multiclass support vector machines.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/S73RA9K6/991427.html:text/html},
  groups = {Other Networks,Other Networks}
}

@techreport{gibson_theory_????,
  title = {The Theory of Affordances},
  timestamp = {2016-09-16T18:47:57Z},
  author = {Gibson, James},
  note = {00023},
  file = {Gibson_The theory of affordances.pdf:/Users/fergalcotter/Dropbox/Papers/Gibson_The theory of affordances.pdf:application/pdf},
  groups = {ICVSS,ICVSS}
}

@inproceedings{anderson_coarse-level_2005,
  title = {Coarse-Level Object Recognition Using Interlevel Products of Complex Wavelets},
  volume = {1},
  timestamp = {2015-11-19T14:13:36Z},
  urldate = {2015-11-03},
  booktitle = {Image {{Processing}}, 2005. {{ICIP}} 2005. {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  author = {Anderson, Ryan and Kingsbury, Nick and Fauqueur, Julien},
  year = {2005},
  note = {00024},
  pages = {I--745},
  file = {Anderson et al_2005_Coarse-level object recognition using interlevel products of complex wavelets.pdf:/Users/fergalcotter/Dropbox/Papers/Anderson et al_2005_Coarse-level object recognition using interlevel products of complex wavelets.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@inproceedings{fei-fei_learning_2004,
  title = {Learning {{Generative Visual Models}} from {{Few Training Examples}}: {{An Incremental Bayesian Approach Tested}} on 101 {{Object Categories}}},
  shorttitle = {Learning {{Generative Visual Models}} from {{Few Training Examples}}},
  doi = {10.1109/CVPR.2004.109},
  abstract = {Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum-likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets.},
  timestamp = {2016-09-13T11:33:09Z},
  booktitle = {Conference on {{Computer Vision}} and {{Pattern Recognition Workshop}}, 2004. {{CVPRW}} '04},
  author = {Fei-Fei, Li and Fergus, R. and Perona, P.},
  month = jun,
  year = {2004},
  note = {00989},
  keywords = {Assembly,Bayesian methods,Humans,Image databases,Image recognition,Machine vision,Maximum likelihood estimation,Parameter estimation,Shape,Testing},
  pages = {178--178},
  file = {Fei-Fei et al_2004_Learning Generative Visual Models from Few Training Examples.pdf:/Users/fergalcotter/Dropbox/Papers/Fei-Fei et al_2004_Learning Generative Visual Models from Few Training Examples.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/IE6MBB2J/abs_all.html:text/html},
  groups = {datasets,datasets,datasets}
}

@article{kendall_posenet:_2015,
  title = {{{PoseNet}}: {{A Convolutional Network}} for {{Real}}-{{Time}} 6-{{DOF Camera Relocalization}}},
  volume = {2015},
  shorttitle = {{{PoseNet}}},
  abstract = {We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 3 degree accuracy for large scale outdoor scenes and 0.5m and 5 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples. The dataset and an online demonstration is available on our project webpage, at http://mi.eng.cam.ac.uk/projects/relocalisation/},
  timestamp = {2015-12-24T15:18:24Z},
  urldate = {2015-12-01},
  journal = {International Conference on Computer Vision (ICCV)},
  author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
  month = may,
  year = {2015},
  note = {00000},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Unread},
  annote = {Comment: 9 pages, 13 figures; Final ICCV 2015 Revision},
  file = {Kendall et al. - 2015 - PoseNet A Convolutional Network for Real-Time 6-D.pdf:/Users/fergalcotter/Dropbox/Papers/Kendall et al. - 2015 - PoseNet A Convolutional Network for Real-Time 6-D.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/4M3B8JSP/1505.html:text/html},
  groups = {CNNs,CNNs}
}

@article{selesnick_dual-tree_2005,
  title = {The Dual-Tree Complex Wavelet Transform},
  volume = {22},
  timestamp = {2016-07-31T19:43:47Z},
  number = {6},
  urldate = {2015-11-03},
  journal = {Signal Processing Magazine, IEEE},
  author = {Selesnick, Ivan W. and Baraniuk, Richard G. and Kingsbury, Nick G.},
  year = {2005},
  note = {01602},
  pages = {123--151},
  annote = {The pretty paper. Great reference},
  file = {Selesnick et al_2005_The dual-tree complex wavelet transform.pdf:/Users/fergalcotter/Dropbox/Papers/Selesnick et al_2005_The dual-tree complex wavelet transform.pdf:application/pdf},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@article{wu_performance_2014,
  title = {Performance Evaluation of Wavelet Scattering Network in Image Texture Classification in Various Color Spaces},
  abstract = {Texture plays an important role in many image analysis applications. In this paper, we give a performance evaluation of color texture classification by performing wavelet scattering network in various color spaces. Experimental results on the KTH\_TIPS\_COL database show that opponent RGB based wavelet scattering network outperforms other color spaces. Therefore, when dealing with the problem of color texture classification, opponent RGB based wavelet scattering network is recommended.},
  timestamp = {2016-02-01T12:57:44Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1407.6423},
  primaryClass = {cs},
  urldate = {2016-02-01},
  journal = {arXiv:1407.6423 [cs]},
  author = {Wu, Jiasong and Jiang, Longyu and Han, Xu and Senhadji, Lotfi and Shu, Huazhong},
  month = jul,
  year = {2014},
  note = {00000},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Similar Work,Unread},
  annote = {Comment: 6 pages, 4 figures, 2 tables},
  file = {Wu et al_2014_Performance evaluation of wavelet scattering network in image texture.pdf:/Users/fergalcotter/Dropbox/Papers/Wu et al_2014_Performance evaluation of wavelet scattering network in image texture.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/VKVTQNTZ/1407.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@article{szegedy_intriguing_2013,
  title = {Intriguing Properties of Neural Networks},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  timestamp = {2016-09-13T11:33:10Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.6199},
  primaryClass = {cs},
  urldate = {2016-08-24},
  journal = {arXiv:1312.6199 [cs]},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  month = dec,
  year = {2013},
  note = {00223},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Szegedy et al_2013_Intriguing properties of neural networks.pdf:/Users/fergalcotter/Dropbox/Papers/Szegedy et al_2013_Intriguing properties of neural networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/PAHW52QC/1312.html:text/html},
  groups = {criticisms,criticisms,criticisms}
}

@article{tygert_convolutional_2015,
  title = {Convolutional Networks and Learning Invariant to Homogeneous Multiplicative Scalings},
  abstract = {The conventional classification schemes -- notably multinomial logistic regression -- used in conjunction with convolutional networks (convnets) are classical in statistics, designed without consideration for the usual coupling with convnets, stochastic gradient descent, and backpropagation. In the specific application to supervised learning for convnets, a simple scale-invariant classification stage turns out to be more robust than multinomial logistic regression, appears to result in slightly lower errors on several standard test sets, has similar computational costs, and features precise control over the actual rate of learning. "Scale-invariant" means that multiplying the input values by any nonzero scalar leaves the output unchanged.},
  timestamp = {2016-02-07T23:26:11Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.08230},
  primaryClass = {cs},
  urldate = {2016-02-07},
  journal = {arXiv:1506.08230 [cs]},
  author = {Tygert, Mark and Szlam, Arthur and Chintala, Soumith and Ranzato, Marc'Aurelio and Tian, Yuandong and Zaremba, Wojciech},
  month = jun,
  year = {2015},
  note = {00000},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: 12 pages, 6 figures, 4 tables (ICLR reviewers need only consider the first 9 pages -- the remaining pages are appendices)},
  file = {Tygert et al_2015_Convolutional networks and learning invariant to homogeneous multiplicative.pdf:/Users/fergalcotter/Dropbox/Papers/Tygert et al_2015_Convolutional networks and learning invariant to homogeneous multiplicative.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/VDA8AIHF/1506.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@article{girshick_rich_2014,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  abstract = {Object detection performance, as measured on the
canonical PASCAL VOC dataset, has plateaued in the last
few years. The best-performing methods are complex ensemble
systems that typically combine multiple low-level
image features with high-level context. In this paper, we
propose a simple and scalable detection algorithm that improves
mean average precision (mAP) by more than 30\%
relative to the previous best result on VOC 2012\textemdash{}achieving
a mAP of 53.3\%. Our approach combines two key insights:
(1) one can apply high-capacity convolutional neural networks
(CNNs) to bottom-up region proposals in order to
localize and segment objects and (2) when labeled training
data is scarce, supervised pre-training for an auxiliary task,
followed by domain-specific fine-tuning, yields a significant
performance boost. Since we combine region proposals
with CNNs, we call our method R-CNN: Regions with CNN
features. Source code for the complete system is available at
http://www.cs.berkeley.edu/\texttildelow{}rbg/rcnn.},
  timestamp = {2016-09-16T18:33:57Z},
  journal = {Computer Vision and Pattern Recognition (CVPR)},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = {2014},
  note = {00933},
  keywords = {Unread},
  file = {Girshick et al_2014_Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:/Users/fergalcotter/Dropbox/Papers/Girshick et al_2014_Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:application/pdf},
  groups = {CNNs,CNNs}
}

@phdthesis{sifre_rigid-motion_2014-1,
  title = {Rigid-{{Motion Scattering}} for {{Image Classification}}},
  abstract = {Image classification is the problem of assigning a label that best describes the content of
unknown images, given a set of training images with known labels. This thesis introduces
image classification algorithms based on the scattering transform, studies their properties
and describes extensive classification experiments on challenging texture and object image
datasets.
Images are high dimensional signals for which generic machine learning algorithms fail
when applied directly on the raw pixel space. Therefore, most successful approaches involve
building a specific low dimensional representation on which the classification is performed.
Traditionally, the representation was engineered to reduce the dimensionality of images by
building invariance to geometric transformations while retaining discriminative features.
More recently, deep convolutional networks have achieved state-of-the-art results on most
image classification tasks. Such networks progressively build more invariant representations
through a hierarchy of convolutional layers where all the weights are learned.
This thesis proposes several scattering representations. Those scattering representations
have a structure similar to convolutional networks, but the weights of scattering are
designed to provide mathematical guaranty of invariance to geometric transformations, stability
to deformations and energy preservation. In this thesis, we focus on affine and more
specifically on rigid-motion transformations, which consist in translations and rotations,
and which are common in real world images.
Translation scattering is a cascade of two dimensional wavelet modulus operators which
builds translation invariance. We propose a first separable rigid-motion separable scattering,
which applies a first scattering along the position variable to build translation invariance,
followed by a second scattering transform along the rotational orbits of the first
scattering, to build invariance to rotations.
As any separable representation, separable scattering has the advantage of simplicity
but also loses some information about the joint distribution of positions and orientations
in the intermediate layers of the representation. We define a joint rigid-motion scattering
which does retain this information. The joint scattering consists in a cascade of wavelet
modulus applied directly on the joint rigid-motion group. We introduce convolutions,
wavelets, a wavelet transform and scattering on the rigid-motion group and propose fast
implementations. Both separable and joint scattering are applied to texture image classi-
fication with state-of-the-art results on most available texture datasets.
Finally, we demonstrate the applicability of joint scattering and group convolutions
on generic object image datasets. It is shown that convolutional networks performances
are enhanced through the use of separable convolutions, similar to the rigid-motion convolutions.
Also, a non-invariant version of the rigid-motion scattering is demonstrated to
achieve results similar to those obtained by the first layers of convolutional networks.},
  timestamp = {2016-01-25T17:32:46Z},
  school = {Ecole Polytechnique},
  author = {Sifre, Laurent},
  month = oct,
  year = {2014},
  note = {00000},
  keywords = {Unread},
  file = {Sifre_2014_Rigid-Motion Scattering for Image Classification.pdf:/Users/fergalcotter/Dropbox/Papers/Sifre_2014_Rigid-Motion Scattering for Image Classification.pdf:application/pdf},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@article{sutskever_importance_2013,
  title = {On the Importance of Initialization and Momentum in Deep Learning},
  abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this...},
  timestamp = {2016-08-07T13:04:00Z},
  number = {3},
  urldate = {2016-08-07},
  journal = {ResearchGate},
  author = {Sutskever, I. and Martens, J. and Dahl, G. and Hinton, G.},
  month = jan,
  year = {2013},
  note = {00337},
  pages = {1139--1147},
  file = {Sutskever et al_2013_On the importance of initialization and momentum in deep learning.pdf:/Users/fergalcotter/Dropbox/Papers/Sutskever et al_2013_On the importance of initialization and momentum in deep learning.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/UP78SIPZ/286271944_On_the_importance_of_initialization_and_momentum_in_deep_learning.html:text/html},
  groups = {network features,network features,network features}
}

@article{zeiler_stochastic_2013,
  title = {Stochastic {{Pooling}} for {{Regularization}} of {{Deep Convolutional Neural Networks}}},
  abstract = {We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.},
  timestamp = {2016-08-09T11:46:12Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.3557},
  primaryClass = {cs, stat},
  urldate = {2016-08-09},
  journal = {arXiv:1301.3557 [cs, stat]},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  month = jan,
  year = {2013},
  note = {00204},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annote = {Comment: 9 pages},
  file = {Zeiler_Fergus_2013_Stochastic Pooling for Regularization of Deep Convolutional Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Zeiler_Fergus_2013_Stochastic Pooling for Regularization of Deep Convolutional Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/6UA5KHCV/1301.html:text/html},
  groups = {network features,network features,network features}
}

@article{miller_image_2008,
  title = {Image Denoising Using Derotated Complex Wavelet Coefficients},
  volume = {17},
  issn = {1057-7149},
  doi = {10.1109/TIP.2008.926146},
  abstract = {A method for removing additive Gaussian noise from digital images is described. It is based on statistical modeling of the coefficients of a redundant, oriented, complex multiscale transform. Two types of modeling are used to model the wavelet coefficients. Both are based on Gaussian scale mixture (GSM) modeling of neighborhoods of coefficients at adjacent locations and scales. Modeling of edge and ridge discontinuities is performed using wavelet coefficients derotated by twice the phase of the coefficient at the same location and the next coarser scale. Other areas are modeled using standard wavelet coefficients. An adaptive Bayesian model selection framework is used to determine the modeling applied to each neighborhood. The proposed algorithm succeeds in providing improved denoising performance at structural image features, reducing ringing artifacts and enhancing sharpness, while avoiding degradation in other areas. The method outperforms previously published methods visually and in standard tests.},
  language = {eng},
  timestamp = {2016-08-05T01:29:54Z},
  number = {9},
  journal = {IEEE transactions on image processing: a publication of the IEEE Signal Processing Society},
  author = {Miller, Mark and Kingsbury, Nick},
  month = sep,
  year = {2008},
  note = {00069},
  keywords = {Algorithms,Artifacts,Computer Simulation,Image Enhancement,Image Interpretation; Computer-Assisted,Models; Statistical,Normal Distribution,Reproducibility of Results,rotation,Sensitivity and Specificity},
  pages = {1500--1511},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications},
  pmid = {18701390}
}

@article{kingsbury_complex_2001,
  title = {Complex Wavelets for Shift Invariant Analysis and Filtering of Signals},
  volume = {10},
  doi = {10.1006/acha.2000.0343},
  abstract = {This paper describes a form of discrete wavelet transform, which generates complex coefficients by using a dual tree of wavelet filters to obtain their real and imaginary parts. This introduces limited redundancy (2(m) : 1 for m-dimensional signals) and allows the transform to provide approximate shift invariance and directionally selective filters (properties lacking in the traditional wavelet transform) while preserving the usual properties of perfect reconstruction and computational efficiency with good well-balanced frequency responses. Here we analyze why the new transform can be designed to be shift invariant and describe how to estimate the accuracy of this approximation and design suitable filters to achieve this. We discuss two different variants of the new transform, based on odd/even and quarter-sample shift (Q-shift) filters, respectively. We then describe briefly how the dual tree may be extended for images and other multi-dimensional signals, and finally summarize a range of applications of the transform that take advantage of its unique properties. (C) 2001 Academic Press.},
  timestamp = {2016-01-20T12:28:26Z},
  number = {3},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Kingsbury, N.},
  month = may,
  year = {2001},
  note = {01524},
  pages = {234--253},
  file = {Kingsbury_2001_Complex wavelets for shift invariant analysis and filtering of signals.pdf:/Users/fergalcotter/Dropbox/Papers/Kingsbury_2001_Complex wavelets for shift invariant analysis and filtering of signals.pdf:application/pdf},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@article{springenberg_striving_2014-1,
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  shorttitle = {Striving for {{Simplicity}}},
  abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
  timestamp = {2016-08-09T11:44:59Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6806},
  primaryClass = {cs},
  urldate = {2016-08-09},
  journal = {arXiv:1412.6806 [cs]},
  author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  month = dec,
  year = {2014},
  note = {00084},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {paper about getting rid of max pooling
~},
  file = {Springenberg et al_2014_Striving for Simplicity.pdf:/Users/fergalcotter/Dropbox/Papers/Springenberg et al_2014_Striving for Simplicity.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/7QA6UFET/1412.html:text/html},
  groups = {network features,network features,network features}
}

@book{simon_haykin_neural_2009,
  edition = {3},
  title = {Neural {{Networks}} and {{Learning Machines}}},
  isbn = {978-0-13-147139-9},
  timestamp = {2016-08-10T08:48:27Z},
  publisher = {{Pearson Prentice Hall}},
  author = {{Simon Haykin}},
  year = {2009},
  note = {03340},
  file = {[Simon_O._Haykin]_Neural_Networks_and_Learning_Mac(BookZZ.org).pdf:/Users/fergalcotter/Dropbox/Papers/Books/[Simon_O._Haykin]_Neural_Networks_and_Learning_Mac(BookZZ.org).pdf:application/pdf},
  groups = {Books}
}

@article{citti_cortical_2006,
  title = {A {{Cortical Based Model}} of {{Perceptual Completion}} in the {{Roto}}-{{Translation Space}}},
  volume = {24},
  issn = {0924-9907, 1573-7683},
  doi = {10.1007/s10851-005-3630-2},
  abstract = {We present a mathematical model of perceptual completion and formation of subjective surfaces, which is at the same time inspired by the architecture of the visual cortex, and is the lifting in the 3-dimensional rototranslation group of the phenomenological variational models based on elastica functional. The initial image is lifted by the simple cells to a surface in the rototranslation group and the completion process is modeled via a diffusion driven motion by curvature. The convergence of the motion to a minimal surface is proved. Results are presented both for modal and amodal completion in classic Kanizsa images.},
  language = {en},
  timestamp = {2016-08-03T11:12:49Z},
  number = {3},
  urldate = {2016-08-03},
  journal = {Journal of Mathematical Imaging and Vision},
  author = {Citti, G. and Sarti, A.},
  month = feb,
  year = {2006},
  note = {00224},
  pages = {307--326},
  file = {Citti_Sarti_2006_A Cortical Based Model of Perceptual Completion in the Roto-Translation Space.pdf:/Users/fergalcotter/Dropbox/Papers/Citti_Sarti_2006_A Cortical Based Model of Perceptual Completion in the Roto-Translation Space.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/FTTFS74H/s10851-005-3630-2.html:text/html},
  groups = {Background,Background,Background}
}

@incollection{flake_square_2012,
  series = {Lecture Notes in Computer Science},
  title = {Square {{Unit Augmented}}, {{Radially Extended}}, {{Multilayer Perceptrons}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {Consider a multilayer perceptron (MLP) with d inputs, a single hidden sigmoidal layer and a linear output. By adding an additional d inputs to the network with values set to the square of the first d inputs, properties reminiscent of higher-order neural networks and radial basis function networks (RBFN) are added to the architecture with little added expense in terms of weight requirements. Of particular interest, this architecture has the ability to form localized features in a d-dimensional space with a single hidden node but can also span large volumes of the input space; thus, the architecture has the localized properties of an RBFN but does not suffer as badly from the curse of dimensionality. I refer to a network of this type as a SQuare Unit Augmented, Radially Extended, MultiLayer Perceptron (SQUARE-MLP or SMLP).},
  language = {en},
  timestamp = {2016-08-09T23:23:36Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Flake, Gary William},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00057},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {143--161},
  file = {Flake_2012_Square Unit Augmented, Radially Extended, Multilayer Perceptrons.pdf:/Users/fergalcotter/Dropbox/Papers/Flake_2012_Square Unit Augmented, Radially Extended, Multilayer Perceptrons.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/W37U8RFM/978-3-642-35289-8_11.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_11}
}

@incollection{fritsch_applying_2012,
  series = {Lecture Notes in Computer Science},
  title = {Applying {{Divide}} and {{Conquer}} to {{Large Scale Pattern Recognition Tasks}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {Rather than presenting a specific trick, this paper aims at providing a methodology for large scale, real-world classification tasks involving thousands of classes and millions of training patterns. Such problems arise in speech recognition, handwriting recognition and speaker or writer identification, just to name a few. Given the typically very large number of classes to be distinguished, many approaches focus on parametric methods to independently estimate class conditional likelihoods. In contrast, we demonstrate how the principles of modularity and hierarchy can be applied to directly estimate posterior class probabilities in a connectionist framework. Apart from offering better discrimination capability, we argue that a hierarchical classification scheme is crucial in tackling the above mentioned problems. Furthermore, we discuss training issues that have to be addressed when an almost infinite amount of training data is available.},
  language = {en},
  timestamp = {2016-08-09T23:23:27Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Fritsch, J{\"u}rgen and Finke, Michael},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00012},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {311--338},
  file = {Fritsch_Finke_2012_Applying Divide and Conquer to Large Scale Pattern Recognition Tasks.pdf:/Users/fergalcotter/Dropbox/Papers/Fritsch_Finke_2012_Applying Divide and Conquer to Large Scale Pattern Recognition Tasks.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/3G97B542/978-3-642-35289-8_20.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_20}
}

@inproceedings{fauqueur_multiscale_2006,
  title = {Multiscale Keypoint Detection Using the Dual-Tree Complex Wavelet Transform},
  timestamp = {2016-01-14T14:02:31Z},
  urldate = {2015-11-03},
  booktitle = {Image {{Processing}}, 2006 {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  author = {Fauqueur, Julien and Kingsbury, Nick and Anderson, Richard},
  year = {2006},
  note = {00060},
  pages = {1625--1628},
  file = {Multiscale keypoint detection using the dual-tree complex wavelet transform.pdf:/Users/fergalcotter/Dropbox/Papers/Multiscale keypoint detection using the dual-tree complex wavelet transform.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@article{he_identity_2016,
  title = {Identity {{Mappings}} in {{Deep Residual Networks}}},
  abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
  timestamp = {2016-08-07T12:15:41Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.05027},
  primaryClass = {cs},
  urldate = {2016-08-07},
  journal = {arXiv:1603.05027 [cs]},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = mar,
  year = {2016},
  note = {00021},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  annote = {Comment: ECCV 2016 camera-ready},
  file = {He et al_2016_Identity Mappings in Deep Residual Networks.pdf:/Users/fergalcotter/Dropbox/Papers/He et al_2016_Identity Mappings in Deep Residual Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/5V5HMV93/1603.html:text/html},
  groups = {network features,network features,network features}
}

@inproceedings{jarrett_what_2009,
  title = {What Is the Best Multi-Stage Architecture for Object Recognition?},
  doi = {10.1109/ICCV.2009.5459469},
  abstract = {In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63\% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6\%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 ($>$ 65\%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53\%).},
  timestamp = {2016-01-14T14:02:32Z},
  booktitle = {2009 {{IEEE}} 12th {{International Conference}} on {{Computer Vision}}},
  author = {Jarrett, K. and Kavukcuoglu, K. and Ranzato, M. and LeCun, Y.},
  month = sep,
  year = {2009},
  note = {00640},
  keywords = {_tablet,Brain modeling,Caltech-101,Error analysis,Feature extraction,feature pooling layer,feature rectification,filter bank,Gabor filters,Histograms,Image edge detection,Learning systems,local contrast normalization,multistage architecture,nonlinear transformation,NORB dataset,object recognition,Refining,supervised learning,unprocessed MNIST dataset,unsupervised learning},
  pages = {2146--2153},
  annote = {A pre-turning point paper analysing multiple layers from a simpler persepective.
Talks about designing filter bank for first two stages.
Typically first stage is gradient descent, randomly picked configurations or "deep belief networks".
Their conclusions:

Using
rectifying non-linearity is the most important improving factor.
They also said using a local normalization layer improves performance.
Random filters used in two-stage systems with proper non-linearities are just as good as learned filters
2 stages are better than 1
},
  file = {IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/5APEF4F7/articleDetails.html:text/html},
  groups = {Generic Design,Generic Design,Generic Design}
}

@incollection{plate_controlling_2012,
  series = {Lecture Notes in Computer Science},
  title = {Controlling the {{Hyperparameter Search}} in {{MacKay}}'s {{Bayesian Neural Network Framework}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {In order to achieve good generalization with neural networks overfitting must be controlled. Weight penalty factors are one common method of providing this control. However, using weight penalties creates the additional search problem of finding the optimal penalty factors. MacKay [5] proposed an approximate Bayesian framework for training neural networks, in which penalty factors are treated as hyperparameters and found in an iterative search. However, for classification networks trained with cross-entropy error, this search is slow and unstable, and it is not obvious how to improve it. This paper describes and compares several strategies for controlling this search. Some of these strategies greatly improve the speed and stability of the search. Test runs on a range of tasks are described.},
  language = {en},
  timestamp = {2016-08-09T23:23:32Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Plate, Tony},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00006},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {91--110},
  file = {Plate_2012_Controlling the Hyperparameter Search in MacKay’s Bayesian Neural Network.pdf:/Users/fergalcotter/Dropbox/Papers/Plate_2012_Controlling the Hyperparameter Search in MacKay’s Bayesian Neural Network.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/BK32DQZ3/978-3-642-35289-8_7.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_7}
}

@techreport{patrick_daly_natbib_2013,
  title = {Natbib - {{Natural Sciences Citations}} and {{References}}},
  timestamp = {2016-08-10T08:48:28Z},
  author = {{Patrick Daly}},
  month = sep,
  year = {2013},
  note = {00002},
  file = {Patrick Daly_2013_Natbib - Natural Sciences Citations and References.pdf:/Users/fergalcotter/Dropbox/Papers/Patrick Daly_2013_Natbib - Natural Sciences Citations and References.pdf:application/pdf},
  groups = {Latex,Latex}
}

@article{chatfield_return_2014,
  title = {Return of the {{Devil}} in the {{Details}}: {{Delving Deep}} into {{Convolutional Nets}}},
  shorttitle = {Return of the {{Devil}} in the {{Details}}},
  abstract = {The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.},
  timestamp = {2016-08-07T16:20:26Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1405.3531},
  primaryClass = {cs},
  urldate = {2016-08-07},
  journal = {arXiv:1405.3531 [cs]},
  author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  month = may,
  year = {2014},
  note = {00495},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote = {Comment: Published in proceedings of BMVC 2014},
  file = {Chatfield et al_2014_Return of the Devil in the Details.pdf:/Users/fergalcotter/Dropbox/Papers/Chatfield et al_2014_Return of the Devil in the Details.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/KZKPEKFA/1405.html:text/html},
  groups = {Generic Design,Generic Design,Generic Design}
}

@article{kulkarni_deep_2015,
  title = {Deep {{Convolutional Inverse Graphics Network}}},
  abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.},
  timestamp = {2016-02-02T20:11:55Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.03167},
  primaryClass = {cs},
  urldate = {2016-02-02},
  journal = {arXiv:1503.03167 [cs]},
  author = {Kulkarni, Tejas D. and Whitney, Will and Kohli, Pushmeet and Tenenbaum, Joshua B.},
  month = mar,
  year = {2015},
  note = {00020},
  keywords = {_tablet,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: First two authors contributed equally},
  file = {Kulkarni et al_2015_Deep Convolutional Inverse Graphics Network.pdf:/Users/fergalcotter/Dropbox/Papers/Kulkarni et al_2015_Deep Convolutional Inverse Graphics Network.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/IWJIMMUX/1503.html:text/html},
  groups = {Visualization \& Generative,Visualization \& Generative,Visualization \& Generative}
}

@incollection{schraudolph_centering_2012,
  series = {Lecture Notes in Computer Science},
  title = {Centering {{Neural Network Gradient Factors}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {It has long been known that neural networks can learn faster when their input and hidden unit activities are centered about zero; recently we have extended this approach to also encompass the centering of error signals [15]. Here we generalize this notion to all factors involved in the network's gradient, leading us to propose centering the slope of hidden unit activation functions as well. Slope centering removes the linear component of backpropagated error; this improves credit assignment in networks with shortcut connections. Benchmark results show that this can speed up learning significantly without adversely affecting the trained network's generalization ability.},
  language = {en},
  timestamp = {2016-08-09T23:23:35Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Schraudolph, Nicol N.},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00025},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {205--223},
  file = {Schraudolph_2012_Centering Neural Network Gradient Factors.pdf:/Users/fergalcotter/Dropbox/Papers/Schraudolph_2012_Centering Neural Network Gradient Factors.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/4DZJCA6A/978-3-642-35289-8_14.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_14}
}

@article{zhang_variational_2015,
  series = {Special Issue in Honour of William J. (Bill) Fitzgerald},
  title = {Variational {{Bayesian}} Image Restoration with Group-Sparse Modeling of Wavelet Coefficients},
  volume = {47},
  issn = {1051-2004},
  doi = {10.1016/j.dsp.2015.04.011},
  abstract = {In this work, we present a recent wavelet-based image restoration framework based on a group-sparse Gaussian scale mixture model. A hierarchical Bayesian estimation is derived using a combination of variational Bayesian inference and a subband-adaptive majorization\textendash{}minimization method that simplifies computation of the posterior distribution. We show that both of these iterative methods can converge together without needing nested loops, and thus good solutions can be found rapidly in the non-convex search space. We also integrate our method, variational Bayesian with majorization minimization (VBMM), with tree-structured modeling of the wavelet coefficients. This extension achieves significant gains in performance over the coefficient-sparse version of the algorithm. The experimental results demonstrate that the proposed method and its tree-structured extensions are effective for various imaging applications such as image deconvolution, image superresolution and compressive sensing magnetic resonance imaging (MRI) reconstruction, and that they outperform more conventional sparsity-inducing methods based on the l 1 -norm.},
  timestamp = {2016-08-05T01:23:51Z},
  urldate = {2016-08-05},
  journal = {Digital Signal Processing},
  author = {Zhang, Ganchi and Kingsbury, Nick},
  month = dec,
  year = {2015},
  note = {00001},
  keywords = {Dual-tree complex wavelets,image restoration,Majorization minimization,Variational Bayesian inference,Wavelet group-sparse modeling},
  pages = {157--168},
  file = {Zhang_Kingsbury_2015_Variational Bayesian image restoration with group-sparse modeling of wavelet.pdf:/Users/fergalcotter/Dropbox/Papers/Zhang_Kingsbury_2015_Variational Bayesian image restoration with group-sparse modeling of wavelet.pdf:application/pdf;ScienceDirect Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/BJZWPDEH/S1051200415001438.html:text/html},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@article{hinton_improving_2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
  timestamp = {2016-09-13T11:33:11Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1207.0580},
  primaryClass = {cs},
  urldate = {2016-08-22},
  journal = {arXiv:1207.0580 [cs]},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  month = jul,
  year = {2012},
  note = {01289},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Hinton et al_2012_Improving neural networks by preventing co-adaptation of feature detectors.pdf:/Users/fergalcotter/Dropbox/Papers/Hinton et al_2012_Improving neural networks by preventing co-adaptation of feature detectors.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/9TTU5NU7/1207.html:text/html},
  groups = {Generic Design,Generic Design,Generic Design}
}

@inproceedings{zhang_bayesian_2008,
  title = {A {{Bayesian}} Wavelet-Based Multidimensional Deconvolution with Sub-Band Emphasis},
  timestamp = {2015-11-19T14:13:40Z},
  urldate = {2015-11-03},
  booktitle = {Engineering in {{Medicine}} and {{Biology Society}}},
  author = {Zhang, Yingsong and Kingsbury, Nick},
  year = {2008},
  note = {00007},
  pages = {3024--3027},
  file = {Zhang_Kingsbury_2008_A Bayesian wavelet-based multidimensional deconvolution with sub-band emphasis.pdf:/Users/fergalcotter/Dropbox/Papers/Zhang_Kingsbury_2008_A Bayesian wavelet-based multidimensional deconvolution with sub-band emphasis.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@article{torralba_statistics_2003,
  title = {Statistics of Natural Image Categories},
  volume = {14},
  issn = {0954-898X},
  abstract = {In this paper we study the statistical properties of natural images belonging to different categories and their relevance for scene and object categorization tasks. We discuss how second-order statistics are correlated with image categories, scene scale and objects. We propose how scene categorization could be computed in a feedforward manner in order to provide top-down and contextual information very early in the visual processing chain. Results show how visual categorization based directly on low-level features, without grouping or segmentation stages, can benefit object localization and identification. We show how simple image statistics can be used to predict the presence and absence of objects in the scene before exploring the image.},
  language = {ENG},
  timestamp = {2016-10-25T17:05:32Z},
  number = {3},
  journal = {Network (Bristol, England)},
  author = {Torralba, Antonio and Oliva, Aude},
  month = aug,
  year = {2003},
  keywords = {Nature,Photic Stimulation,Statistics as Topic},
  pages = {391--412},
  file = {Torralba_Oliva_2003_Statistics of natural image categories.pdf:/Users/fergalcotter/Dropbox/Papers/Torralba_Oliva_2003_Statistics of natural image categories.pdf:application/pdf},
  groups = {Statistical Methods,Statistical Methods},
  pmid = {12938764}
}

@article{poggio_computational_2012,
  title = {The Computational Magic of the Ventral Stream: Sketch of a Theory (and Why Some Deep Architectures Work).},
  copyright = {http://creativecommons.org/licenses/by-nc-nd/3.0/},
  shorttitle = {The Computational Magic of the Ventral Stream},
  abstract = {This paper explores the theoretical consequences of a simple assumption: the computational goal of the feedforward path in the ventral stream -- from V1, V2, V4 and to IT -- is to discount image transformations, after learning them during development.},
  timestamp = {2016-01-14T14:02:33Z},
  urldate = {2015-11-30},
  author = {Poggio, Tomaso and Mutch, Jim and Leibo, Joel and Rosasco, Lorenzo and Tacchetti, Andrea},
  month = dec,
  year = {2012},
  note = {00018},
  keywords = {_tablet,Unread,Useful},
  annote = {A big document analysing why things work. A good introductory step.},
  file = {Poggio et al_2012_The computational magic of the ventral stream.pdf:/Users/fergalcotter/Dropbox/Papers/Poggio et al_2012_The computational magic of the ventral stream.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/G427TA9X/76248.html:text/html},
  groups = {Biological Vision,CNNs,Visualization \& Generative,Biological Vision,CNNs,Visualization \& Generative,Visualization \& Generative}
}

@article{turner_texture_????,
  title = {Texture Discrimination by {{Gabor}} Functions},
  volume = {55},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00341922},
  abstract = {A 2D Gabor filter can be realized as a sinusoidal plane wave of some frequency and orientation within a two dimensional Gaussian envelope. Its spatial extent, frequency and orientation preferences as well as bandwidths are easily controlled by the parameters used in generating the filters. However, there is an ``uncertainty relation'' associated with linear filters which limits the resolution simultaneously attainable in space and frequency. Daugman (1985) has determined that 2D Gabor filters are members of a class of functions achieving optimal joint resolution in the 2D space and 2D frequency domains. They have also been found to be a good model for two dimensional receptive fields of simple cells in the striate cortex (Jones 1985; Jones et al. 1985). The characteristic of optimal joint resolution in both space and frequency suggests that these filters are appropriate operators for tasks requiring simultaneous measurement in these domains. Texture discrimination is such a task. Computer application of a set of Gabor filters to a variety of textures found to be preattentively discriminable produces results in which differently textured regions are distinguished by firstorder differences in the values measured by the filters. This ability to reduce the statistical complexity distinguishing differently textured region as well as the sensitivity of these filters to certain types of local features suggest that Gabor functions can act as detectors of certain ``texton'' types. The performance of the computer models suggests that cortical neurons with Gabor like receptive fields may be involved in preattentive texture discrimination.},
  language = {en},
  timestamp = {2016-08-02T17:28:58Z},
  number = {2-3},
  urldate = {2016-07-31},
  journal = {Biological Cybernetics},
  author = {Turner, M. R.},
  note = {00688},
  pages = {71--82},
  file = {Turner_Texture discrimination by Gabor functions.pdf:/Users/fergalcotter/Dropbox/Papers/Turner_Texture discrimination by Gabor functions.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/3JW63WEC/BF00341922.html:text/html},
  groups = {Background,Background,Background}
}

@article{mishkin_systematic_2016,
  title = {Systematic Evaluation of {{CNN}} Advances on the {{ImageNet}}},
  abstract = {The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatibility with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is bigger than the observed improvement when all modifications are introduced, but the "deficit" is small suggesting independence of their benefits. We show that the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.},
  timestamp = {2016-08-09T11:42:21Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.02228},
  primaryClass = {cs},
  urldate = {2016-08-09},
  journal = {arXiv:1606.02228 [cs]},
  author = {Mishkin, Dmytro and Sergievskiy, Nikolay and Matas, Jiri},
  month = jun,
  year = {2016},
  note = {00000},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: Submitted to CVIU Special Issue on Deep Learning. Updated dataset quality experiment},
  file = {Mishkin et al_2016_Systematic evaluation of CNN advances on the ImageNet.pdf:/Users/fergalcotter/Dropbox/Papers/Mishkin et al_2016_Systematic evaluation of CNN advances on the ImageNet.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/HQVFDW22/1606.html:text/html},
  groups = {summary papers,summary papers,summary papers}
}

@inproceedings{zeiler_adaptive_2011,
  title = {Adaptive Deconvolutional Networks for Mid and High Level Feature Learning},
  doi = {10.1109/ICCV.2011.6126474},
  abstract = {We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling. When trained on natural images, the layers of our model capture image information in a variety of forms: low-level edges, mid-level edge junctions, high-level object parts and complete objects. To build our model we rely on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches. This makes it possible to learn multiple layers of representation and we show models with 4 layers, trained on images from the Caltech-101 and 256 datasets. When combined with a standard classifier, features extracted from these models outperform SIFT, as well as representations from other feature learning methods.},
  timestamp = {2016-01-14T14:02:34Z},
  booktitle = {2011 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zeiler, M.D. and Taylor, G.W. and Fergus, R.},
  month = nov,
  year = {2011},
  note = {00138},
  keywords = {Adaptation models,adaptive deconvolutional networks,Caltech-101 datasets,Caltech-256 datasets,classifier,complete objects,Computational modeling,convolutional sparse coding,deconvolution,Feature extraction,hierarchical model,high level feature learning,high-level object parts,image classification,image decompositions,Image reconstruction,Image representation,inference mechanisms,inference scheme,Key Paper,learning (artificial intelligence),low-level edges,Mathematical model,max pooling,mid-level edge junctions,mid level feature learning,natural images,Switches,Training},
  pages = {2018--2025},
  file = {Zeiler et al. - 2011 - Adaptive deconvolutional networks for mid and high.pdf:/Users/fergalcotter/Dropbox/Papers/Zeiler et al. - 2011 - Adaptive deconvolutional networks for mid and high.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/AR3QUZ6T/abs_all.html:text/html},
  groups = {Visualization \& Generative,Visualization \& Generative,Visualization \& Generative}
}

@book{goodfellow_deep_2016,
  title = {Deep {{Learning}}},
  timestamp = {2017-09-04T20:26:40Z},
  publisher = {{MIT Press}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  note = {Book in preparation for MIT Press},
  file = {Goodfellow et al_2016_Deep Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Goodfellow et al_2016_Deep Learning.pdf:application/pdf},
  groups = {Books}
}

@article{mackay_practical_1992,
  title = {A {{Practical Bayesian Framework}} for {{Backpropagation Networks}}},
  volume = {4},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1992.4.3.448},
  language = {en},
  timestamp = {2016-09-13T11:33:12Z},
  number = {3},
  urldate = {2016-08-24},
  journal = {Neural Computation},
  author = {MacKay, David J. C.},
  month = may,
  year = {1992},
  note = {01735},
  pages = {448--472},
  file = {MacKay_1992_A Practical Bayesian Framework for Backpropagation Networks.pdf:/Users/fergalcotter/Dropbox/Papers/MacKay_1992_A Practical Bayesian Framework for Backpropagation Networks.pdf:application/pdf},
  groups = {Bayesian Models,CNNs,Bayesian Models,Bayesian Models,CNNs}
}

@article{koren_matrix_2009,
  title = {Matrix {{Factorization Techniques}} for {{Recommender Systems}}},
  volume = {42},
  issn = {0018-9162},
  doi = {10.1109/MC.2009.263},
  abstract = {As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels.},
  timestamp = {2016-09-30T13:07:32Z},
  number = {8},
  journal = {Computer},
  author = {Koren, Y. and Bell, R. and Volinsky, C.},
  month = aug,
  year = {2009},
  note = {02675},
  keywords = {Bioinformatics,Collaboration,Computational intelligence,Filtering,Genomics,information filtering,matrix decomposition,Matrix factorization,matrix factorization technique,Motion pictures,Nearest neighbor searches,nearest neighbor technique,Netflix Prize,Netflix Prize competition,Predictive models,product recommendation system,recommender system,Recommender systems,retail data processing,Sea measurements},
  pages = {30--37},
  file = {Koren et al_2009_Matrix Factorization Techniques for Recommender Systems.pdf:/Users/fergalcotter/Dropbox/Papers/Koren et al_2009_Matrix Factorization Techniques for Recommender Systems_2.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/JCZWZ85T/5197422.html:text/html},
  groups = {Miscellaneous,Miscellaneous}
}

@article{tinbergen_aims_1963,
  title = {On Aims and Methods of {{Ethology}}},
  volume = {20},
  copyright = {1963 Blackwell Verlag GmbH},
  issn = {1439-0310},
  doi = {10.1111/j.1439-0310.1963.tb01161.x},
  abstract = {Ich habe in diesem Aufsatz kurz anzudeuten versucht, was meiner Ansicht nach das Wesentliche in Fragestellung und Methode der Ethologie ist und weshalb wir in Konrad Lorenz den Begr{\"u}nder moderner Ethologie erblicken. Hierbei habe ich vielleicht das Arbeitsgebiet der Ethologie weiter gefa{\ss}t, als unter Ethologen gebr{\"a}uchlich ist. Wenn man aber die vielartige Arbeit jener Forscher {\"u}bersieht, die sich Ethologen nennen, ist man zu dieser weiten Fassung geradezu gezwungen. Ich habe in meiner Darstellung weder Vollst{\"a}ndigkeit noch Gleichgewicht angestrebt und, um zur Fortf{\"u}hrung des Gespr{\"a}chs anzuregen, ruhig meine Steckenpferde geritten, vor allem das Verh{\"a}ltnis zwischen Ethologie und Physiologie, die Gefahr der Vernachl{\"a}ssigung der Frage der Arterhaltung, Fragen der Methodik der ontogenetischen Forschung, und Aufgaben und Methoden der Evolutionsforschung. Bei der Einsch{\"a}tzung des Anteils, den Lorenz an der Entwicklung der Ethologie genommen hat und noch nimmt, habe ich als seinen Hauptbeitrag den bezeichnet, da{\ss} er uns gezeigt hat, wie man bew{\"a}hrtes ``biologisches Denken'' folgerichtig auf Verhalten anwenden kann. Da{\ss} er dabei an die Arbeit seiner Vorg{\"a}nger angekn{\"u}pft hat, ist nicht mehr verwunderlich, als da{\ss} jeder Vater selbst einen Vater hat. Insbesondere scheint mir das Wesentliche an Lorenz` Arbeit zu sein, da{\ss} er klar gesehen hat, da{\ss} Verhaltensweisen Teile von ``Organen'', von Systemen der Arterhaltung sind; da{\ss} ihre Verursachung genau so exakt untersucht werden kann wie die gleich welcher anderer Lebensvorg{\"a}nge, da{\ss} ihr arterhaltender Wert ebenso systematisch und exakt aufweisbar ist wie ihre Verursachung, da{\ss} Verhaltensontogenie in grunds{\"a}tzlich gleicher Weise erforscht werden kann wie die Ontogenie der Form und da{\ss} die Erforschung der Verhaltensevolution der Untersuchung der Strukturevolution parallel geht. Und obwohl Lorenz ein riesiges Tatsachenmaterial gesammelt hat, ist die Ethologie doch noch mehr durch seine Fragestellung und durch k{\"u}hne Hypothesen gef{\"o}rdert als durch eigene Nachpr{\"u}fung dieser Hypothesen. Ohne den Wert solcher Nachpr{\"u}fung zu untersch{\"a}tzen \textemdash{} ohne die es nat{\"u}rlich keine Weiterentwicklung g{\"a}be \textemdash{} m{\"o}chte ich doch behaupten, da{\ss} die durch Nachpr{\"u}fung notwendig gewordenen Modifikationen neben der Leistung des urspr{\"u}nglichen Ansatzes vergleichsweise unbedeutend sind. Nebenbei sei auch daran erinnert, da{\ss} eine der vielen heilsamen Nachwirkungen der Lorenzschen Arbeit das wachsende Interesse ist, das die Humanpsychologie der Ethologie entgegenbringt - ein erster Ansatz einer Entwicklung, deren Tragweite wir noch kaum {\"u}bersehen k{\"o}nnen. Am Schlu{\ss} noch eine Bemerkung zur Terminologie. Ich habe hier das Wort ``Ethologie'' auf einen Riesenkomplex von Wissenschaften angewandt, von denen manche, wie Psychologie und Physiologie, schon l{\"a}ngst anerkannte Namen tragen. Das hei{\ss}t nat{\"u}rlich nicht, da{\ss} ich den Namen Ethologie f{\"u}r dieses ganze Gebiet vorschlagen will; das w{\"a}re geschichtlich einfach falsch, weil das Wort historisch nur die Arbeit einer kleinen Gruppe von Zoologen kennzeichnet. Der Name ist nat{\"u}rlich gleichg{\"u}ltig; worauf es mir vor allem ankommt, ist darzutun, da{\ss} wir das Zusammenwachsen vieler Einzeldisziplinen zu einer vielumfassenden Wissenschaft erleben, f{\"u}r die es nur einen richtigen Namen gibt: ``Verhaltensbiologie''. Selbstverst{\"a}ndlich ist diese synthetische Entwicklung nicht die Arbeit eines Mannes oder gar die der Ethologen. Sie ist die Folge einer allgemeinen Neigung, Br{\"u}cken zwischen verwandten Wissenschaften zu schlagen, einer Neigung, die sich in vielen Disziplinen entwickelt hat. Unter den Zoologen ist es Lorenz, der hierzu am meisten beigetragen und zudem manche Nachbardisziplinen st{\"a}rker beeinflu{\ss}t hat als irgendein anderer. Ich bin sogar davon {\"u}berzeugt, da{\ss} diese Einwirkungen auf Nachbarwissenschaften noch lange anhalten werden und da{\ss} die Verhaltensbiologie erst am Anfang ihrer Ontogenie steht.},
  language = {en},
  timestamp = {2016-07-29T14:44:31Z},
  number = {4},
  urldate = {2016-07-11},
  journal = {Zeitschrift f{\"u}r Tierpsychologie},
  author = {Tinbergen, N.},
  month = jan,
  year = {1963},
  note = {02845},
  pages = {410--433},
  file = {Tinbergen_1963_On aims and methods of Ethology.pdf:/Users/fergalcotter/Dropbox/Papers/Tinbergen_1963_On aims and methods of Ethology.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/DWJD2S7M/abstract.html:text/html},
  groups = {ICVSS,ICVSS}
}

@article{sweldens_lifting_1998-1,
  title = {The {{Lifting Scheme}}: {{A Construction}} of {{Second Generation Wavelets}}},
  volume = {29},
  issn = {0036-1410},
  shorttitle = {The {{Lifting Scheme}}},
  doi = {10.1137/S0036141095289051},
  timestamp = {2016-09-30T13:07:33Z},
  number = {2},
  urldate = {2016-09-28},
  journal = {SIAM J. Math. Anal.},
  author = {Sweldens, Wim},
  month = mar,
  year = {1998},
  note = {02920},
  keywords = {lifting scheme,multiresolution,second generation wavelet,wavelet},
  pages = {511--546},
  file = {Sweldens_1998_The Lifting Scheme.pdf:/Users/fergalcotter/Dropbox/Papers/Sweldens_1998_The Lifting Scheme.pdf:application/pdf},
  groups = {Lifting,Lifting,Lifting,Graphs,Graphs}
}

@article{bengio_objective_2015,
  title = {An Objective Function for {{STDP}}},
  abstract = {We introduce a predictive objective function for the rate aspect of spike-timing dependent plasticity (STDP), i.e., ignoring the effects of synchrony of spikes but looking at spiking \{$\backslash$em rate changes\}. The proposed weight update is proportional to the presynaptic spiking (or firing) rate times the \{$\backslash$em temporal change\} of the integrated postsynaptic activity. We present an intuitive explanation for the relationship between spike-timing and weight change that arises when the weight change follows this rule. Spike-based simulations agree with the proposed relationship between spike timing and the temporal change of postsynaptic activity. They show a strong correlation between the biologically observed STDP behavior and the behavior obtained from simulations where the weight change follows the gradient of the predictive objective function.},
  timestamp = {2016-02-22T17:45:44Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.05936},
  primaryClass = {cs, q-bio},
  urldate = {2016-02-22},
  journal = {arXiv:1509.05936 [cs, q-bio]},
  author = {Bengio, Yoshua and Mesnard, Thomas and Fischer, Asja and Zhang, Saizheng and Wu, Yuhai},
  month = sep,
  year = {2015},
  note = {00001},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {Bengio et al_2015_An objective function for STDP.pdf:/Users/fergalcotter/Dropbox/Papers/Bengio et al_2015_An objective function for STDP.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/FUVMH9JE/1509.html:text/html},
  groups = {Biological Vision,Biological Vision}
}

@article{blakemore_development_1970,
  title = {Development of the {{Brain}} Depends on the {{Visual Environment}}},
  volume = {228},
  copyright = {\textcopyright{} 1970 Nature Publishing Group},
  doi = {10.1038/228477a0},
  language = {en},
  timestamp = {2016-07-27T13:36:18Z},
  number = {5270},
  urldate = {2016-07-27},
  journal = {Nature},
  author = {Blakemore, Colin and Cooper, Grahame F.},
  month = oct,
  year = {1970},
  note = {01069},
  pages = {477--478},
  file = {Blakemore_Cooper_1970_Development of the Brain depends on the Visual Environment.pdf:/Users/fergalcotter/Dropbox/Papers/Blakemore_Cooper_1970_Development of the Brain depends on the Visual Environment.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/RZWHKQBZ/228477a0.html:text/html},
  groups = {Biological Vision,Biological Vision}
}

@inproceedings{kingsbury_redundant_2003,
  title = {Redundant Representation with Complex Wavelets: {{How}} to Achieve Sparsity},
  volume = {1},
  shorttitle = {Redundant Representation with Complex Wavelets},
  timestamp = {2015-11-19T14:13:41Z},
  urldate = {2015-11-03},
  booktitle = {Image {{Processing}}, 2003. {{ICIP}} 2003. {{Proceedings}}. 2003 {{International Conference}} On},
  publisher = {{IEEE}},
  author = {Kingsbury, Nick and Reeves, Tanya},
  year = {2003},
  note = {00034},
  pages = {I--45},
  file = {Kingsbury_Reeves_2003_Redundant representation with complex wavelets.pdf:/Users/fergalcotter/Dropbox/Papers/Kingsbury_Reeves_2003_Redundant representation with complex wavelets.pdf:application/pdf},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@article{lecun_gradient-based_1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  volume = {86},
  issn = {0018-9219},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day},
  timestamp = {2016-07-29T14:44:32Z},
  number = {11},
  journal = {Proceedings of the IEEE},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  month = nov,
  year = {1998},
  note = {05502},
  keywords = {2D shape variability,back-propagation,backpropagation,Character recognition,cheque reading,complex decision surface synthesis,Convolution,convolutional neural network character recognizers,document recognition,document recognition systems,Feature extraction,field extraction,gradient-based learning,gradient based learning technique,graph transformer networks,GTN,handwritten character recognition,handwritten digit recognition task,Hidden Markov models,high-dimensional patterns,language modeling,machine learning,Multi-layer neural network,multilayer neural networks,multilayer perceptrons,multimodule systems,Neural networks,optical character recognition,Optical character recognition software,Optical computing,pattern recognition,performance measure minimization,principal component analysis,segmentation recognition},
  pages = {2278--2324},
  file = {Lecun et al_1998_Gradient-based learning applied to document recognition.pdf:/Users/fergalcotter/Dropbox/Papers/Lecun et al_1998_Gradient-based learning applied to document recognition.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/RVPHPIBG/abs_all.html:text/html},
  groups = {ICVSS,state of the art,ICVSS,state of the art,state of the art}
}

@article{lecun_backpropagation_1989,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  volume = {1},
  issn = {0899-7667},
  doi = {10.1162/neco.1989.1.4.541},
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  timestamp = {2016-01-14T14:02:35Z},
  number = {4},
  journal = {Neural Computation},
  author = {LeCun, Y and Boser, B and Denker, J and Henderson, D and Howard, R and Hubbard, W and Jackel, L},
  month = dec,
  year = {1989},
  note = {01508},
  keywords = {Unread},
  pages = {541--551},
  file = {LeCun et al_1989_Backpropagation Applied to Handwritten Zip Code Recognition.pdf:/Users/fergalcotter/Dropbox/Papers/LeCun et al_1989_Backpropagation Applied to Handwritten Zip Code Recognition.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/TAQBWUJD/freeabs_all.html:text/html},
  groups = {CNNs,CNNs}
}

@article{fei-fei_one-shot_2006,
  title = {One-Shot Learning of Object Categories},
  volume = {28},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2006.79},
  abstract = {Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood (ML) and maximum a posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.},
  timestamp = {2016-03-02T00:10:25Z},
  number = {4},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Fei-Fei, Li and Fergus, R. and Perona, P.},
  month = apr,
  year = {2006},
  note = {02005},
  keywords = {Algorithms,Artificial Intelligence,Automotive materials,Bayesian implementation,Bayesian methods,Bayes methods,Bayes Theorem,Cluster Analysis,Computer Simulation,few images,Image databases,Image Enhancement,Image Interpretation; Computer-Assisted,Image recognition,Imaging; Three-Dimensional,Information Storage and Retrieval,Layout,learning,learning (artificial intelligence),Management training,maximum a posteriori method,maximum likelihood method,Models; Biological,Models; Statistical,object categories,object category posterior model,one-shot learning,Pattern Recognition; Automated,priors.,probabilistic models,probability density function,recognition,Reproducibility of Results,Rough surfaces,Sensitivity and Specificity,statistical analysis,Surface roughness,Taxonomy,Testing,unsupervised,Useful,variational inference},
  pages = {594--611},
  file = {Fei-Fei et al_2006_One-shot learning of object categories.pdf:/Users/fergalcotter/Dropbox/Papers/Fei-Fei et al_2006_One-shot learning of object categories.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/7A6SKW7S/abs_all.html:text/html},
  groups = {Bayesian Models,Bayesian Models,Bayesian Models}
}

@incollection{sallee_learning_2003,
  title = {Learning {{Sparse Multiscale Image Representations}}},
  timestamp = {2016-07-28T11:43:01Z},
  urldate = {2016-07-28},
  booktitle = {Advances in {{Neural Information Processing Systems}} 15},
  publisher = {{MIT Press}},
  author = {Sallee, Phil and Olshausen, Bruno A.},
  editor = {Becker, S. and Thrun, S. and Obermayer, K.},
  year = {2003},
  note = {00081},
  keywords = {Read Now},
  pages = {1351--1358},
  file = {Sallee_Olshausen_2003_Learning Sparse Multiscale Image Representations.pdf:/Users/fergalcotter/Dropbox/Papers/Sallee_Olshausen_2003_Learning Sparse Multiscale Image Representations.pdf:application/pdf;NIPS Snapshort:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/HQJN6Q5C/2145-learning-sparse-multiscale-image-representations.html:text/html},
  groups = {Sparsity for Vision,Sparsity for Vision}
}

@article{chia_office_2015,
  title = {Office Sitting Made Less Sedentary: {{A}} Future-Forward Approach to Reducing Physical Inactivity at Work},
  shorttitle = {Office Sitting Made Less Sedentary},
  timestamp = {2016-06-19T22:08:43Z},
  urldate = {2016-06-19},
  author = {Chia, Michael and Chen, Bokai and Suppiah, Haresh T.},
  year = {2015},
  note = {00001},
  file = {Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/DNVNFK5K/17585.html:text/html},
  groups = {Cords Papers,Cords Papers}
}

@phdthesis{yingsong_zhang_sparse_2011-1,
  type = {PhD Thesis},
  title = {Sparse {{Reconstruction Algorithms}} and the {{Application}} in {{Image Processing}}},
  abstract = {This dissertation investigates the sparse-regularized linear inverse problem
and its applications in image deconvolution, interpolation and denoising
problems. Wavelets provide sparse representations of a wide
range of natural images and data. For this reason, we are interested
in applying the sparse regularization onto the wavelet coefficients of
images, particularly large images and 3D datasets which presents serious
challenges to the design of algorithms for signal recovery. The
conventional sparse-signal-recovery methods, have been traditionally
based on greedy heuristics (e.g. matching-pursuit based methods) or
convex relaxation of `0 (`1 minimization). Such methods become very
computational expensive when the dimensionality of the problem is
large.
In this dissertation, we proposed two algorithms to perform the sparsesignal
recovery efficiently and accurately on large images and 3D datasets.
SAIWave is specially designed for deconvolution. It is a Subband-
Adaptive and generalized version of the popular Iterative thresholding
algorithm that takes different update steps and thresholds for each
subband, which is shown to accelerate the convergence. The SAIWave
algorithm runs in parallel, updating all of the subbands at the same
time. We also give an algorithm for selecting the parameter for each
subband that decides the update steps and thresholds. The other algorithm
is L0RL2 . L0RL2 is developed for the purpose of general
sparse-signal recovery. We introduce a new penalty function, which
has some useful geometric properties with regard to the continuation
parameter $^2$. These properties are then utilized to develop algorithms
(IRLS-$^2$ and L0RL2 ). Both algorithms are shown to recover sparse
signals with fewer measurements than the conventional methods, while
being efficient and accurate. L0RL2 is then combined with the development
of SAIWave to incorporate typical signal structures (group and
tree).
Finally, we consider two image applications of the L0RL2 algorithm.
The first one is to recover an image to a higher resolution than the observation
(super-resolution). The second proposes a simplified image
model to work with the L0RL2 algorithm on image denoising problems.
The proposed model adopts a hierarchical structure to describe
the multi-scale properties of wavelet coefficients. This is able to model
the non-Gaussian features of the wavelet coefficients' marginal distributions.
In our experiments, the denoising result based on the proposed
model has the visual effect of improving the image sharpness.},
  timestamp = {2016-07-28T10:39:17Z},
  school = {University of Cambridge},
  author = {{Yingsong Zhang}},
  year = {2011},
  note = {00000},
  annote = {Chapter 5 of this paper explores how reweighted least squares can be better for sparsifying than L1 methods.},
  file = {Thesis_zys_submitted.pdf:/Users/fergalcotter/Dropbox/Papers/Thesis_zys_submitted.pdf:application/pdf},
  groups = {Sparsity for Vision,Sparsity for Vision}
}

@incollection{_introduction_????,
  title = {Introduction to {{Moments}}},
  timestamp = {2016-07-31T10:01:02Z},
  note = {00002},
  file = {Introduction to Moments.pdf:/Users/fergalcotter/Dropbox/Papers/Introduction to Moments.pdf:application/pdf},
  groups = {Notes,Notes}
}

@article{howard_improvements_2013,
  title = {Some {{Improvements}} on {{Deep Convolutional Neural Network Based Image Classification}}},
  abstract = {We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techiques include adding more image transformations to training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55\% using no external data which is over a 20\% relative improvement on the previous year's winner.},
  timestamp = {2016-08-08T19:33:14Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.5402},
  primaryClass = {cs},
  urldate = {2016-08-08},
  journal = {arXiv:1312.5402 [cs]},
  author = {Howard, Andrew G.},
  month = dec,
  year = {2013},
  note = {00036},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Howard_2013_Some Improvements on Deep Convolutional Neural Network Based Image.pdf:/Users/fergalcotter/Dropbox/Papers/Howard_2013_Some Improvements on Deep Convolutional Neural Network Based Image.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/WSXN77IT/1312.html:text/html},
  groups = {Generic Design,Generic Design,Generic Design}
}

@techreport{davide_scaramuzza_sparse_2009,
  title = {Sparse {{Codes}} for {{Natural Images}}},
  abstract = {The human visual system, at the primary cortex, has
receptive fields that are spatially localized, oriented
and bandpass. It has been shown that a certain
learning algorithm to produce sparse codes for natural
images leads to basis functions with similar properties.
This learning algorithm optimizes a cost function that
trades off representation quality for sparseness, and
searches for sets of natural images, which basis
functions lead to good sparse approximations. The
result of the learning algorithm is a dictionary of basis
functions with localization in space, direction and
scale.
In this paper, dictionaries for different set of images
are showed and their own properties are described
and verified. It will be showed that the learning
algorithm leads to overcomplete bases functions that
``capture'' the intrinsic structure of the images. This
allows efficient coding of the images with good
representation quality. The results are applied to
image approximation and denoising.},
  timestamp = {2016-07-28T11:25:22Z},
  institution = {EPFL},
  author = {{Davide Scaramuzza}},
  month = jul,
  year = {2009},
  note = {00000},
  file = {Davide Scaramuzza_Sparse Codes for Natural Images.pdf:/Users/fergalcotter/Dropbox/Papers/Davide Scaramuzza_Sparse Codes for Natural Images.pdf:application/pdf},
  groups = {Sparsity for Vision,Sparsity for Vision}
}

@inproceedings{shaffrey_unsupervised_2002,
  title = {Unsupervised Image Segmentation via {{Markov}} Trees and Complex Wavelets},
  volume = {3},
  timestamp = {2016-01-14T14:02:36Z},
  urldate = {2015-11-03},
  booktitle = {Image {{Processing}}. 2002. {{Proceedings}}. 2002 {{International Conference}} On},
  publisher = {{IEEE}},
  author = {Shaffrey, Ci{\'a}n W. and Kingsbury, Nick G. and Jermyn, Ian H.},
  year = {2002},
  note = {00042},
  pages = {801--804},
  file = {Shaffrey et al_2002_Unsupervised image segmentation via Markov trees and complex wavelets.pdf:/Users/fergalcotter/Dropbox/Papers/Shaffrey et al_2002_Unsupervised image segmentation via Markov trees and complex wavelets.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@article{field_what_1994,
  title = {What {{Is}} the {{Goal}} of {{Sensory Coding}}?},
  volume = {6},
  issn = {0899-7667},
  doi = {10.1162/neco.1994.6.4.559},
  abstract = {A number of recent attempts have been made to describe early sensory coding in terms of a general information processing strategy. In this paper, two strategies are contrasted. Both strategies take advantage of the redundancy in the environment to produce more effective representations. The first is described as a ``compact'' coding scheme. A compact code performs a transform that allows the input to be represented with a reduced number of vectors (cells) with minimal RMS error. This approach has recently become popular in the neural network literature and is related to a process called Principal Components Analysis (PCA). A number of recent papers have suggested that the optimal ``compact'' code for representing natural scenes will have units with receptive field profiles much like those found in the retina and primary visual cortex. However, in this paper, it is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway. In contrast, it is proposed that the visual system is near to optimal in representing natural scenes only if optimality is defined in terms of ``sparse distributed'' coding. In a sparse distributed code, all cells in the code have an equal response probability across the class of images but have a low response probability for any single image. In such a code, the dimensionality is not reduced. Rather, the redundancy of the input is transformed into the redundancy of the firing pattern of cells. It is proposed that the signature for a sparse code is found in the fourth moment of the response distribution (i.e., the kurtosis). In measurements with 55 calibrated natural scenes, the kurtosis was found to peak when the bandwidths of the visual code matched those of cells in the mammalian visual cortex. Codes resembling ``wavelet transforms'' are proposed to be effective because the response histograms of such codes are spar- e (i.e., show high kurtosis) when presented with natural scenes. It is proposed that the structure of the image that allows sparse coding is found in the phase spectrum of the image. It is suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet). Possible reasons for why sensory systems would evolve toward sparse coding are presented.},
  timestamp = {2016-07-28T11:11:57Z},
  number = {4},
  journal = {Neural Computation},
  author = {Field, D. J.},
  month = jul,
  year = {1994},
  note = {01262},
  pages = {559--601},
  file = {Field_1994_What Is the Goal of Sensory Coding.pdf:/Users/fergalcotter/Dropbox/Papers/Field_1994_What Is the Goal of Sensory Coding.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/3BW3J8NS/freeabs_all.html:text/html},
  groups = {Biological Vision,Biological Vision}
}

@article{srivastava_dropout:_2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  volume = {15},
  timestamp = {2016-01-14T14:02:37Z},
  journal = {Journal of Machine Learning Research},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  note = {00417},
  pages = {1929--1958},
  file = {Srivastava et al_2014_Dropout.pdf:/Users/fergalcotter/Dropbox/Papers/Srivastava et al_2014_Dropout.pdf:application/pdf},
  groups = {Other Networks,network features,Other Networks,network features,network features}
}

@techreport{singh_deep_2015,
  address = {Department of Engineering, Signal Processing},
  type = {First Year PhD Report},
  title = {Deep {{Wavelet Networks}} for {{Object Recognition}}},
  timestamp = {2016-08-21T20:34:41Z},
  institution = {University of Cambridge},
  author = {Singh, Amarjot},
  month = dec,
  year = {2015},
  note = {00000},
  keywords = {Unread},
  file = {Amarjot Singh_Deep Wavelet Networks for Object Recognition.pdf:/Users/fergalcotter/Dropbox/Papers/Amarjot Singh_Deep Wavelet Networks for Object Recognition.pdf:application/pdf},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@article{olshausen_how_2005,
  title = {How Close Are We to Understanding V1?},
  volume = {17},
  issn = {0899-7667},
  doi = {10.1162/0899766054026639},
  abstract = {A wide variety of papers have reviewed what is known about the function of primary visual cortex. In this review, rather than stating what is known, we attempt to estimate how much is still unknown about V1 function. In particular, we identify five problems with the current view of V1 that stem largely from experimental and theoretical biases, in addition to the contributions of nonlinearities in the cortex that are not well understood. Our purpose is to open the door to new theories, a number of which we describe, along with some proposals for testing them.},
  language = {eng},
  timestamp = {2016-07-26T16:55:49Z},
  number = {8},
  journal = {Neural Computation},
  author = {Olshausen, Bruno A. and Field, David J.},
  month = aug,
  year = {2005},
  note = {00326},
  keywords = {Animals,Bias (Epidemiology),Humans,Models; Neurological,Neurons,Nonlinear Dynamics,visual cortex,Visual Pathways},
  pages = {1665--1699},
  file = {Olshausen_Field_2005_How close are we to understanding v1.pdf:/Users/fergalcotter/Dropbox/Papers/Olshausen_Field_2005_How close are we to understanding v1.pdf:application/pdf},
  groups = {Biological Vision,Biological Vision},
  pmid = {15969914}
}

@techreport{cochran_subfig_2005,
  title = {Subfig},
  abstract = {This article documents the LATEX package `subfig', which provides support for the inclusion of small, `sub', figures and tables. It simplifies the positioning, captioning and labeling of such objects within a single figure or table environment and to
continue a figure or table across multiple pages. In addition, this package allows such sub-captions to be written to a List-of-Floats page as desired. The `subfig' package requires the `caption' package by H.A. Sommerfeldt and replaces the older
`subfigure' package.},
  timestamp = {2016-09-13T11:33:13Z},
  author = {Cochran, Steven},
  month = jul,
  year = {2005},
  note = {00003},
  file = {Cochran_2005_Subfig.pdf:/Users/fergalcotter/Dropbox/Papers/Cochran_2005_Subfig.pdf:application/pdf},
  groups = {Latex,Latex}
}

@article{raichle_two_2010,
  title = {Two Views of Brain Function},
  volume = {14},
  issn = {1879-307X},
  doi = {10.1016/j.tics.2010.01.008},
  abstract = {Traditionally studies of brain function have focused on task-evoked responses. By their very nature, such experiments tacitly encourage a reflexive view of brain function. Although such an approach has been remarkably productive, it ignores the alternative possibility that brain functions are mainly intrinsic, involving information processing for interpreting, responding to and predicting environmental demands. Here I argue that the latter view best captures the essence of brain function, a position that accords well with the allocation of the brain's energy resources. Recognizing the importance of intrinsic activity will require integrating knowledge from cognitive and systems neuroscience with cellular and molecular neuroscience where ion channels, receptors, components of signal transduction and metabolic pathways are all in a constant state of flux.},
  language = {eng},
  timestamp = {2016-07-26T16:29:25Z},
  number = {4},
  journal = {Trends in Cognitive Sciences},
  author = {Raichle, Marcus E.},
  month = apr,
  year = {2010},
  note = {00505},
  keywords = {Brain,Brain Mapping,Cognition,Energy Metabolism,Humans,Magnetic Resonance Imaging,Nerve Net,Neurons,neurophysiology,Signal Transduction},
  pages = {180--190},
  file = {Raichle Two views of brain function 2010.pdf:/Users/fergalcotter/Dropbox/Papers/Raichle Two views of brain function 2010.pdf:application/pdf},
  groups = {Biological Vision,Biological Vision},
  pmid = {20206576}
}

@inproceedings{anderson_rotation-invariant_2006,
  title = {Rotation-Invariant Object Recognition Using Edge Profile Clusters},
  timestamp = {2015-11-19T14:13:43Z},
  urldate = {2015-11-03},
  booktitle = {Signal {{Processing Conference}}, 2006 14th {{European}}},
  publisher = {{IEEE}},
  author = {Anderson, Ryan and Kingsbury, Nick and Fauqueur, Julien},
  year = {2006},
  note = {00010},
  pages = {1--5},
  file = {Rotation-invariant object recognition using edge-profile clusters.pdf:/Users/fergalcotter/Dropbox/Papers/Rotation-invariant object recognition using edge-profile clusters.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@article{hu_visual_1962,
  title = {Visual Pattern Recognition by Moment Invariants},
  volume = {8},
  issn = {0096-1000},
  doi = {10.1109/TIT.1962.1057692},
  abstract = {In this paper a theory of two-dimensional moment invariants for planar geometric figures is presented. A fundamental theorem is established to relate such moment invariants to the well-known algebraic invariants. Complete systems of moment invariants under translation, similitude and orthogonal transformations are derived. Some moment invariants under general two-dimensional linear transformations are also included. Both theoretical formulation and practical models of visual pattern recognition based upon these moment invariants are discussed. A simple simulation program together with its performance are also presented. It is shown that recognition of geometrical patterns and alphabetical characters independently of position, size and orientation can be accomplished. It is also indicated that generalization is possible to include invariance with parallel projection.},
  timestamp = {2016-07-31T09:16:29Z},
  number = {2},
  journal = {IRE Transactions on Information Theory},
  author = {Hu, Ming-Kuei},
  month = feb,
  year = {1962},
  note = {08075},
  keywords = {Artificial Intelligence,Bibliographies,Character recognition,Decision theory,Distribution functions,Image analysis,Information processing,Information theory,pattern recognition,Senior members,Shape},
  pages = {179--187},
  file = {Hu_1962_Visual pattern recognition by moment invariants.pdf:/Users/fergalcotter/Dropbox/Papers/Hu_1962_Visual pattern recognition by moment invariants.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/33E8XWA5/abs_all.html:text/html},
  groups = {Other Networks,Other Networks}
}

@article{lennie_cost_2003,
  title = {The Cost of Cortical Computation},
  volume = {13},
  issn = {0960-9822},
  abstract = {Electrophysiological recordings show that individual neurons in cortex are strongly activated when engaged in appropriate tasks, but they tell us little about how many neurons might be engaged by a task, which is important to know if we are to understand how cortex encodes information. For human cortex, I estimate the cost of individual spikes, then, from the known energy consumption of cortex, I establish how many neurons can be active concurrently. The cost of a single spike is high, and this severely limits, possibly to fewer than 1\%, the number of neurons that can be substantially active concurrently. The high cost of spikes requires the brain not only to use representational codes that rely on very few active neurons, but also to allocate its energy resources flexibly among cortical regions according to task demand. The latter constraint explains the investment in local control of hemodynamics, exploited by functional magnetic resonance imaging, and the need for mechanisms of selective attention.},
  language = {eng},
  timestamp = {2016-08-09T22:17:09Z},
  number = {6},
  journal = {Current biology: CB},
  author = {Lennie, Peter},
  month = mar,
  year = {2003},
  note = {00627},
  keywords = {Action Potentials,Attention,Brain Mapping,Cell Count,Energy Metabolism,Excitatory Postsynaptic Potentials,Hemodynamics,Humans,Linear Models,Models; Neurological,Neocortex,Neurons,Neurotransmitter Agents,Synaptic Transmission,Time Factors},
  pages = {493--497},
  groups = {Biological Vision,Biological Vision},
  pmid = {12646132}
}

@inproceedings{kingsbury_dual-tree_1998-1,
  title = {The Dual-Tree Complex Wavelet Transform: {{A}} New Efficient Tool for Image Restoration and Enhancement},
  shorttitle = {The Dual-Tree Complex Wavelet Transform},
  abstract = {A new implementation of the Discrete Wavelet Transform is presented for applications such as image restoration and enhancement. It employs a dual tree of wavelet filters to obtain the real and imaginary parts of the complex wavelet coefficients. This introduces limited redundancy (4 : 1 for 2-dimensional signals) and allows the transform to provide approximate shift invariance and directionally selective filters (properties lacking in the traditional wavelet transform) while preserving the usual properties of perfect reconstruction and computational efficiency. We show how the dual-tree complex wavelet transform can provide a good basis for multi-resolution image denoising and de-blurring.},
  timestamp = {2016-03-02T00:10:26Z},
  booktitle = {Signal {{Processing Conference}} ({{EUSIPCO}} 1998), 9th {{European}}},
  author = {Kingsbury, Nick},
  month = sep,
  year = {1998},
  note = {00345},
  keywords = {2-dimensional signals,complex wavelet coefficients,Continuous wavelet transforms,directionally selective filters,discrete wavelet transform,Discrete wavelet transforms,dual tree,dual-tree complex wavelet transform,filtering theory,image denoising,Image Enhancement,Image reconstruction,image restoration,multiresolution image deblurring,multiresolution image denoising,shift invariance,Wavelet analysis,wavelet filters},
  pages = {1--4},
  file = {Kingsbury_1998_The dual-tree complex wavelet transform.pdf:/Users/fergalcotter/Dropbox/Papers/Kingsbury_1998_The dual-tree complex wavelet transform.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/ADK2WINZ/articleDetails.html:text/html},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@article{mallat_multiresolution_1989,
  title = {Multiresolution {{Approximations}} and {{Wavelet Orthonormal Bases}} of {{L2}}({{R}})},
  volume = {315},
  copyright = {Copyright \textcopyright{} 1989 American Mathematical Society},
  issn = {0002-9947},
  doi = {10.2307/2001373},
  abstract = {A multiresolution approximation is a sequence of embedded vector spaces (Vj)j$\in$ Z for approximating L2(R) functions. We study the properties of a multiresolution approximation and prove that it is characterized by a 2$\pi$-periodic function which is further described. From any multiresolution approximation, we can derive a function $\psi$(x) called a wavelet such that \$($\backslash$sqrt\{2\^j\}$\backslash$pi(2\^jx - k))\_\{(k,j)$\backslash$in Z\^2\}\$ is an orthonormal basis of L2(R). This provides a new approach for understanding and computing wavelet orthonormal bases. Finally, we characterize the asymptotic decay rate of multiresolution approximation errors for functions in a Sobolev space Hs.},
  timestamp = {2016-01-14T14:17:08Z},
  number = {1},
  urldate = {2015-11-04},
  journal = {Transactions of the American Mathematical Society},
  author = {Mallat, Stephane G.},
  month = sep,
  year = {1989},
  note = {03220 (manual)},
  pages = {69--87},
  file = {Mallat_1989_Multiresolution Approximations and Wavelet Orthonormal Bases of L2(R).pdf:/Users/fergalcotter/Dropbox/Papers/Mallat_1989_Multiresolution Approximations and Wavelet Orthonormal Bases of L2(R).pdf:application/pdf},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@phdthesis{mark_miller_multiscale_2006,
  address = {Department of Engineering, Signal Processing},
  type = {PhD Thesis},
  title = {Multiscale {{Techniques}} for {{Imaging Problems}}},
  abstract = {This thesis demonstrates the use of multiscale bases in solving inverse imaging problems. It uses a complex wavelet decomposition called the Duel Tree Complex Wavelet Transform (DT-CWT) which has a number of advantages over real wavelet 
transforms used in many image processing applications. A novel method for defining the statistics of 2-D photographic images useful in image estimation is defined. The new method is based on the DT-CWT but a phase rotation is applied to the coefficients to create complex coefficients whose phase is shift-invariant at multiscale edge and ridge features. This is in addition to the magnitude shift invariance achieved by the DT-CWT. Furthermore, the phase of the new coefficient is consistent for a particular type of feature in a given subband allowing more accurate estimation of the statistics of the subband from a degraded image.

These derotated coefficients are used to model multiscale edges and ridge features and are combined with state-of-the-art Gaussian Scale Mixture (GSM) denoising techniques to create a novel denoising algorithm. The additional modelling is combined with existing techniques using a Bayesian adaptive model selection framework. The algorithm succeeds in providing improved denoising performance at structural image features, reducing ringing artifacts and enhancing sharpness, while avoiding degradation in other areas. The method outperforms previously published methods visually and in standard tests.

The DT-CWT is also applied to the imaging of geological subsurface structures using marine seismic data collected on the sea surface. In standard reconstruction techniques, the reflectivity model parameters are defined as a grid of point scatterers over the area or volume of the subsurface to be imaged. We propose an approach to subsurface imaging using the DT-CWT as a basis for the reflectivity. This basis is used in conjunction with an iterative optimisation which frames the problem as a linearised inverse scattering problem. We demonstrate the method on synthetic data and a marine seismic data set acquired over the Gippsland Basin near Australia. The technique is shown to reduce noise and processing artifacts while preserving discontinuities. It is likely to be particularly useful in cases where the acquired data are incomplete.},
  timestamp = {2016-08-07T17:20:56Z},
  school = {University of Cambridge},
  author = {{Mark Miller}},
  year = {2006},
  note = {00002},
  file = {Mark Miller_2006_Multiscale Techniques for Imaging Problems.pdf:/Users/fergalcotter/Dropbox/Papers/Mark Miller_2006_Multiscale Techniques for Imaging Problems.pdf:application/pdf},
  groups = {DTCWT applications,DTCWT applications,DTCWT applications}
}

@article{bowmaker_visual_1980,
  title = {Visual Pigments of Rods and Cones in a Human Retina.},
  volume = {298},
  issn = {0022-3751},
  abstract = {1. Microspectrophotometric measurements have been made of the photopigments of individual rods and cones from the retina of a man. The measuring beam was passed transversely through the isolated outer segments. 2. The mean absorbance spectrum for rods (n = 11) had a peak at 497.6 +/- 3.3 nm and the mean transverse absorbance was 0.035 +/- 0.007. 3. Three classes of cones were identified. The long-wave cones ('red' cones) had a lambda max of 562.8 +/- 4.7 nm (n = 19) with a mean transverse absorbance of 0.027 +/- 0.005. The middle-wave cones ('green' cones) had a lambda max of 533.8 +/- 3.7 nm (n = 11) with a mean transverse absorbance of 0.032 +/- 0.007. The short-wave cones ('blue' cones) had a lambda max of 420.3 +/- 4.7 nm (n = 3) with a mean transverse absorbance of 0.037 +/- 0.011. 4. If assumptions are made about the length of cones and about pre-receptoral absorption, it is possible to derive psychophysical sensitivities for the cones that closely resemble the appropriate pi mechanisms of W. S. Stiles. 5. If assumptions are made about the length of rods and about pre-receptoral absorption, however, the psychophysical sensitivity derived for the rods is considerably broader than the C.I.E. scotopic sensitivity function.},
  timestamp = {2016-07-29T14:44:34Z},
  urldate = {2016-07-26},
  journal = {The Journal of Physiology},
  author = {Bowmaker, J K and Dartnall, H J},
  month = jan,
  year = {1980},
  note = {00385},
  pages = {501--511},
  file = {Bowmaker_Dartnall_1980_Visual pigments of rods and cones in a human retina.pdf:/Users/fergalcotter/Dropbox/Papers/Bowmaker_Dartnall_1980_Visual pigments of rods and cones in a human retina.pdf:application/pdf},
  groups = {Biological Vision,Biological Vision},
  pmid = {7359434},
  pmcid = {PMC1279132}
}

@article{gao_matrix_2016,
  title = {Matrix {{Neural Networks}}},
  abstract = {Traditional neural networks assume vectorial inputs as the network is arranged as layers of single line of computing units called neurons. This special structure requires the non-vectorial inputs such as matrices to be converted into vectors. This process can be problematic. Firstly, the spatial information among elements of the data may be lost during vectorisation. Secondly, the solution space becomes very large which demands very special treatments to the network parameters and high computational cost. To address these issues, we propose matrix neural networks (MatNet), which takes matrices directly as inputs. Each neuron senses summarised information through bilinear mapping from lower layer units in exactly the same way as the classic feed forward neural networks. Under this structure, back prorogation and gradient descent combination can be utilised to obtain network parameters efficiently. Furthermore, it can be conveniently extended for multimodal inputs. We apply MatNet to MNIST handwritten digits classification and image super resolution tasks to show its effectiveness. Without too much tweaking MatNet achieves comparable performance as the state-of-the-art methods in both tasks with considerably reduced complexity.},
  timestamp = {2016-01-25T11:40:22Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1601.03805},
  primaryClass = {cs},
  urldate = {2016-01-25},
  journal = {arXiv:1601.03805 [cs]},
  author = {Gao, Junbin and Guo, Yi and Wang, Zhiyong},
  month = jan,
  year = {2016},
  note = {00000},
  keywords = {Computer Science - Learning},
  annote = {Interesting paper, proposes using a special matrix neuron rather than stacking and making it a vector, i.e. the last fully connected layer.
They use a separable transform, i.e. UXV' to activate a neuron, so we cut down the degrees of freedom massively, which I guess makes it all a lot quicker to train.
Comment: 20 pages, 5 figures},
  file = {Gao et al_2016_Matrix Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Gao et al_2016_Matrix Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/7PVQSNW5/1601.html:text/html},
  groups = {Other Networks,Other Networks}
}

@inproceedings{bruna_classification_2011,
  title = {Classification with Scattering Operators},
  doi = {10.1109/CVPR.2011.5995635},
  abstract = {A scattering vector is a local descriptor including multiscale and multi-direction co-occurrence information. It is computed with a cascade of wavelet decompositions and complex modulus. This scattering representation is locally translation invariant and linearizes deformations. A supervised classification algorithm is computed with a PCA model selection on scattering vectors. State of the art results are obtained for handwritten digit recognition and texture classification.},
  timestamp = {2016-07-28T21:22:41Z},
  booktitle = {2011 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Bruna, J. and Mallat, S.},
  month = jun,
  year = {2011},
  note = {00058},
  keywords = {complex modulus,Convolution,Databases,handwritten character recognition,handwritten digit recognition,image classification,image texture,learning (artificial intelligence),locally translation invariant,multidirection co-occurrence information,multiscale co-occurrence information,PCA model selection,principal component analysis,Scattering,scattering operators,scattering representation,scattering vector,supervised classification algorithm,texture classification,Training,wavelet decompositions,wavelet transforms},
  pages = {1561--1566},
  file = {Bruna_Mallat_2011_Classification with scattering operators.pdf:/Users/fergalcotter/Dropbox/Papers/Bruna_Mallat_2011_Classification with scattering operators.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/BZISGAQA/articleDetails.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@article{goodman_european_2016,
  title = {European {{Union}} Regulations on Algorithmic Decision-Making and a "Right to Explanation"},
  abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for machine learning researchers to take the lead in designing algorithms and evaluation frameworks which avoid discrimination.},
  timestamp = {2016-08-12T20:35:10Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.08813},
  primaryClass = {cs, stat},
  urldate = {2016-08-12},
  journal = {arXiv:1606.08813 [cs, stat]},
  author = {Goodman, Bryce and Flaxman, Seth},
  month = jun,
  year = {2016},
  note = {00000},
  keywords = {Computer Science - Computers and Society,Computer Science - Learning,Statistics - Machine Learning},
  annote = {Comment: presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY},
  file = {Goodman_Flaxman_2016_European Union regulations on algorithmic decision-making and a right to.pdf:/Users/fergalcotter/Dropbox/Papers/Goodman_Flaxman_2016_European Union regulations on algorithmic decision-making and a right to.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/52TZI8TV/1606.html:text/html},
  groups = {criticisms,criticisms,criticisms}
}

@article{hubel_receptive_1962,
  title = {Receptive Fields, Binocular Interaction and Functional Architecture in the Cat's Visual Cortex},
  volume = {160},
  issn = {0022-3751},
  language = {eng},
  timestamp = {2016-01-20T12:28:55Z},
  journal = {The Journal of Physiology},
  author = {Hubel, D. H. and Wiesel, T. N.},
  month = jan,
  year = {1962},
  note = {10058},
  keywords = {Cerebral Cortex,CEREBRAL CORTEX/physiology,Useful},
  pages = {106--154},
  file = {Hubel_Wiesel_1962_Receptive fields, binocular interaction and functional architecture in the.pdf:/Users/fergalcotter/Dropbox/Papers/Hubel_Wiesel_1962_Receptive fields, binocular interaction and functional architecture in the.pdf:application/pdf},
  groups = {Biological Vision,Biological Vision}
}

@article{mishkin_all_2015,
  title = {All You Need Is a Good Init},
  abstract = {Layer-sequential unit-variance (LSUV) initialization - a simple method for weight initialization for deep net learning - is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.},
  timestamp = {2016-08-09T11:43:29Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.06422},
  primaryClass = {cs},
  urldate = {2016-08-09},
  journal = {arXiv:1511.06422 [cs]},
  author = {Mishkin, Dmytro and Matas, Jiri},
  month = nov,
  year = {2015},
  note = {00012},
  keywords = {Computer Science - Learning},
  annote = {Comment: Published as a conference paper at ICLR 2016},
  file = {Mishkin_Matas_2015_All you need is a good init.pdf:/Users/fergalcotter/Dropbox/Papers/Mishkin_Matas_2015_All you need is a good init.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/35DWH2Q8/1511.html:text/html},
  groups = {network features,network features,network features}
}

@incollection{sanger_optimality_1989,
  title = {An {{Optimality Principle}} for {{Unsupervised Learning}}},
  timestamp = {2016-07-27T17:04:09Z},
  urldate = {2016-07-27},
  booktitle = {Advances in {{Neural Information Processing Systems}} 1},
  publisher = {{Morgan-Kaufmann}},
  author = {Sanger, Terence D.},
  editor = {Touretzky, D. S.},
  year = {1989},
  note = {00163},
  pages = {11--19},
  file = {Sanger_1989_An Optimality Principle for Unsupervised Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Sanger_1989_An Optimality Principle for Unsupervised Learning.pdf:application/pdf;NIPS Snapshort:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/XNJ5TDC6/139-an-optimality-principle-for-unsupervised-learning.html:text/html},
  groups = {Unsupervised Methods,Unsupervised Methods}
}

@article{mahendran_understanding_2015,
  title = {Understanding {{Deep Image Representations}} by {{Inverting Them}}},
  abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
  timestamp = {2016-01-14T14:02:39Z},
  urldate = {2015-12-01},
  journal = {Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
  author = {Mahendran, Aravindh and Vedaldi, Andrea},
  year = {2015},
  note = {00042},
  keywords = {_tablet,Computer Science - Computer Vision and Pattern Recognition,Key Paper,Unread},
  annote = {Inverts output of many image recognition tools, including older ones like HOG and SIFT, as well as newer ones like CNNs.
Similar to Zeiler and Fergus 2014, and Simonyan et al. 2015
However, Deconvnets look at how certain network outputs are obtained, whereas this network looks for what information is preserved by the networkoutput.
They talk a lot about natural image priors, which I think they use to select their regularizers and loss function - makes them use a Total Variation regularizer rather than Lp norm (piecewise smooth).
They come up with some nasty optimisation problem, which is non convex, and use SGD to train.
~},
  file = {Mahendran_Vedaldi_2015_Understanding Deep Image Representations by Inverting Them.pdf:/Users/fergalcotter/Dropbox/Papers/Mahendran_Vedaldi_2015_Understanding Deep Image Representations by Inverting Them.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/N3SBS2D7/1412.html:text/html},
  groups = {Visualization \& Generative,Visualization \& Generative,Visualization \& Generative}
}

@book{tor_norretranders_user_1998,
  title = {The {{User Illusion}}},
  abstract = {As John Casti wrote, "Finally, a book that really does explain consciousness." This groundbreaking work by Denmark's leading science writ...},
  timestamp = {2016-07-29T14:44:35Z},
  urldate = {2016-07-26},
  publisher = {{Viking}},
  author = {{Tor N{\o}rretranders}},
  year = {1998},
  note = {00728},
  file = {Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/BQDC4J5T/106732.html:text/html},
  groups = {Biological Vision,Biological Vision}
}

@article{goodfellow_explaining_2014,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  timestamp = {2016-09-13T11:33:14Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6572},
  primaryClass = {cs, stat},
  urldate = {2016-08-24},
  journal = {arXiv:1412.6572 [cs, stat]},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  month = dec,
  year = {2014},
  note = {00109},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {Goodfellow et al_2014_Explaining and Harnessing Adversarial Examples.pdf:/Users/fergalcotter/Dropbox/Papers/Goodfellow et al_2014_Explaining and Harnessing Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/WCKBT46V/1412.html:text/html},
  groups = {criticisms,criticisms,criticisms}
}

@article{shang_understanding_2016,
  title = {Understanding and {{Improving Convolutional Neural Networks}} via {{Concatenated Rectified Linear Units}}},
  abstract = {Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the property of convolutional neural networks, as well as a generic method to improve the performance of many CNN architectures. Specifically, we first examine existing CNN models and observe an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). Inspired by our observation, we propose a novel, simple yet effective activation scheme called concatenated ReLU (CRelu) and theoretically analyze its reconstruction property in CNNs. We integrate CRelu into several state-of-the-art CNN architectures and demonstrate improvement in their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer trainable parameters. Our results suggest that better understanding of the properties of CNNs can lead to significant performance improvement with a simple modification.},
  timestamp = {2016-08-09T11:52:51Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.05201},
  primaryClass = {cs},
  urldate = {2016-08-09},
  journal = {arXiv:1603.05201 [cs]},
  author = {Shang, Wenling and Sohn, Kihyuk and Almeida, Diogo and Lee, Honglak},
  month = mar,
  year = {2016},
  note = {00000},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  annote = {Notes that redundant filters are learned with opposite phase!},
  file = {Shang et al_2016_Understanding and Improving Convolutional Neural Networks via Concatenated.pdf:/Users/fergalcotter/Dropbox/Papers/Shang et al_2016_Understanding and Improving Convolutional Neural Networks via Concatenated.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/PGW3I37S/1603.html:text/html},
  groups = {CNNs,CNNs}
}

@article{ioffe_batch_2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  timestamp = {2016-02-02T19:49:43Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.03167},
  primaryClass = {cs},
  urldate = {2016-02-02},
  journal = {arXiv:1502.03167 [cs]},
  author = {Ioffe, Sergey and Szegedy, Christian},
  month = feb,
  year = {2015},
  note = {00137},
  keywords = {Computer Science - Learning,Unread},
  file = {Ioffe_Szegedy_2015_Batch Normalization.pdf:/Users/fergalcotter/Dropbox/Papers/Ioffe_Szegedy_2015_Batch Normalization.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/GKWSH23K/1502.html:text/html},
  groups = {network features,Generic Design,network features,Generic Design,network features,Generic Design,reading group,reading group,reading group}
}

@article{waldspurger_phase_2012,
  title = {Phase {{Recovery}}, {{MaxCut}} and {{Complex Semidefinite Programming}}},
  abstract = {Phase retrieval seeks to recover a signal x from the amplitude |Ax| of linear measurements. We cast the phase retrieval problem as a non-convex quadratic program over a complex phase vector and formulate a tractable relaxation (called PhaseCut) similar to the classical MaxCut semidefinite program. We solve this problem using a provably convergent block coordinate descent algorithm whose structure is similar to that of the original greedy algorithm in Gerchberg-Saxton, where each iteration is a matrix vector product. Numerical results show the performance of this approach over three different phase retrieval problems, in comparison with greedy phase retrieval algorithms and matrix completion formulations.},
  timestamp = {2016-02-01T16:22:48Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1206.0102},
  primaryClass = {math},
  urldate = {2016-02-01},
  journal = {arXiv:1206.0102 [math]},
  author = {Waldspurger, Ir{\`e}ne and {d'Aspremont}, Alexandre and Mallat, St{\'e}phane},
  month = jun,
  year = {2012},
  note = {00125},
  keywords = {Mathematics - Optimization and Control,Unread},
  annote = {Comment: Submitted revision},
  file = {Waldspurger et al_2012_Phase Recovery, MaxCut and Complex Semidefinite Programming.pdf:/Users/fergalcotter/Dropbox/Papers/Waldspurger et al_2012_Phase Recovery, MaxCut and Complex Semidefinite Programming.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/9SVAKHTF/1206.html:text/html},
  groups = {advanced,advanced,advanced}
}

@article{hubel_receptive_1968,
  title = {Receptive Fields and Functional Architecture of Monkey Striate Cortex},
  volume = {195},
  issn = {0022-3751},
  abstract = {1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded.2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent.3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other.4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven.5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.},
  language = {eng},
  timestamp = {2016-07-29T14:44:36Z},
  number = {1},
  journal = {The Journal of Physiology},
  author = {Hubel, D. H. and Wiesel, T. N.},
  month = mar,
  year = {1968},
  note = {04955},
  keywords = {Animals,Color Perception,Evoked Potentials,Haplorhini,Light,Motion Perception,Occipital Lobe,Retina,Vision; Ocular,Visual Fields},
  pages = {215--243},
  file = {Hubel_Wiesel_1968_Receptive fields and functional architecture of monkey striate cortex.pdf:/Users/fergalcotter/Dropbox/Papers/Hubel_Wiesel_1968_Receptive fields and functional architecture of monkey striate cortex.pdf:application/pdf},
  groups = {Biological Vision,Biological Vision},
  pmid = {4966457},
  pmcid = {PMC1557912}
}

@article{grossmann_decomposition_1984,
  title = {Decomposition of {{Hardy Functions}} into {{Square Integrable Wavelets}} of {{Constant Shape}}},
  volume = {15},
  issn = {0036-1410},
  doi = {10.1137/0515056},
  abstract = {An arbitrary square integrable real-valued function (or, equivalently, the associated Hardy function) can be conveniently analyzed into a suitable family of square integrable wavelets of constant shape, (i.e. obtained by shifts and dilations from any one of them.) The resulting integral transform is isometric and self-reciprocal if the wavelets satisfy an ``admissibility condition'' given here. Explicit expressions are obtained in the case of a particular analyzing family that plays a role analogous to that of coherent states (Gabor wavelets) in the usual \$L\_2 \$ -theory. They are written in terms of a modified \$$\backslash$Gamma \$-function that is introduced and studied. From the point of view of group theory, this paper is concerned with square integrable coefficients of an irreducible representation of the nonunimodular \$ax + b\$-group.},
  timestamp = {2016-08-07T17:20:57Z},
  number = {4},
  urldate = {2016-08-05},
  journal = {SIAM Journal on Mathematical Analysis},
  author = {Grossmann, A. and Morlet, J.},
  month = jul,
  year = {1984},
  note = {03274},
  pages = {723--736},
  file = {Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/WAPVVHSJ/0515056.html:text/html},
  groups = {Wavelets and DTCWT,Wavelets and DTCWT}
}

@incollection{sgallari_scale_2007,
  address = {Berlin, Heidelberg},
  title = {Scale {{Spaces}} on {{Lie Groups}}},
  volume = {4485},
  isbn = {978-3-540-72822-1 978-3-540-72823-8},
  language = {en},
  timestamp = {2016-08-03T11:14:27Z},
  urldate = {2016-08-03},
  booktitle = {Scale {{Space}} and {{Variational Methods}} in {{Computer Vision}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Duits, Remco and Burgeth, Bernhard},
  editor = {Sgallari, Fiorella and Murli, Almerico and Paragios, Nikos},
  year = {2007},
  note = {00000},
  pages = {300--312},
  file = {Duits_Burgeth_2007_Scale Spaces on Lie Groups.pdf:/Users/fergalcotter/Dropbox/Papers/Duits_Burgeth_2007_Scale Spaces on Lie Groups.pdf:application/pdf},
  groups = {Background,Background,Background}
}

@inproceedings{lowe_object_1999,
  title = {Object Recognition from Local Scale-Invariant Features},
  volume = {2},
  doi = {10.1109/ICCV.1999.790410},
  abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
  timestamp = {2016-08-22T22:21:13Z},
  booktitle = {The {{Proceedings}} of the {{Seventh IEEE International Conference}} on {{Computer Vision}}, 1999},
  author = {Lowe, D. G.},
  year = {1999},
  note = {11668},
  keywords = {3D projection,blurred image gradients,candidate object matches,cluttered partially occluded images,computational geometry,computation time,Computer science,Electrical capacitance tomography,Feature extraction,Filters,image matching,Image recognition,inferior temporal cortex,Layout,least squares approximations,Lighting,local geometric deformations,local image features,local scale-invariant features,low residual least squares solution,multiple orientation planes,nearest neighbor indexing method,Neurons,object recognition,primate vision,Programmable logic arrays,Reactive power,robust object recognition,staged filtering approach,unknown model parameters},
  pages = {1150--1157 vol.2},
  file = {Lowe_1999_Object recognition from local scale-invariant features.pdf:/Users/fergalcotter/Dropbox/Papers/Lowe_1999_Object recognition from local scale-invariant features.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/5HF8M2MV/abs_all.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@incollection{rustamov_wavelets_2013,
  title = {Wavelets on {{Graphs}} via {{Deep Learning}}},
  timestamp = {2016-09-30T13:07:34Z},
  urldate = {2016-09-28},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  publisher = {{Curran Associates, Inc.}},
  author = {Rustamov, Raif and Guibas, Leonidas J},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  year = {2013},
  note = {00023},
  pages = {998--1006},
  file = {Rustamov_Guibas_2013_Wavelets on Graphs via Deep Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Rustamov_Guibas_2013_Wavelets on Graphs via Deep Learning.pdf:application/pdf;NIPS Snapshort:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/QTBMRX87/5046-wavelets-on-graphs-via-deep-learning.html:text/html},
  groups = {Lifting,Lifting,Lifting}
}

@article{bruna_mathematical_2015,
  title = {A Mathematical Motivation for Complex-Valued Convolutional Networks},
  abstract = {A complex-valued convolutional network (convnet) implements the repeated application of the following composition of three operations, recursively applying the composition to an input vector of nonnegative real numbers: (1) convolution with complex-valued vectors followed by (2) taking the absolute value of every entry of the resulting vectors followed by (3) local averaging. For processing real-valued random vectors, complex-valued convnets can be viewed as "data-driven multiscale windowed power spectra," "data-driven multiscale windowed absolute spectra," "data-driven multiwavelet absolute values," or (in their most general configuration) "data-driven nonlinear multiwavelet packets." Indeed, complex-valued convnets can calculate multiscale windowed spectra when the convnet filters are windowed complex-valued exponentials. Standard real-valued convnets, using rectified linear units (ReLUs), sigmoidal (for example, logistic or tanh) nonlinearities, max. pooling, etc., do not obviously exhibit the same exact correspondence with data-driven wavelets (whereas for complex-valued convnets, the correspondence is much more than just a vague analogy). Courtesy of the exact correspondence, the remarkably rich and rigorous body of mathematical analysis for wavelets applies directly to (complex-valued) convnets.},
  timestamp = {2016-02-01T15:54:26Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.03438},
  primaryClass = {cs, stat},
  urldate = {2016-02-01},
  journal = {arXiv:1503.03438 [cs, stat]},
  author = {Bruna, Joan and Chintala, Soumith and LeCun, Yann and Piantino, Serkan and Szlam, Arthur and Tygert, Mark},
  month = mar,
  year = {2015},
  note = {00000},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Similar Work,Statistics - Machine Learning,Unread},
  annote = {Comment: 11 pages, 3 figures; this is the retitled version submitted to the journal, "Neural Computation"},
  file = {Bruna et al_2015_A mathematical motivation for complex-valued convolutional networks.pdf:/Users/fergalcotter/Dropbox/Papers/Bruna et al_2015_A mathematical motivation for complex-valued convolutional networks.pdf:application/pdf;Bruna et al_2015_A mathematical motivation for complex-valued convolutional networks.pdf:/Users/fergalcotter/Dropbox/Papers/Bruna et al_2015_A mathematical motivation for complex-valued convolutional networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/62RU4AWA/1503.html:text/html},
  groups = {Scatternets and Handcrafted,Scatternets and Handcrafted}
}

@inproceedings{bengio_greedy_2007,
  title = {Greedy {{Layer}}-{{Wise Training}} of {{Deep Networks}}},
  abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.},
  timestamp = {2016-07-29T14:44:37Z},
  author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  year = {2007},
  note = {01545},
  pages = {153--160},
  file = {Bengio et al_2007_Greedy Layer-Wise Training of Deep Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Bengio et al_2007_Greedy Layer-Wise Training of Deep Networks.pdf:application/pdf},
  groups = {Unsupervised Methods,Unsupervised Methods}
}

@article{socher_zero-shot_2013,
  title = {Zero-{{Shot Learning Through Cross}}-{{Modal Transfer}}},
  abstract = {This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.},
  timestamp = {2016-02-25T16:12:28Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.3666},
  primaryClass = {cs},
  urldate = {2016-02-25},
  journal = {arXiv:1301.3666 [cs]},
  author = {Socher, Richard and Ganjoo, Milind and Sridhar, Hamsa and Bastani, Osbert and Manning, Christopher D. and Ng, Andrew Y.},
  month = jan,
  year = {2013},
  note = {00100},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Read Now,Useful},
  file = {Socher et al_2013_Zero-Shot Learning Through Cross-Modal Transfer.pdf:/Users/fergalcotter/Dropbox/Papers/Socher et al_2013_Zero-Shot Learning Through Cross-Modal Transfer.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/78VPFZ2V/1301.html:text/html},
  groups = {Bayesian Models,Bayesian Models,Bayesian Models}
}

@article{noh_learning_2015,
  title = {Learning {{Deconvolution Network}} for {{Semantic Segmentation}}},
  abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5\%) among the methods trained with no external data through ensemble with the fully convolutional network.},
  timestamp = {2016-11-01T22:45:43Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.04366},
  primaryClass = {cs},
  urldate = {2016-10-28},
  journal = {arXiv:1505.04366 [cs]},
  author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
  month = may,
  year = {2015},
  note = {00136},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Noh et al_2015_Learning Deconvolution Network for Semantic Segmentation.pdf:/Users/fergalcotter/Dropbox/Papers/Noh et al_2015_Learning Deconvolution Network for Semantic Segmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/Q7VPUBXD/1505.html:text/html},
  groups = {Deconv,Deconv}
}

@article{goodfellow_generative_2014,
  title = {Generative {{Adversarial Networks}}},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  timestamp = {2016-02-02T20:14:53Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2661},
  primaryClass = {cs, stat},
  urldate = {2016-02-02},
  journal = {arXiv:1406.2661 [cs, stat]},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  month = jun,
  year = {2014},
  note = {00000},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {Goodfellow et al_2014_Generative Adversarial Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Goodfellow et al_2014_Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/XHXG68C8/1406.html:text/html},
  groups = {Visualization \& Generative,Visualization \& Generative,Visualization \& Generative}
}

@article{lin_network_2013,
  title = {Network {{In Network}}},
  abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
  timestamp = {2016-05-06T12:38:34Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.4400},
  primaryClass = {cs},
  urldate = {2016-04-18},
  journal = {arXiv:1312.4400 [cs]},
  author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  month = dec,
  year = {2013},
  note = {01717},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: 10 pages, 4 figures, for iclr2014},
  file = {Lin et al_2013_Network In Network.pdf:/Users/fergalcotter/Dropbox/Papers/Lin et al_2013_Network In Network.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/3Q3DA52E/1312.html:text/html},
  groups = {network features,network features,network features}
}

@incollection{smagt_solving_2012,
  series = {Lecture Notes in Computer Science},
  title = {Solving the {{Ill}}-{{Conditioning}} in {{Neural Network Learning}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {In this paper we investigate the feed-forward learning problem. The well-known ill-conditioning which is present in most feed-forward learning problems is shown to be the result of the structure of the network. Also, the well-known problem that weights between `higher' layers in the network have to settle before `lower' weights can converge is addressed. We present a solution to these problems by modifying the structure of the network through the addition of linear connections which carry shared weights. We call the new network structure the linearly augmented feed-forward network, and it is shown that the universal approximation theorems are still valid. Simulation experiments show the validity of the new method, and demonstrate that the new network is less sensitive to local minima and learns faster than the original network.},
  language = {en},
  timestamp = {2016-08-09T23:23:39Z},
  number = {7700},
  urldate = {2016-08-09},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {van der Smagt, Patrick and Hirzinger, Gerd},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  note = {00024},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Complexity,Computation by Abstract Devices,Information Systems Applications (incl. Internet),pattern recognition},
  pages = {191--203},
  file = {Smagt_Hirzinger_2012_Solving the Ill-Conditioning in Neural Network Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Smagt_Hirzinger_2012_Solving the Ill-Conditioning in Neural Network Learning.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/K5DUPI2F/978-3-642-35289-8_13.html:text/html},
  groups = {Tricks of the Trade,Tricks of the Trade},
  doi = {10.1007/978-3-642-35289-8_13}
}

@inproceedings{bouvrie_invariance_2009,
  title = {On Invariance in Hierarchical Models},
  timestamp = {2016-03-02T00:10:27Z},
  urldate = {2016-02-25},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bouvrie, Jake and Rosasco, Lorenzo and Poggio, Tomaso},
  year = {2009},
  note = {00037},
  pages = {162--170},
  file = {Bouvrie_2009_On Invariance in Hierarchical Models.pdf:/Users/fergalcotter/Dropbox/Papers/Bouvrie_2009_On Invariance in Hierarchical Models.pdf:application/pdf;[HTML] from nips.cc:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/FEW45XNW/3732-on-invariance-in-hierarchical-models.html:text/html},
  groups = {Biological Vision,Biological Vision}
}

@inproceedings{xie_aggregated_2016,
  title = {Aggregated {{Residual Transformations}} for {{Deep Neural Networks Saining Xie}}, {{Ross Girshick}}, {{Piotr Doll{\'a}r}}, {{Zhuowen Tu}}, {{Kaiming He}}},
  abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, codenamed ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart.},
  timestamp = {2016-11-23T01:36:14Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.05431},
  urldate = {2016-11-23},
  author = {Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  month = nov,
  year = {2016},
  file = {Xie et al_2016_Aggregated Residual Transformations for Deep Neural Networks Saining Xie, Ross.pdf:/Users/fergalcotter/Dropbox/Papers/Xie et al_2016_Aggregated Residual Transformations for Deep Neural Networks Saining Xie, Ross.pdf:application/pdf},
  groups = {state of the art,state of the art,state of the art}
}

@article{long_fully_2014,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
  timestamp = {2017-01-23T15:15:13Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.4038},
  primaryClass = {cs},
  urldate = {2017-01-23},
  journal = {arXiv:1411.4038 [cs]},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  month = nov,
  year = {2014},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote = {Comment: to appear in CVPR (2015)},
  file = {Long et al_2014_Fully Convolutional Networks for Semantic Segmentation.pdf:/Users/fergalcotter/Dropbox/Papers/Long et al_2014_Fully Convolutional Networks for Semantic Segmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/EZMS9QEN/1411.html:text/html},
  groups = {Deconv,Deconv}
}

@article{he_unsupervised_2013,
  title = {Unsupervised {{Feature Learning}} by {{Deep Sparse Coding}}},
  abstract = {In this paper, we propose a new unsupervised feature learning framework, namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer architecture for visual object recognition tasks. The main innovation of the framework is that it connects the sparse-encoders from different layers by a sparse-to-dense module. The sparse-to-dense module is a composition of a local spatial pooling step and a low-dimensional embedding process, which takes advantage of the spatial smoothness information in the image. As a result, the new method is able to learn several levels of sparse representation of the image which capture features at a variety of abstraction levels and simultaneously preserve the spatial smoothness between the neighboring image patches. Combining the feature representations from multiple layers, DeepSC achieves the state-of-the-art performance on multiple object recognition tasks.},
  timestamp = {2017-01-24T13:58:02Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.5783},
  primaryClass = {cs},
  urldate = {2017-01-24},
  journal = {arXiv:1312.5783 [cs]},
  author = {He, Yunlong and Kavukcuoglu, Koray and Wang, Yun and Szlam, Arthur and Qi, Yanjun},
  month = dec,
  year = {2013},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: 9 pages, submitted to ICLR},
  file = {He et al_2013_Unsupervised Feature Learning by Deep Sparse Coding.pdf:/Users/fergalcotter/Dropbox/Papers/He et al_2013_Unsupervised Feature Learning by Deep Sparse Coding.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/GFGSG5UP/1312.html:text/html},
  groups = {Sparse Coding,Sparse Coding}
}

@article{maaten_accelerating_2014,
  title = {Accelerating T-{{SNE}} Using {{Tree}}-{{Based Algorithms}}},
  volume = {15},
  timestamp = {2017-01-31T11:57:34Z},
  urldate = {2017-01-31},
  journal = {Journal of Machine Learning Research},
  author = {van der Maaten, Laurens},
  year = {2014},
  pages = {3221--3245},
  file = {Maaten_2014_Accelerating t-SNE using Tree-Based Algorithms.pdf:/Users/fergalcotter/Dropbox/Papers/Maaten_2014_Accelerating t-SNE using Tree-Based Algorithms.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/HBUTIJMP/vandermaaten14a.html:text/html},
  groups = {Visualization,Visualization}
}

@article{_visualizing_????,
  title = {Visualizing {{High}}-{{Dimensional Data}} Using t-{{SNE}}},
  abstract = {Visualizing High-Dimensional Data using t-SNE on ResearchGate, the professional network for scientists.},
  timestamp = {2017-01-31T11:58:10Z},
  urldate = {2017-01-31},
  journal = {ResearchGate},
  file = {Visualizing High-Dimensional Data using t-SNE.pdf:/Users/fergalcotter/Dropbox/Papers/Visualizing High-Dimensional Data using t-SNE.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/4T6XE3AW/303157903_Visualizing_High-Dimensional_Data_using_t-SNE.html:text/html},
  groups = {Visualization,Visualization}
}

@article{gangeh_supervised_2015,
  title = {Supervised {{Dictionary Learning}} and {{Sparse Representation}}-{{A Review}}},
  abstract = {Dictionary learning and sparse representation (DLSR) is a recent and successful mathematical model for data representation that achieves state-of-the-art performance in various fields such as pattern recognition, machine learning, computer vision, and medical imaging. The original formulation for DLSR is based on the minimization of the reconstruction error between the original signal and its sparse representation in the space of the learned dictionary. Although this formulation is optimal for solving problems such as denoising, inpainting, and coding, it may not lead to optimal solution in classification tasks, where the ultimate goal is to make the learned dictionary and corresponding sparse representation as discriminative as possible. This motivated the emergence of a new category of techniques, which is appropriately called supervised dictionary learning and sparse representation (S-DLSR), leading to more optimal dictionary and sparse representation in classification tasks. Despite many research efforts for S-DLSR, the literature lacks a comprehensive view of these techniques, their connections, advantages and shortcomings. In this paper, we address this gap and provide a review of the recently proposed algorithms for S-DLSR. We first present a taxonomy of these algorithms into six categories based on the approach taken to include label information into the learning of the dictionary and/or sparse representation. For each category, we draw connections between the algorithms in this category and present a unified framework for them. We then provide guidelines for applied researchers on how to represent and learn the building blocks of an S-DLSR solution based on the problem at hand. This review provides a broad, yet deep, view of the state-of-the-art methods for S-DLSR and allows for the advancement of research and development in this emerging area of research.},
  timestamp = {2017-02-07T22:18:15Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.05928},
  primaryClass = {cs},
  urldate = {2017-02-07},
  journal = {arXiv:1502.05928 [cs]},
  author = {Gangeh, Mehrdad J. and Farahat, Ahmed K. and Ghodsi, Ali and Kamel, Mohamed S.},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Gangeh et al_2015_Supervised Dictionary Learning and Sparse Representation-A Review.pdf:/Users/fergalcotter/Dropbox/Papers/Gangeh et al_2015_Supervised Dictionary Learning and Sparse Representation-A Review.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/CSVWQ567/1502.html:text/html},
  groups = {Sparse Coding,Sparse Coding}
}

@article{chen_atomic_1998,
  title = {Atomic {{Decomposition}} by {{Basis Pursuit}}},
  volume = {20},
  issn = {1064-8275},
  doi = {10.1137/S1064827596304010},
  abstract = {The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB).Basis Pursuit (BP) is a principle for decomposing a signal into an "optimal" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising.BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.},
  timestamp = {2017-02-07T20:49:07Z},
  number = {1},
  urldate = {2017-02-07},
  journal = {SIAM Journal on Scientific Computing},
  author = {Chen, S. and Donoho, D. and Saunders, M.},
  month = jan,
  year = {1998},
  pages = {33--61},
  file = {Chen et al_1998_Atomic Decomposition by Basis Pursuit.pdf:/Users/fergalcotter/Dropbox/Papers/Chen et al_1998_Atomic Decomposition by Basis Pursuit.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/KW3XP4N3/S1064827596304010.html:text/html},
  groups = {Pursuit Methods,Pursuit Methods,Pursuit Methods}
}

@article{tariyal_greedy_2016-1,
  title = {Greedy {{Deep Dictionary Learning}}},
  abstract = {In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVD. Our method yields better results than all.},
  timestamp = {2017-02-07T21:50:11Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.00203},
  primaryClass = {cs, stat},
  urldate = {2017-02-07},
  journal = {arXiv:1602.00203 [cs, stat]},
  author = {Tariyal, Snigdha and Majumdar, Angshul and Singh, Richa and Vatsa, Mayank},
  month = jan,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Statistics - Machine Learning},
  file = {Tariyal et al_2016_Greedy Deep Dictionary Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Tariyal et al_2016_Greedy Deep Dictionary Learning_2.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/NZC2B95A/1602.html:text/html},
  groups = {Sparse Coding,Sparse Coding}
}

@article{sulam_trainlets:_2016,
  title = {Trainlets: {{Dictionary Learning}} in {{High Dimensions}}},
  volume = {64},
  issn = {1053-587X, 1941-0476},
  shorttitle = {Trainlets},
  doi = {10.1109/TSP.2016.2540599},
  abstract = {Sparse representations has shown to be a very powerful model for real world signals, and has enabled the development of applications with notable performance. Combined with the ability to learn a dictionary from signal examples, sparsity-inspired algorithms are often achieving state-of-the-art results in a wide variety of tasks. Yet, these methods have traditionally been restricted to small dimensions mainly due to the computational constraints that the dictionary learning problem entails. In the context of image processing, this implies handling small image patches. In this work we show how to efficiently handle bigger dimensions and go beyond the small patches in sparsity-based signal and image processing methods. We build our approach based on a new cropped wavelet decomposition, which enables a multi-scale analysis with virtually no border effects. We then employ this as the base dictionary within a double sparsity model to enable the training of adaptive dictionaries. To cope with the increase of training data, while at the same time improving the training performance, we present an Online Sparse Dictionary Learning (OSDL) algorithm to train this model effectively, enabling it to handle millions of examples. This work shows that dictionary learning can be up-scaled to tackle a new level of signal dimensions, obtaining large adaptable atoms that we call trainlets.},
  timestamp = {2017-02-07T21:18:39Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.00212},
  number = {12},
  urldate = {2017-02-07},
  journal = {IEEE Transactions on Signal Processing},
  author = {Sulam, Jeremias and Ophir, Boaz and Zibulevsky, Michael and Elad, Michael},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Key Paper},
  pages = {3180--3193},
  file = {Sulam et al_2016_Trainlets.pdf:/Users/fergalcotter/Dropbox/Papers/Sulam et al_2016_Trainlets.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/IKVQQDFJ/1602.html:text/html},
  groups = {Sparse Coding,Sparse Coding}
}

@article{mairal_sparse_2014,
  title = {Sparse {{Modeling}} for {{Image}} and {{Vision Processing}}},
  volume = {8},
  issn = {1572-2740, 1572-2759},
  doi = {10.1561/0600000058},
  abstract = {Sparse Modeling for Image and Vision Processing},
  language = {English},
  timestamp = {2017-02-07T21:59:45Z},
  number = {2-3},
  urldate = {2017-02-07},
  journal = {Foundations and Trends\textregistered{} in Computer Graphics and Vision},
  author = {Mairal, Julien and Bach, Francis and Ponce, Jean},
  month = dec,
  year = {2014},
  pages = {85--283},
  file = {Mairal et al_2014_Sparse Modeling for Image and Vision Processing.pdf:/Users/fergalcotter/Dropbox/Papers/Mairal et al_2014_Sparse Modeling for Image and Vision Processing.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/DA7CGKHS/CGV-058.html:text/html},
  groups = {Sparse Coding,Sparse Coding}
}

@article{aharon_k-svd:_2006,
  title = {K-{{SVD}}: {{An Algorithm}} for {{Designing Overcomplete Dictionaries}} for {{Sparse Representation}}},
  volume = {54},
  issn = {1053-587X},
  shorttitle = {-{{SVD}}},
  doi = {10.1109/TSP.2006.881199},
  abstract = {In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method-the K-SVD algorithm-generalizing the K-means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data},
  timestamp = {2017-02-07T21:47:00Z},
  number = {11},
  journal = {IEEE Transactions on Signal Processing},
  author = {Aharon, M. and Elad, M. and Bruckstein, A.},
  month = nov,
  year = {2006},
  keywords = {Algorithm design and analysis,Atom decomposition,basis pursuit,Clustering algorithms,codebook,Dictionaries,dictionary,Feature extraction,FOCUSS,gain-shape VQ,Image coding,image data,Image representation,Inverse problems,Iterative algorithms,iterative method,iterative methods,K-means clustering process,K-SVD,linear transforms,matching pursuit,Matching pursuit algorithms,overcomplete dictionary,Prototypes,Pursuit algorithms,Signal design,signals sparse representation,singular value decomposition,sparse coding,sparse representation,sparsity constraints,Training,transforms,vector quantization},
  pages = {4311--4322},
  file = {Aharon et al_2006_-SVD.pdf:/Users/fergalcotter/Dropbox/Papers/Aharon et al_2006_-SVD.pdf:application/pdf;ksvd_notes.pdf:/Users/fergalcotter/Dropbox/Papers/Notes/ksvd_notes.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/XXB5BWC4/1710377.html:text/html},
  groups = {Sparse Coding,Sparse Coding}
}

@article{mallat_matching_1993,
  title = {Matching Pursuits with Time-Frequency Dictionaries},
  volume = {41},
  issn = {1053-587X},
  doi = {10.1109/78.258082},
  abstract = {The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992)},
  timestamp = {2017-02-07T20:49:49Z},
  number = {12},
  journal = {IEEE Transactions on Signal Processing},
  author = {Mallat, S. G. and Zhang, Zhifeng},
  month = dec,
  year = {1993},
  keywords = {adaptive signal representations,adaptive time-frequency transform,Dictionaries,Fourier transforms,Gabor functions,Interference,linear waveform expansion,matching pursuit algorithm,Matching pursuit algorithms,matching pursuit decomposition,Natural languages,noisy signals,optimized wavepacket orthonormal basis,pattern extraction,Pursuit algorithms,signal energy distribution,signal expansion,signal processing,Signal processing algorithms,Signal representations,signal structures,Time frequency analysis,time-frequency analysis,time-frequency dictionaries,time-frequency plane,Vocabulary,wavelet transforms},
  pages = {3397--3415},
  file = {Mallat_Zhang_1993_Matching pursuits with time-frequency dictionaries.pdf:/Users/fergalcotter/Dropbox/Papers/Mallat_Zhang_1993_Matching pursuits with time-frequency dictionaries.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/B8W6HX52/258082.html:text/html},
  groups = {Pursuit Methods,Pursuit Methods,Pursuit Methods}
}

@article{papyan_convolutional_2016,
  title = {Convolutional {{Neural Networks Analyzed}} via {{Convolutional Sparse Coding}}},
  abstract = {Convolutional neural networks (CNN) have led to many state-of-the-art results spanning through various fields. However, a clear and profound theoretical understanding of the forward pass, the core algorithm of CNN, is still lacking. In parallel, within the wide field of sparse approximation, Convolutional Sparse Coding (CSC) has gained increasing attention in recent years. A theoretical study of this model was recently conducted, establishing it as a reliable and stable alternative to the commonly practiced patch-based processing. Herein, we propose a novel multi-layer model, ML-CSC, in which signals are assumed to emerge from a cascade of CSC layers. This is shown to be tightly connected to CNN, so much so that the forward pass of the CNN is in fact the thresholding pursuit serving the ML-CSC model. This connection brings a fresh view to CNN, as we are able to attribute to this architecture theoretical claims such as uniqueness of the representations throughout the network, and their stable estimation, all guaranteed under simple local sparsity conditions. Lastly, identifying the weaknesses in the above pursuit scheme, we propose an alternative to the forward pass, which is connected to deconvolutional, recurrent and residual networks, and has better theoretical guarantees.},
  timestamp = {2017-02-13T19:25:34Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.08194},
  primaryClass = {cs, stat},
  urldate = {2017-02-13},
  journal = {arXiv:1607.08194 [cs, stat]},
  author = {Papyan, Vardan and Romano, Yaniv and Elad, Michael},
  month = jul,
  year = {2016},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {Papyan et al_2016_Convolutional Neural Networks Analyzed via Convolutional Sparse Coding.pdf:/Users/fergalcotter/Dropbox/Papers/Papyan et al_2016_Convolutional Neural Networks Analyzed via Convolutional Sparse Coding.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/B3T6WVHP/1607.html:text/html},
  groups = {Sparse Coding,Sparse Coding}
}

@inproceedings{_expected_????,
  title = {Expected {{Patch Log Likelihood}} with a {{Sparse Prior}} ({{PDF Download Available}})},
  doi = {http://dx.doi.org/10.1007/978-3-319-14612-6_8},
  abstract = {Official Full-Text Publication: Expected Patch Log Likelihood with a Sparse Prior on ResearchGate, the professional network for scientists.},
  timestamp = {2017-02-13T21:30:31Z},
  urldate = {2017-02-13},
  booktitle = {{{ResearchGate}}},
  file = {Expected Patch Log Likelihood with a Sparse Prior (PDF Download Available).pdf:/Users/fergalcotter/Dropbox/Papers/Expected Patch Log Likelihood with a Sparse Prior (PDF Download Available).pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/XQEF7U4R/280297742_Expected_Patch_Log_Likelihood_with_a_Sparse_Prior.html:text/html},
  groups = {Sparse Coding,Sparse Coding}
}

@article{rubinstein_dictionaries_2010,
  title = {Dictionaries for {{Sparse Representation Modeling}}},
  volume = {98},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2010.2040551},
  abstract = {Sparse and redundant representation modeling of data assumes an ability to describe signals as linear combinations of a few atoms from a pre-specified dictionary. As such, the choice of the dictionary that sparsifies the signals is crucial for the success of this model. In general, the choice of a proper dictionary can be done using one of two ways: i) building a sparsifying dictionary based on a mathematical model of the data, or ii) learning a dictionary to perform best on a training set. In this paper we describe the evolution of these two paradigms. As manifestations of the first approach, we cover topics such as wavelets, wavelet packets, contourlets, and curvelets, all aiming to exploit 1-D and 2-D mathematical models for constructing effective dictionaries for signals and images. Dictionary learning takes a different route, attaching the dictionary to a set of examples it is supposed to serve. From the seminal work of Field and Olshausen, through the MOD, the K-SVD, the Generalized PCA and others, this paper surveys the various options such training has to offer, up to the most recent contributions and structures.},
  timestamp = {2017-02-14T01:37:34Z},
  number = {6},
  journal = {Proceedings of the IEEE},
  author = {Rubinstein, R. and Bruckstein, A. M. and Elad, M.},
  month = jun,
  year = {2010},
  keywords = {Dictionaries,dictionary learning,Displays,Harmonic analysis,Joining processes,Key Paper,mathematical data model,Mathematical model,principal component analysis,redundant signal representation modeling,Sampling methods,signal approximation,signal processing,signal representation,Signal representations,signal sampling,sparse coding,sparse representation,sparse signal representation modeling,training set,Wavelet packets,wavelet transforms},
  pages = {1045--1057},
  file = {Rubinstein et al_2010_Dictionaries for Sparse Representation Modeling.pdf:/Users/fergalcotter/Dropbox/Papers/Rubinstein et al_2010_Dictionaries for Sparse Representation Modeling.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/WNJQUHTU/5452966.html:text/html},
  groups = {Sparse Coding,Sparse Coding}
}

@article{rubinstein_double_2010,
  title = {Double {{Sparsity}}: {{Learning Sparse Dictionaries}} for {{Sparse Signal Approximation}}},
  volume = {58},
  issn = {1053-587X},
  shorttitle = {Double {{Sparsity}}},
  doi = {10.1109/TSP.2009.2036477},
  abstract = {An efficient and flexible dictionary structure is proposed for sparse and redundant signal representation. The proposed sparse dictionary is based on a sparsity model of the dictionary atoms over a base dictionary, and takes the form D = ?? A, where ?? is a fixed base dictionary and A is sparse. The sparse dictionary provides efficient forward and adjoint operators, has a compact representation, and can be effectively trained from given example data. In this, the sparse structure bridges the gap between implicit dictionaries, which have efficient implementations yet lack adaptability, and explicit dictionaries, which are fully adaptable but non-efficient and costly to deploy. In this paper, we discuss the advantages of sparse dictionaries, and present an efficient algorithm for training them. We demonstrate the advantages of the proposed structure for 3-D image denoising.},
  timestamp = {2017-02-14T13:22:28Z},
  number = {3},
  journal = {IEEE Transactions on Signal Processing},
  author = {Rubinstein, R. and Zibulevsky, M. and Elad, M.},
  month = mar,
  year = {2010},
  keywords = {3D image denoising,computed tomography,dictionary learning,double sparsity,Image coding,image denoising,K-SVD,learning sparse dictionaries,signal denoising,signal representation,sparse coding,sparse matrices,sparse representation,sparse signal approximation},
  pages = {1553--1564},
  file = {Rubinstein et al_2010_Double Sparsity.pdf:/Users/fergalcotter/Dropbox/Papers/Rubinstein et al_2010_Double Sparsity.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/QDIJ7KZV/5325694.html:text/html},
  groups = {Sparse Coding,Sparse Coding}
}

@article{adler_compressed_2016,
  title = {Compressed {{Learning}}: {{A Deep Neural Network Approach}}},
  shorttitle = {Compressed {{Learning}}},
  abstract = {Compressed Learning (CL) is a joint signal processing and machine learning framework for inference from a signal, using a small number of measurements obtained by linear projections of the signal. In this paper we present an end-to-end deep learning approach for CL, in which a network composed of fully-connected layers followed by convolutional layers perform the linear sensing and non-linear inference stages. During the training phase, the sensing matrix and the non-linear inference operator are jointly optimized, and the proposed approach outperforms state-of-the-art for the task of image classification. For example, at a sensing rate of 1\% (only 8 measurements of 28 X 28 pixels images), the classification error for the MNIST handwritten digits dataset is 6.46\% compared to 41.06\% with state-of-the-art.},
  timestamp = {2017-02-14T13:38:01Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.09615},
  primaryClass = {cs},
  urldate = {2017-02-14},
  journal = {arXiv:1610.09615 [cs]},
  author = {Adler, Amir and Elad, Michael and Zibulevsky, Michael},
  month = oct,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Useful},
  file = {Adler et al_2016_Compressed Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Adler et al_2016_Compressed Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/ET8FKJSV/1610.html:text/html},
  groups = {Compressed Learning,Compressed Learning,Compressed Learning}
}

@inproceedings{mairal_online_2009,
  address = {New York, NY, USA},
  series = {ICML '09},
  title = {Online {{Dictionary Learning}} for {{Sparse Coding}}},
  isbn = {978-1-60558-516-1},
  doi = {10.1145/1553374.1553463},
  abstract = {Sparse coding---that is, modelling data vectors as sparse linear combinations of basis elements---is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the audio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and large datasets.},
  timestamp = {2017-02-15T12:54:25Z},
  urldate = {2017-02-15},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  publisher = {{ACM}},
  author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
  year = {2009},
  pages = {689--696},
  file = {Mairal et al_2009_Online Dictionary Learning for Sparse Coding.pdf:/Users/fergalcotter/Dropbox/Papers/Mairal et al_2009_Online Dictionary Learning for Sparse Coding.pdf:application/pdf},
  groups = {Compressed Learning,Compressed Learning,Compressed Learning}
}

@incollection{goodfellow_generative_2014-1,
  title = {Generative {{Adversarial Nets}}},
  timestamp = {2017-02-15T20:30:14Z},
  urldate = {2017-02-15},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  publisher = {{Curran Associates, Inc.}},
  author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  pages = {2672--2680},
  file = {Goodfellow et al_2014_Generative Adversarial Nets.pdf:/Users/fergalcotter/Dropbox/Papers/Goodfellow et al_2014_Generative Adversarial Nets.pdf:application/pdf;NIPS Snapshort:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/NM7PW2TB/5423-generative-adversarial-nets.html:text/html},
  groups = {gan,gan,gan}
}

@article{goodfellow_nips_2016,
  title = {{{NIPS}} 2016 {{Tutorial}}: {{Generative Adversarial Networks}}},
  shorttitle = {{{NIPS}} 2016 {{Tutorial}}},
  abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
  timestamp = {2017-02-15T20:30:03Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.00160},
  primaryClass = {cs},
  urldate = {2017-02-15},
  journal = {arXiv:1701.00160 [cs]},
  author = {Goodfellow, Ian},
  month = dec,
  year = {2016},
  keywords = {Computer Science - Learning},
  annote = {Comment: v2 and v3 are both typo fixes. No substantive changes relative to v1},
  file = {Goodfellow_2016_NIPS 2016 Tutorial.pdf:/Users/fergalcotter/Dropbox/Papers/Goodfellow_2016_NIPS 2016 Tutorial.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/2GD4N3H2/1701.html:text/html},
  groups = {gan,gan,gan}
}

@article{radford_unsupervised_2015,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  timestamp = {2017-02-15T20:28:55Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.06434},
  primaryClass = {cs},
  urldate = {2017-02-15},
  journal = {arXiv:1511.06434 [cs]},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  month = nov,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  annote = {Comment: Under review as a conference paper at ICLR 2016},
  file = {Radford et al_2015_Unsupervised Representation Learning with Deep Convolutional Generative.pdf:/Users/fergalcotter/Dropbox/Papers/Radford et al_2015_Unsupervised Representation Learning with Deep Convolutional Generative.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/NHXIPIKA/1511.html:text/html},
  groups = {gan,gan,gan}
}

@article{zhang_accelerating_2015,
  title = {Accelerating {{Very Deep Convolutional Networks}} for {{Classification}} and {{Detection}}},
  abstract = {This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs that have substantially impacted the computer vision community. Unlike previous methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly, while previous methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (e.g., $>$=10) layers are approximated. For the widely used very deep VGG-16 model, our method achieves a whole-model speedup of 4x with merely a 0.3\% increase of top-5 error in ImageNet classification. Our 4x accelerated VGG-16 model also shows a graceful accuracy degradation for object detection when plugged into the Fast R-CNN detector.},
  timestamp = {2017-02-21T00:12:22Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.06798},
  primaryClass = {cs},
  urldate = {2017-02-21},
  journal = {arXiv:1505.06798 [cs]},
  author = {Zhang, Xiangyu and Zou, Jianhua and He, Kaiming and Sun, Jian},
  month = may,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: TPAMI, accepted. arXiv admin note: substantial text overlap with arXiv:1411.4229},
  file = {Zhang et al_2015_Accelerating Very Deep Convolutional Networks for Classification and Detection.pdf:/Users/fergalcotter/Dropbox/Papers/Zhang et al_2015_Accelerating Very Deep Convolutional Networks for Classification and Detection.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/8MR8FV39/1505.html:text/html},
  groups = {CNNS,CNNS}
}

@article{denton_exploiting_2014,
  title = {Exploiting {{Linear Structure Within Convolutional Networks}} for {{Efficient Evaluation}}},
  abstract = {We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the linear structure present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2x, while keeping the accuracy within 1\% of the original model.},
  timestamp = {2017-02-21T00:24:29Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1404.0736},
  primaryClass = {cs},
  urldate = {2017-02-21},
  journal = {arXiv:1404.0736 [cs]},
  author = {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  month = apr,
  year = {2014},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  annote = {They take a trained network, and look at ways to compress its representation afterwards, to make it quicker to compute. This is very interesting, as some of the things they find are clues as to what is redundant in a CNN.
~
First two layers
E.g. FFT for convolution
SVD decomposition for matrices
They also do some really important things like explore what is a meaningful way to approximate a tensor.
~},
  file = {Denton et al_2014_Exploiting Linear Structure Within Convolutional Networks for Efficient.pdf:/Users/fergalcotter/Dropbox/Papers/Denton et al_2014_Exploiting Linear Structure Within Convolutional Networks for Efficient.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/ZAK9TKEZ/1404.html:text/html},
  groups = {CNNS,CNNS}
}

@article{denton_deep_2015,
  title = {Deep {{Generative Image Models}} Using a {{Laplacian Pyramid}} of {{Adversarial Networks}}},
  abstract = {In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach (Goodfellow et al.). Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40\% of the time, compared to 10\% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.},
  timestamp = {2017-02-21T21:36:14Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.05751},
  primaryClass = {cs},
  urldate = {2017-02-21},
  journal = {arXiv:1506.05751 [cs]},
  author = {Denton, Emily and Chintala, Soumith and Szlam, Arthur and Fergus, Rob},
  month = jun,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Denton et al_2015_Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Denton et al_2015_Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/2MCCPMSW/1506.html:text/html},
  groups = {gan,gan,gan}
}

@article{salimans_improved_2016,
  title = {Improved {{Techniques}} for {{Training GANs}}},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  timestamp = {2017-02-21T21:40:23Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.03498},
  primaryClass = {cs},
  urldate = {2017-02-21},
  journal = {arXiv:1606.03498 [cs]},
  author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Salimans et al_2016_Improved Techniques for Training GANs.pdf:/Users/fergalcotter/Dropbox/Papers/Salimans et al_2016_Improved Techniques for Training GANs.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/6UCWQ2HP/1606.html:text/html},
  groups = {gan,gan,gan}
}

@article{springenberg_striving_2014-2,
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  shorttitle = {Striving for {{Simplicity}}},
  abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
  timestamp = {2017-02-21T21:14:00Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6806},
  primaryClass = {cs},
  urldate = {2017-02-21},
  journal = {arXiv:1412.6806 [cs]},
  author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  month = dec,
  year = {2014},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: accepted to ICLR-2015 workshop track; no changes other than style},
  file = {Springenberg et al_2014_Striving for Simplicity.pdf:/Users/fergalcotter/Dropbox/Papers/Springenberg et al_2014_Striving for Simplicity_3.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/8JCSQR73/1412.html:text/html},
  groups = {Fun,Fun}
}

@article{jaderberg_speeding_2014-1,
  title = {Speeding up {{Convolutional Neural Networks}} with {{Low Rank Expansions}}},
  abstract = {The focus of this paper is speeding up the evaluation of convolutional neural networks. While delivering impressive results across a range of computer vision and machine learning tasks, these networks are computationally demanding, limiting their deployability. Convolutional layers generally consume the bulk of the processing time, and so in this work we present two simple schemes for drastically speeding up these layers. This is achieved by exploiting cross-channel or filter redundancy to construct a low rank basis of filters that are rank-1 in the spatial domain. Our methods are architecture agnostic, and can be easily applied to existing CPU and GPU convolutional frameworks for tuneable speedup performance. We demonstrate this with a real world network designed for scene text character recognition, showing a possible 2.5x speedup with no loss in accuracy, and 4.5x speedup with less than 1\% drop in accuracy, still achieving state-of-the-art on standard benchmarks.},
  timestamp = {2017-02-21T12:29:28Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1405.3866},
  primaryClass = {cs},
  urldate = {2017-02-21},
  journal = {arXiv:1405.3866 [cs]},
  author = {Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  month = may,
  year = {2014},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Jaderberg et al_2014_Speeding up Convolutional Neural Networks with Low Rank Expansions.pdf:/Users/fergalcotter/Dropbox/Papers/Jaderberg et al_2014_Speeding up Convolutional Neural Networks with Low Rank Expansions_2.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/F6DUMUDM/1405.html:text/html},
  groups = {CNNS,CNNS}
}

@article{goodfellow_generative_2014-2,
  title = {Generative {{Adversarial Networks}}},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  timestamp = {2017-02-21T23:59:09Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2661},
  primaryClass = {cs, stat},
  urldate = {2017-02-21},
  journal = {arXiv:1406.2661 [cs, stat]},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  month = jun,
  year = {2014},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {Goodfellow et al_2014_Generative Adversarial Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Goodfellow et al_2014_Generative Adversarial Networks_2.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/VW9RF5DG/1406.html:text/html},
  groups = {gan,gan,gan}
}

@inproceedings{rigamonti_learning_2013,
  title = {Learning {{Separable Filters}}},
  doi = {10.1109/CVPR.2013.355},
  abstract = {Learning filters to produce sparse image representations in terms of over complete dictionaries has emerged as a powerful way to create image features for many different purposes. Unfortunately, these filters are usually both numerous and non-separable, making their use computationally expensive. In this paper, we show that such filters can be computed as linear combinations of a smaller number of separable ones, thus greatly reducing the computational complexity at no cost in terms of performance. This makes filter learning approaches practical even for large images or 3D volumes, and we show that we significantly outperform state-of-the-art methods on the linear structure extraction task, in terms of both accuracy and speed. Moreover, our approach is general and can be used on generic filter banks to reduce the complexity of the convolutions.},
  timestamp = {2017-02-22T11:58:30Z},
  booktitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Rigamonti, R. and Sironi, A. and Lepetit, V. and Fua, P.},
  month = jun,
  year = {2013},
  keywords = {Biomedical imaging,computational complexity,computer vision,Convolution,convolution complexity reduction,Dictionaries,Feature extraction,filtering theory,filter learning approaches,generic filter banks,Image representation,learning separable filters,Linear programming,linear structure extraction task,optimization,sparse image representations,Three-dimensional displays},
  pages = {2754--2761},
  file = {Rigamonti et al_2013_Learning Separable Filters.pdf:/Users/fergalcotter/Dropbox/Papers/Rigamonti et al_2013_Learning Separable Filters.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/X6DQBAPW/6619199.html:text/html},
  groups = {Sparse Coding,CNNS,Sparse Coding,CNNS}
}

@article{huang_densely_2016,
  title = {Densely {{Connected Convolutional Networks}}},
  abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and models are available at https://github.com/liuzhuang13/DenseNet .},
  timestamp = {2017-03-02T11:59:41Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.06993},
  primaryClass = {cs},
  urldate = {2017-03-02},
  journal = {arXiv:1608.06993 [cs]},
  author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q. and {van der Maaten}, Laurens},
  month = aug,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  annote = {Comment: 12 pages},
  file = {Huang et al_2016_Densely Connected Convolutional Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Huang et al_2016_Densely Connected Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/56IB4VD2/1608.html:text/html},
  groups = {Reading Groups,Reading Groups}
}

@article{veit_residual_2016,
  title = {Residual {{Networks Behave Like Ensembles}} of {{Relatively Shallow Networks}}},
  abstract = {In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
  timestamp = {2017-03-02T12:01:03Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.06431},
  primaryClass = {cs},
  urldate = {2017-03-02},
  journal = {arXiv:1605.06431 [cs]},
  author = {Veit, Andreas and Wilber, Michael and Belongie, Serge},
  month = may,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: NIPS 2016},
  file = {Veit et al_2016_Residual Networks Behave Like Ensembles of Relatively Shallow Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Veit et al_2016_Residual Networks Behave Like Ensembles of Relatively Shallow Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/U8BVPSPN/1605.html:text/html},
  groups = {Reading Groups,Reading Groups}
}

@article{johnson_perceptual_2016,
  title = {Perceptual {{Losses}} for {{Real}}-{{Time Style Transfer}} and {{Super}}-{{Resolution}}},
  abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a $\backslash$emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing $\backslash$emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
  timestamp = {2017-03-08T15:00:41Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.08155},
  primaryClass = {cs},
  urldate = {2017-03-08},
  journal = {arXiv:1603.08155 [cs]},
  author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  file = {Johnson et al_2016_Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf:/Users/fergalcotter/Dropbox/Papers/Johnson et al_2016_Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/KQNHMKDT/1603.html:text/html},
  groups = {style,style,style}
}

@article{he_powerful_2016,
  title = {A {{Powerful Generative Model Using Random Weights}} for the {{Deep Image Representation}}},
  abstract = {To what extent is the success of deep visualization due to the training? Could we do deep visualization using untrained, random weight networks? To address this issue, we explore new and powerful generative models for three popular deep visualization tasks using untrained, random weight convolutional neural networks. First we invert representations in feature spaces and reconstruct images from white noise inputs. The reconstruction quality is statistically higher than that of the same method applied on well trained networks with the same architecture. Next we synthesize textures using scaled correlations of representations in multiple layers and our results are almost indistinguishable with the original natural texture and the synthesized textures based on the trained network. Third, by recasting the content of an image in the style of various artworks, we create artistic images with high perceptual quality, highly competitive to the prior work of Gatys et al. on pretrained networks. To our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the representation of deep network architecture and sheds light on new understandings on deep visualization.},
  timestamp = {2017-03-09T20:17:39Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.04801},
  primaryClass = {cs},
  urldate = {2017-03-09},
  journal = {arXiv:1606.04801 [cs]},
  author = {He, Kun and Wang, Yan and Hopcroft, John},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: 10 pages, 10 figures, submited to NIPS 2016 conference. Computer Vision and Pattern Recognition, Neurons and Cognition, Neural and Evolutionary Computing},
  file = {He et al_2016_A Powerful Generative Model Using Random Weights for the Deep Image.pdf:/Users/fergalcotter/Dropbox/Papers/He et al_2016_A Powerful Generative Model Using Random Weights for the Deep Image.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/ETV53D68/1606.html:text/html},
  groups = {priors,priors,priors}
}

@article{dosovitskiy_generating_2016,
  title = {Generating {{Images}} with {{Perceptual Similarity Metrics}} Based on {{Deep Networks}}},
  abstract = {Image-generating machine learning models are typically trained with loss functions based on distance in the image space. This often leads to over-smoothed results. We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric better reflects perceptually similarity of images and thus leads to better results. We show three applications: autoencoder training, a modification of a variational autoencoder, and inversion of deep convolutional networks. In all cases, the generated images look sharp and resemble natural images.},
  timestamp = {2017-03-09T21:37:03Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.02644},
  primaryClass = {cs},
  urldate = {2017-03-09},
  journal = {arXiv:1602.02644 [cs]},
  author = {Dosovitskiy, Alexey and Brox, Thomas},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: minor corrections},
  file = {Dosovitskiy_Brox_2016_Generating Images with Perceptual Similarity Metrics based on Deep Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Dosovitskiy_Brox_2016_Generating Images with Perceptual Similarity Metrics based on Deep Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/KMGSNMXW/1602.html:text/html},
  groups = {priors,priors,priors}
}

@article{nguyen_synthesizing_2016,
  title = {Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks},
  abstract = {Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right - similar to why we study the human brain - and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).},
  timestamp = {2017-03-09T21:13:09Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.09304},
  primaryClass = {cs},
  urldate = {2017-03-09},
  journal = {arXiv:1605.09304 [cs]},
  author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
  month = may,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: 29 pages, 35 figures, NIPS camera-ready},
  file = {Nguyen et al_2016_Synthesizing the preferred inputs for neurons in neural networks via deep_2.pdf:/Users/fergalcotter/Dropbox/Papers/Nguyen et al_2016_Synthesizing the preferred inputs for neurons in neural networks via deep_2.pdf:application/pdf;arXiv\:1605.09304 PDF:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/D62NE4UU/Nguyen et al. - 2016 - Synthesizing the preferred inputs for neurons in n.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/8DNN3F99/1605.html:text/html},
  groups = {priors,priors,priors}
}

@article{yosinski_understanding_2015,
  title = {Understanding {{Neural Networks Through Deep Visualization}}},
  abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
  timestamp = {2017-03-09T21:33:15Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.06579},
  primaryClass = {cs},
  urldate = {2017-03-09},
  journal = {arXiv:1506.06579 [cs]},
  author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  month = jun,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: 12 pages. To appear at ICML Deep Learning Workshop 2015},
  file = {Yosinski et al_2015_Understanding Neural Networks Through Deep Visualization.pdf:/Users/fergalcotter/Dropbox/Papers/Yosinski et al_2015_Understanding Neural Networks Through Deep Visualization.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/9669FIIF/1506.html:text/html},
  groups = {priors,priors,priors}
}

@article{nguyen_multifaceted_2016,
  title = {Multifaceted {{Feature Visualization}}: {{Uncovering}} the {{Different Types}} of {{Features Learned By Each Neuron}} in {{Deep Neural Networks}}},
  shorttitle = {Multifaceted {{Feature Visualization}}},
  abstract = {We can better understand deep neural networks by identifying which features each of their neurons have learned to detect. To do so, researchers have created Deep Visualization techniques including activation maximization, which synthetically generates inputs (e.g. images) that maximally activate each neuron. A limitation of current techniques is that they assume each neuron detects only one type of feature, but we know that neurons can be multifaceted, in that they fire in response to many different types of features: for example, a grocery store class neuron must activate either for rows of produce or for a storefront. Previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron, creating inappropriate mixes of colors, parts of objects, scales, orientations, etc. Here, we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron. We also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization. By separately synthesizing each type of image a neuron fires in response to, the visualizations have more appropriate colors and coherent global structure. Multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron.},
  timestamp = {2017-03-09T21:07:38Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.03616},
  primaryClass = {cs},
  urldate = {2017-03-09},
  journal = {arXiv:1602.03616 [cs]},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: 23 pages (including SI), 24 figures},
  file = {Nguyen et al_2016_Multifaceted Feature Visualization.pdf:/Users/fergalcotter/Dropbox/Papers/Nguyen et al_2016_Multifaceted Feature Visualization.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/RPE492AD/1602.html:text/html},
  groups = {priors,priors,priors}
}

@article{nguyen_plug_2016,
  title = {Plug \& {{Play Generative Networks}}: {{Conditional Iterative Generation}} of {{Images}} in {{Latent Space}}},
  shorttitle = {Plug \& {{Play Generative Networks}}},
  abstract = {Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. (2016) showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227x227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models "Plug and Play Generative Networks". PPGNs are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replaceable "condition" network C that tells the generator what to draw. We demonstrate the generation of images conditioned on a class (when C is an ImageNet or MIT Places classification network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visualization, which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.},
  timestamp = {2017-03-09T21:09:55Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.00005},
  primaryClass = {cs},
  urldate = {2017-03-09},
  journal = {arXiv:1612.00005 [cs]},
  author = {Nguyen, Anh and Yosinski, Jason and Bengio, Yoshua and Dosovitskiy, Alexey and Clune, Jeff},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Nguyen et al_2016_Plug & Play Generative Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Nguyen et al_2016_Plug & Play Generative Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/99GT8GRH/1612.html:text/html},
  groups = {priors,priors,priors}
}

@article{guberman_complex_2016,
  title = {On {{Complex Valued Convolutional Neural Networks}}},
  abstract = {Convolutional neural networks (CNNs) are the cutting edge model for supervised machine learning in computer vision. In recent years CNNs have outperformed traditional approaches in many computer vision tasks such as object detection, image classification and face recognition. CNNs are vulnerable to overfitting, and a lot of research focuses on finding regularization methods to overcome it. One approach is designing task specific models based on prior knowledge. Several works have shown that properties of natural images can be easily captured using complex numbers. Motivated by these works, we present a variation of the CNN model with complex valued input and weights. We construct the complex model as a generalization of the real model. Lack of order over the complex field raises several difficulties both in the definition and in the training of the network. We address these issues and suggest possible solutions. The resulting model is shown to be a restricted form of a real valued CNN with twice the parameters. It is sensitive to phase structure, and we suggest it serves as a regularized model for problems where such structure is important. This suggestion is verified empirically by comparing the performance of a complex and a real network in the problem of cell detection. The two networks achieve comparable results, and although the complex model is hard to train, it is significantly less vulnerable to overfitting. We also demonstrate that the complex network detects meaningful phase structure in the data.},
  timestamp = {2017-04-19T18:22:20Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.09046},
  primaryClass = {cs},
  urldate = {2017-04-19},
  journal = {arXiv:1602.09046 [cs]},
  author = {Guberman, Nitzan},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: M.Sc. thesis},
  file = {Guberman_2016_On Complex Valued Convolutional Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Guberman_2016_On Complex Valued Convolutional Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/QHNA49US/1602.html:text/html},
  groups = {Complex CNNs,Complex CNNs}
}

@article{kim_fully_2002,
  title = {Fully {{Complex Multi}}-{{Layer Perceptron Network}} for {{Nonlinear Signal Processing}}},
  volume = {32},
  issn = {0922-5773},
  doi = {10.1023/A:1016359216961},
  abstract = {Designing a neural network (NN) to process complex-valued signals is a challenging task since a complex nonlinear activation function (AF) cannot be both analytic and bounded everywhere in the complex plane $\mathbb{C}$. To avoid this difficulty, `splitting', i.e., using a pair of real sigmoidal functions for the real and imaginary components has been the traditional approach. However, this `ad hoc' compromise to avoid the unbounded nature of nonlinear complex functions results in a nowhere analytic AF that performs the error back-propagation (BP) using the split derivatives of the real and imaginary components instead of relying on well-defined fully complex derivatives. In this paper, a fully complex multi-layer perceptron (MLP) structure that yields a simplified complex-valued back-propagation (BP) algorithm is presented. The simplified BP verifies that the fully complex BP weight update formula is the complex conjugate form of real BP formula and the split complex BP is a special case of the fully complex BP. This generalization is possible by employing elementary transcendental functions (ETFs) that are almost everywhere (a.e.) bounded and analytic in $\mathbb{C}$. The properties of fully complex MLP are investigated and the advantage of ETFs over split complex AF is shown in numerical examples where nonlinear magnitude and phase distortions of non-constant modulus modulated signals are successfully restored.},
  language = {en},
  timestamp = {2017-04-19T18:37:30Z},
  number = {1-2},
  urldate = {2017-04-19},
  journal = {Journal of VLSI signal processing systems for signal, image and video technology},
  author = {Kim, Taehwan and Adali, T{\"u}lay},
  month = aug,
  year = {2002},
  pages = {29--43},
  file = {Kim_Adali_2002_Fully Complex Multi-Layer Perceptron Network for Nonlinear Signal Processing.pdf:/Users/fergalcotter/Dropbox/Papers/Kim_Adali_2002_Fully Complex Multi-Layer Perceptron Network for Nonlinear Signal Processing.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/RZK7F6Q2/A1016359216961.html:text/html},
  groups = {Complex CNNs,Complex CNNs}
}

@article{amodei_concrete_2016,
  title = {Concrete {{Problems}} in {{AI Safety}}},
  abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
  timestamp = {2017-04-20T12:59:49Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.06565},
  primaryClass = {cs},
  urldate = {2017-04-20},
  journal = {arXiv:1606.06565 [cs]},
  author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning},
  annote = {Comment: 29 pages},
  file = {Amodei et al_2016_Concrete Problems in AI Safety.pdf:/Users/fergalcotter/Dropbox/Papers/Amodei et al_2016_Concrete Problems in AI Safety.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/ZXPG9DC6/1606.html:text/html},
  groups = {Fun,Fun}
}

@article{oyallon_building_2017,
  title = {Building a {{Regular Decision Boundary}} with {{Deep Networks}}},
  abstract = {In this work, we build a generic architecture of Convolutional Neural Networks to discover empirical properties of neural networks. Our first contribution is to introduce a state-of-the-art framework that depends upon few hyper parameters and to study the network when we vary them. It has no max pooling, no biases, only 13 layers, is purely convolutional and yields up to 95.4\% and 79.6\% accuracy respectively on CIFAR10 and CIFAR100. We show that the nonlinearity of a deep network does not need to be continuous, non expansive or point-wise, to achieve good performance. We show that increasing the width of our network permits being competitive with very deep networks. Our second contribution is an analysis of the contraction and separation properties of this network. Indeed, a 1-nearest neighbor classifier applied on deep features progressively improves with depth, which indicates that the representation is progressively more regular. Besides, we defined and analyzed local support vectors that separate classes locally. All our experiments are reproducible and code is available online, based on TensorFlow.},
  timestamp = {2017-04-25T22:31:09Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.01775},
  primaryClass = {cs},
  urldate = {2017-04-25},
  journal = {arXiv:1703.01775 [cs]},
  author = {Oyallon, Edouard},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  annote = {Comment: CVPR 2017, 8 pages},
  file = {Oyallon_2017_Building a Regular Decision Boundary with Deep Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Oyallon_2017_Building a Regular Decision Boundary with Deep Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/FA9GF6IA/1703.html:text/html},
  groups = {Scatternets,Scatternets}
}

@article{jacobsen_multiscale_2017,
  title = {Multiscale {{Hierarchical Convolutional Networks}}},
  abstract = {Deep neural network algorithms are difficult to analyze because they lack structure allowing to understand the properties of underlying transforms and invariants. Multiscale hierarchical convolutional networks are structured deep convolutional networks where layers are indexed by progressively higher dimensional attributes, which are learned from training data. Each new layer is computed with multidimensional convolutions along spatial and attribute variables. We introduce an efficient implementation of such networks where the dimensionality is progressively reduced by averaging intermediate layers along attribute indices. Hierarchical networks are tested on CIFAR image data bases where they obtain comparable precisions to state of the art networks, with much fewer parameters. We study some properties of the attributes learned from these databases.},
  timestamp = {2017-04-25T22:31:12Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.04140},
  primaryClass = {cs, stat},
  urldate = {2017-04-25},
  journal = {arXiv:1703.04140 [cs, stat]},
  author = {Jacobsen, J{\"o}rn-Henrik and Oyallon, Edouard and Mallat, St{\'e}phane and Smeulders, Arnold W. M.},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {Jacobsen et al_2017_Multiscale Hierarchical Convolutional Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Jacobsen et al_2017_Multiscale Hierarchical Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/B33J9FK4/1703.html:text/html},
  groups = {Scatternets,Scatternets}
}

@article{oyallon_scaling_2017,
  title = {Scaling the {{Scattering Transform}}: {{Deep Hybrid Networks}}},
  shorttitle = {Scaling the {{Scattering Transform}}},
  abstract = {We use the scattering network as a generic and fixed ini-tialization of the first layers of a supervised hybrid deep network. We show that early layers do not necessarily need to be learned, providing the best results to-date with pre-defined representations while being competitive with Deep CNNs. Using a shallow cascade of 1 x 1 convolutions, which encodes scattering coefficients that correspond to spatial windows of very small sizes, permits to obtain AlexNet accuracy on the imagenet ILSVRC2012. We demonstrate that this local encoding explicitly learns invariance w.r.t. rotations. Combining scattering networks with a modern ResNet, we achieve a single-crop top 5 error of 11.4\% on imagenet ILSVRC2012, comparable to the Resnet-18 architecture, while utilizing only 10 layers. We also find that hybrid architectures can yield excellent performance in the small sample regime, exceeding their end-to-end counterparts, through their ability to incorporate geometrical priors. We demonstrate this on subsets of the CIFAR-10 dataset and on the STL-10 dataset.},
  timestamp = {2017-08-23T22:37:14Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.08961},
  primaryClass = {cs},
  urldate = {2017-04-25},
  journal = {arXiv:1703.08961 [cs]},
  author = {Oyallon, Edouard and Belilovsky, Eugene and Zagoruyko, Sergey},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  file = {Oyallon et al_2017_Scaling the Scattering Transform.pdf:/Users/fergalcotter/Dropbox/Papers/Oyallon et al_2017_Scaling the Scattering Transform.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/AH2XRG5D/1703.html:text/html},
  groups = {mlsp cites,Scatternets,Scatternets}
}

@inproceedings{oyallon_hybrid_2017,
  title = {A {{Hybrid Network}}: {{Scattering}} and {{Convnet}}},
  timestamp = {2017-04-25T22:32:48Z},
  author = {Oyallon, Edouard},
  year = {2017},
  file = {Oyallon_2017_A Hybrid Network.pdf:/Users/fergalcotter/Dropbox/Papers/Oyallon_2017_A Hybrid Network.pdf:application/pdf},
  groups = {Scatternets,Scatternets}
}

@article{georgiou_complex_1992,
  title = {Complex Domain Backpropagation},
  volume = {39},
  issn = {1057-7130},
  doi = {10.1109/82.142037},
  abstract = {The backpropagation algorithm is extended to complex domain backpropagation (CDBP) which can be used to train neural networks for which the inputs, weights, activation functions, and outputs are complex-valued. Previous derivations of CDBP were necessarily admitting activation functions that have singularities, which is highly undesirable. In the derivation, CDBP is derived so that that it accommodates classes of suitable activation functions. One such function is found and the circuit implementation of the corresponding neuron is given. CDBP hardware circuits can be used to process sinusoidal signals all at the same frequency (phasors)},
  timestamp = {2017-04-21T15:55:01Z},
  number = {5},
  journal = {IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing},
  author = {Georgiou, G. M. and Koutsougeras, C.},
  month = may,
  year = {1992},
  keywords = {activation functions,analogue computer circuits,Backpropagation algorithms,Circuits,complex domain backpropagation,computerised signal processing,Feedforward systems,Frequency,hardware circuits,Learning systems,Least squares approximation,neural nets,Neural network hardware,Neural networks,neural network training,Neurons,Nonhomogeneous media,phasors,signal processing,sinusoidal signals processing},
  pages = {330--334},
  file = {Georgiou_Koutsougeras_1992_Complex domain backpropagation.pdf:/Users/fergalcotter/Dropbox/Papers/Georgiou_Koutsougeras_1992_Complex domain backpropagation.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/G8XTCEFD/142037.html:text/html},
  groups = {Complex CNNs,Complex CNNs}
}

@article{ioannou_deep_2016,
  title = {Deep {{Roots}}: {{Improving CNN Efficiency}} with {{Hierarchical Filter Groups}}},
  shorttitle = {Deep {{Roots}}},
  abstract = {We propose a new method for creating computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. This allows a significant reduction in computational cost and number of parameters compared to state-of-the-art deep CNNs, without compromising accuracy, by exploiting the sparsity of inter-layer filter dependencies. We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than the baseline architectures with much less computation, as measured by CPU and GPU timings. For example, for ResNet 50, our model has 40\% fewer parameters, 45\% fewer floating point operations, and is 31\% (12\%) faster on a CPU (GPU). For the deeper ResNet 200 our model has 25\% fewer floating point operations and 44\% fewer parameters, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7\% fewer parameters and is 21\% (16\%) faster on a CPU (GPU).},
  timestamp = {2017-05-08T13:46:44Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.06489},
  primaryClass = {cs},
  urldate = {2017-05-08},
  journal = {arXiv:1605.06489 [cs]},
  author = {Ioannou, Yani and Robertson, Duncan and Cipolla, Roberto and Criminisi, Antonio},
  month = may,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: Updated full version of paper, in full letter paper two-column paper. Includes many textual changes, updated CIFAR10 results, and new analysis of inter/intra-layer correlation},
  file = {Ioannou et al_2016_Deep Roots.pdf:/Users/fergalcotter/Dropbox/Papers/Ioannou et al_2016_Deep Roots.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/EDNGZH3R/1605.html:text/html},
  groups = {CNNS,CNNS}
}

@article{denil_predicting_2013,
  title = {Predicting {{Parameters}} in {{Deep Learning}}},
  abstract = {We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95\% of the weights of a network without any drop in accuracy.},
  timestamp = {2017-05-08T13:52:00Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1306.0543},
  primaryClass = {cs, stat},
  urldate = {2017-05-08},
  journal = {arXiv:1306.0543 [cs, stat]},
  author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'Aurelio and {de Freitas}, Nando},
  month = jun,
  year = {2013},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {Denil et al_2013_Predicting Parameters in Deep Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Denil et al_2013_Predicting Parameters in Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/F2RZBGXB/1306.html:text/html},
  groups = {CNNS,CNNS}
}

@article{singh_multi-resolution_2017,
  title = {Multi-{{Resolution Dual}}-{{Tree Wavelet Scattering Network}} for {{Signal Classification}}},
  abstract = {This paper introduces a Deep Scattering network that utilizes Dual-Tree complex wavelets to extract translation invariant representations from an input signal. The computationally efficient Dual-Tree wavelets decompose the input signal into densely spaced representations over scales. Translation invariance is introduced in the representations by applying a non-linearity over a region followed by averaging. The discriminatory information in the densely spaced, locally smooth, signal representations aids the learning of the classifier. The proposed network is shown to outperform Mallat's ScatterNet on four datasets with different modalities on classification accuracy.},
  timestamp = {2017-09-05T12:20:21Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.03345},
  primaryClass = {cs},
  urldate = {2017-05-22},
  journal = {arXiv:1702.03345 [cs]},
  author = {Singh, Amarjot and Kingsbury, Nick},
  month = feb,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Singh_Kingsbury_2017_Multi-Resolution Dual-Tree Wavelet Scattering Network for Signal Classification.pdf:/Users/fergalcotter/Dropbox/Papers/Singh_Kingsbury_2017_Multi-Resolution Dual-Tree Wavelet Scattering Network for Signal Classification.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/TTRPH6UU/1702.html:text/html},
  groups = {mlsp cites,Scatternets,Scatternets}
}

@article{singh_dual-tree_2017,
  title = {Dual-{{Tree Wavelet Scattering Network}} with {{Parametric Log Transformation}} for {{Object Classification}}},
  abstract = {We introduce a ScatterNet that uses a parametric log transformation with Dual-Tree complex wavelets to extract translation invariant representations from a multi-resolution image. The parametric transformation aids the OLS pruning algorithm by converting the skewed distributions into relatively mean-symmetric distributions while the Dual-Tree wavelets improve the computational efficiency of the network. The proposed network is shown to outperform Mallat's ScatterNet on two image datasets, both for classification accuracy and computational efficiency. The advantages of the proposed network over other supervised and some unsupervised methods are also presented using experiments performed on different training dataset sizes.},
  timestamp = {2017-05-22T14:26:34Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.03267},
  primaryClass = {cs},
  urldate = {2017-05-22},
  journal = {arXiv:1702.03267 [cs]},
  author = {Singh, Amarjot and Kingsbury, Nick},
  month = feb,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Singh_Kingsbury_2017_Dual-Tree Wavelet Scattering Network with Parametric Log Transformation for.pdf:/Users/fergalcotter/Dropbox/Papers/Singh_Kingsbury_2017_Dual-Tree Wavelet Scattering Network with Parametric Log Transformation for.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/57DR9Q3V/1702.html:text/html},
  groups = {mlsp cites,Scatternets,Scatternets}
}

@article{arjovsky_towards_2017,
  title = {Towards {{Principled Methods}} for {{Training Generative Adversarial Networks}}},
  abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
  timestamp = {2017-05-25T11:46:06Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.04862},
  primaryClass = {cs, stat},
  urldate = {2017-05-25},
  journal = {arXiv:1701.04862 [cs, stat]},
  author = {Arjovsky, Martin and Bottou, L{\'e}on},
  month = jan,
  year = {2017},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {Arjovsky_Bottou_2017_Towards Principled Methods for Training Generative Adversarial Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Arjovsky_Bottou_2017_Towards Principled Methods for Training Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/FJCV3I4R/1701.html:text/html},
  groups = {gan,gan,gan}
}

@article{arjovsky_wasserstein_2017,
  title = {Wasserstein {{GAN}}},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
  timestamp = {2017-05-25T11:46:08Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.07875},
  primaryClass = {cs, stat},
  urldate = {2017-05-25},
  journal = {arXiv:1701.07875 [cs, stat]},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  month = jan,
  year = {2017},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {Arjovsky et al_2017_Wasserstein GAN.pdf:/Users/fergalcotter/Dropbox/Papers/Arjovsky et al_2017_Wasserstein GAN.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/Q9JREMGA/1701.html:text/html},
  groups = {gan,gan,gan}
}

@article{lai_giraffe:_2015,
  title = {Giraffe: {{Using Deep Reinforcement Learning}} to {{Play Chess}}},
  shorttitle = {Giraffe},
  abstract = {This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer. Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters. Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess.},
  timestamp = {2017-06-06T19:58:48Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.01549},
  primaryClass = {cs},
  journal = {arXiv:1509.01549 [cs]},
  author = {Lai, Matthew},
  month = sep,
  year = {2015},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: MSc Dissertation},
  file = {Lai_2015_Giraffe.pdf:/Users/fergalcotter/Dropbox/Papers/Lai_2015_Giraffe.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/D4JH8HDP/1509.html:text/html},
  groups = {Fun,Fun}
}

@article{baker_designing_2016,
  title = {Designing {{Neural Network Architectures}} Using {{Reinforcement Learning}}},
  abstract = {At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using \$Q\$-learning with an \$$\backslash$epsilon\$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.},
  timestamp = {2017-06-26T17:44:55Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.02167},
  primaryClass = {cs},
  journal = {arXiv:1611.02167 [cs]},
  author = {Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Learning},
  file = {Baker et al_2016_Designing Neural Network Architectures using Reinforcement Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Baker et al_2016_Designing Neural Network Architectures using Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/5G2Q5XWE/1611.html:text/html},
  groups = {CNNS,CNNS}
}

@article{zoph_neural_2016,
  title = {Neural {{Architecture Search}} with {{Reinforcement Learning}}},
  timestamp = {2017-06-26T17:45:38Z},
  urldate = {2017-06-26},
  author = {Zoph, Barret and Le, Quoc},
  month = nov,
  year = {2016},
  file = {Zoph_Le_2016_Neural Architecture Search with Reinforcement Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Zoph_Le_2016_Neural Architecture Search with Reinforcement Learning.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/4T2RJTJ8/forum.html:text/html},
  groups = {CNNS,CNNS}
}

@misc{_why_????,
  title = {Why {{Use QuantLib}}?},
  abstract = {Why Use QuantLib? on ResearchGate, the professional network for scientists.},
  timestamp = {2017-06-17T15:41:27Z},
  urldate = {2017-06-17},
  howpublished = {\url{https://www.researchgate.net/publication/2941142_Why_Use_QuantLib}},
  journal = {ResearchGate},
  file = {Why Use QuantLib.pdf:/Users/fergalcotter/Dropbox/Papers/Why Use QuantLib.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/J2XM7MHD/2941142_Why_Use_QuantLib.html:text/html},
  groups = {finance,finance,finance}
}

@article{klambauer_self-normalizing_2017,
  title = {Self-{{Normalizing Neural Networks}}},
  abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
  timestamp = {2017-06-27T11:29:15Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.02515},
  primaryClass = {cs, stat},
  journal = {arXiv:1706.02515 [cs, stat]},
  author = {Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  annote = {Comment: 9 pages (+ 93 pages appendix)},
  file = {Klambauer et al_2017_Self-Normalizing Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Klambauer et al_2017_Self-Normalizing Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/HZTD2N9Q/1706.html:text/html},
  groups = {CNNS,CNNS}
}

@inproceedings{zisserman_layer_????,
  title = {Layer {{Recurrent Neural Networks}}},
  timestamp = {2017-07-10T15:42:58Z},
  author = {Zisserman, Andrew and Weidi, Xie and Noble, Alison},
  file = {Zisserman et al. - Layer Recurrent Neural Networks.pdf:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/RB7F93CV/Zisserman et al. - Layer Recurrent Neural Networks.pdf:application/pdf},
  groups = {Fun,Fun}
}

@article{ren_faster_2015,
  title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R}}-{{CNN}}},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  timestamp = {2017-07-17T13:32:59Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.01497},
  primaryClass = {cs},
  journal = {arXiv:1506.01497 [cs]},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  month = jun,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote = {Comment: Extended tech report},
  file = {Ren et al_2015_Faster R-CNN.pdf:/Users/fergalcotter/Dropbox/Papers/Ren et al_2015_Faster R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/A5EUC87R/1506.html:text/html},
  groups = {object detection,object detection}
}

@article{girshick_fast_2015,
  title = {Fast {{R}}-{{CNN}}},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  timestamp = {2017-07-17T13:32:53Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1504.08083},
  primaryClass = {cs},
  journal = {arXiv:1504.08083 [cs]},
  author = {Girshick, Ross},
  month = apr,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote = {Comment: To appear in ICCV 2015},
  file = {Girshick_2015_Fast R-CNN.pdf:/Users/fergalcotter/Dropbox/Papers/Girshick_2015_Fast R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/DFZEJR47/1504.html:text/html},
  groups = {object detection,object detection}
}

@article{daubechies_factoring_1998,
  title = {Factoring Wavelet Transforms into Lifting Steps},
  volume = {4},
  issn = {1069-5869, 1531-5851},
  doi = {10.1007/BF02476026},
  abstract = {This article is essentially tutorial in nature. We show how any discrete wavelet transform or two band subband filtering with finite filters can be decomposed into a finite sequence of simple filtering steps, which we call lifting steps but that are also known as ladder structures. This decomposition corresponds to a factorization of the polyphase matrix of the wavelet or subband filters into elementary matrices. That such a factorization is possible is well-known to algebraists (and expressed by the formulaSL(n;R[z, z-1])=E(n;R[z, z-1])); it is also used in linear systems theory in the electrical engineering community. We present here a self-contained derivation, building the decomposition from basic principles such as the Euclidean algorithm, with a focus on applying it to wavelet filtering. This factorization provides an alternative for the lattice factorization, with the advantage that it can also be used in the biorthogonal, i.e., non-unitary case. Like the lattice factorization, the decomposition presented here asymptotically reduces the computational complexity of the transform by a factor two. It has other applications, such as the possibility of defining a wavelet-like transform that maps integers to integers.},
  language = {en},
  timestamp = {2017-07-17T11:22:29Z},
  number = {3},
  urldate = {2017-07-17},
  journal = {Journal of Fourier Analysis and Applications},
  author = {Daubechies, Ingrid and Sweldens, Wim},
  month = may,
  year = {1998},
  pages = {247--269},
  file = {Daubechies_Sweldens_1998_Factoring wavelet transforms into lifting steps.pdf:/Users/fergalcotter/Dropbox/Papers/Daubechies_Sweldens_1998_Factoring wavelet transforms into lifting steps.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/6GWNKZMQ/BF02476026.html:text/html},
  groups = {Wavelets,Wavelets}
}

@article{hochreiter_long_1997,
  title = {Long {{Short}}-{{Term Memory}}},
  volume = {9},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  timestamp = {2017-07-20T20:04:48Z},
  number = {8},
  journal = {Neural Comput.},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  month = nov,
  year = {1997},
  pages = {1735--1780},
  file = {Hochreiter_Schmidhuber_1997_Long Short-Term Memory.pdf:/Users/fergalcotter/Dropbox/Papers/Hochreiter_Schmidhuber_1997_Long Short-Term Memory.pdf:application/pdf},
  groups = {memcnn,memcnn,memcnn}
}

@inproceedings{wang_beyond_2017,
  title = {Beyond {{Filters}}: {{Compact Feature Map}} for {{Portable Deep Model}}},
  shorttitle = {Beyond {{Filters}}},
  abstract = {Convolutional neural networks (CNNs) have shown extraordinary performance in a number of applications, but they are usually of heavy design for the accuracy reason. Beyond compressing the filters i...},
  language = {en},
  timestamp = {2017-07-29T19:40:24Z},
  urldate = {2017-07-29},
  booktitle = {{{PMLR}}},
  author = {Wang, Yunhe and Xu, Chang and Xu, Chao and Tao, Dacheng},
  month = jul,
  year = {2017},
  pages = {3703--3711},
  file = {Wang et al_2017_Beyond Filters.pdf:/Users/fergalcotter/Dropbox/Papers/Wang et al_2017_Beyond Filters.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/CP3W8MUA/wang17m.html:text/html},
  groups = {CNNS,CNNS}
}

@article{andra_vlsi_2002,
  title = {A {{VLSI}} Architecture for Lifting-Based Forward and Inverse Wavelet Transform},
  volume = {50},
  issn = {1053-587X},
  doi = {10.1109/78.992147},
  abstract = {We propose an architecture that performs the forward and inverse discrete wavelet transform (DWT) using a lifting-based scheme for the set of seven filters proposed in JPEG2000. The architecture consists of two row processors, two column processors, and two memory modules. Each processor contains two adders, one multiplier, and one shifter. The precision of the multipliers and adders has been determined using extensive simulation. Each memory module consists of four banks in order to support the high computational bandwidth. The architecture has been designed to generate an output every cycle for the JPEG2000 default filters. The schedules have been generated by hand and the corresponding timings listed. Finally, the architecture has been implemented in behavioral VHDL. The estimated area of the proposed architecture in 0.18-$\mu$ technology is 2.8 nun square, and the estimated frequency of operation is 200 MHz},
  timestamp = {2017-08-03T16:49:27Z},
  number = {4},
  journal = {IEEE Transactions on Signal Processing},
  author = {Andra, K. and Chakrabarti, C. and Acharya, T.},
  month = apr,
  year = {2002},
  keywords = {0.18 micron,200 MHz,adders,Bandwidth,behavioral VHDL,channel bank filters,circuit CAD,column processors,Computational modeling,Computer architecture,digital signal processing chips,discrete wavelet transform,Discrete wavelet transforms,filter bank,Filters,Frequency estimation,hardware description languages,Image coding,integrated circuit design,Inverse problems,JPEG2000 default filters,lifting-based forward wavelet transform,lifting-based inverse wavelet transform,Matrix converters,memory modules,multiplier precision,row processors,shifter,simulation,Transform coding,Very large scale integration,VLSI,VLSI architecture,wavelet transforms},
  pages = {966--977},
  file = {Andra et al_2002_A VLSI architecture for lifting-based forward and inverse wavelet transform.pdf:/Users/fergalcotter/Dropbox/Papers/Andra et al_2002_A VLSI architecture for lifting-based forward and inverse wavelet transform.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/4QRHCNWT/992147.html:text/html},
  groups = {Wavelets,Wavelets}
}

@article{taubman_jpeg2000:_2002,
  title = {{{JPEG2000}}: Standard for Interactive Imaging},
  volume = {90},
  issn = {0018-9219},
  shorttitle = {{{JPEG2000}}},
  doi = {10.1109/JPROC.2002.800725},
  abstract = {JPEG2000 is the latest image compression standard to emerge from the Joint Photographic Experts Group (JPEG) working under the auspices of the International Standards Organization. Although the new standard does offer superior compression performance to JPEG, JPEG2000 provides a whole new way of interacting with compressed imagery in a scalable and interoperable fashion. This paper provides a tutorial-style review of the new standard, explaining the technology on which it is based and drawing comparisons with JPEG and other compression standards. The paper also describes new work, exploiting the capabilities of JPEG2000 in client-server systems for efficient interactive browsing of images over the Internet.},
  timestamp = {2017-08-03T15:26:03Z},
  number = {8},
  journal = {Proceedings of the IEEE},
  author = {Taubman, D. S. and Marcellin, M. W.},
  month = aug,
  year = {2002},
  keywords = {Bit rate,client-server systems,data compression,IEC standards,Image coding,image compression,Image resolution,interactive imaging,International Standards Organization,interoperable compression,ISO standards,Joint Photographic Experts Group,JPEG2000,review,Scalability,scalable compression,Standards development,Standards organizations,Streaming media,Transform coding},
  pages = {1336--1357},
  file = {Taubman_Marcellin_2002_JPEG2000.pdf:/Users/fergalcotter/Dropbox/Papers/Taubman_Marcellin_2002_JPEG2000.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/VKF3JQEV/1037564.html:text/html},
  groups = {Wavelets,Wavelets}
}

@article{sweldens_lifting_1998,
  title = {The {{Lifting Scheme}}: {{A Construction}} of {{Second Generation Wavelets}}},
  volume = {29},
  issn = {0036-1410},
  shorttitle = {The {{Lifting Scheme}}},
  doi = {10.1137/S0036141095289051},
  abstract = {We present the lifting scheme, a simple construction of second generation wavelets; these are wavelets that are not necessarily translates and dilates of one fixed function. Such wavelets can be adapted to intervals, domains, surfaces, weights, and irregular samples. We show how the lifting scheme leads to a faster, in-place calculation of the wavelet transform. Several examples are included.},
  timestamp = {2017-08-03T15:13:49Z},
  number = {2},
  journal = {SIAM Journal on Mathematical Analysis},
  author = {Sweldens, W.},
  month = mar,
  year = {1998},
  pages = {511--546},
  file = {Sweldens_1998_The Lifting Scheme.pdf:/Users/fergalcotter/Dropbox/Papers/Sweldens_1998_The Lifting Scheme_2.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/3CHDSSRC/S0036141095289051.html:text/html},
  groups = {Wavelets,Wavelets}
}

@article{tay_flexible_1993,
  title = {Flexible Design of Multidimensional Perfect Reconstruction {{FIR}} 2-Band Filters Using Transformations of Variables},
  volume = {2},
  issn = {1057-7149},
  doi = {10.1109/83.242356},
  abstract = {An approach to designing multidimensional linear-phase FIR diamond subband filters having the perfect reconstruction property is presented. It is based on a transformation of variables technique and is equivalent to the generalized McClellan transformation. Methods for designing a whole class of transformation are given. The approach consists of two parts; design of the transformation and design of the 1-D filters. The use of Lagrange halfband filters to design the 1-D filters is discussed. The modification of a particular Lagrange halfband filter which gives a pair of simple 1-D filters that are almost similar to each other in their frequency characteristics but still form a perfect reconstruction pair is presented. The design technique is extended to other types of two-channel sampling lattice and subband shapes, in particular, the parallelogram and the diagonally quadrant subband cases. Several numerical design examples are presented to illustrate the flexibility of the design method},
  timestamp = {2017-08-04T12:59:02Z},
  number = {4},
  journal = {IEEE Transactions on Image Processing},
  author = {Tay, D. B. H. and Kingsbury, N. G.},
  month = oct,
  year = {1993},
  keywords = {1-D filters,Design methodology,diagonally quadrant subband,filter bank,filtering and prediction theory,Finite impulse response filter,Frequency,Image coding,image processing,Image reconstruction,Lagrange halfband filters,Lagrangian functions,linear-phase FIR diamond subband filters,McClellan transformation,multidimensional 2-band filters image processing,multidimensional digital filters,Multidimensional systems,Nonlinear filters,parallelogram,perfect reconstruction property,Shape,transformations of variables,two-channel sampling lattice},
  pages = {466--480},
  file = {Tay_Kingsbury_1993_Flexible design of multidimensional perfect reconstruction FIR 2-band filters.pdf:/Users/fergalcotter/Dropbox/Papers/Tay_Kingsbury_1993_Flexible design of multidimensional perfect reconstruction FIR 2-band filters.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/JNXMN3PF/242356.html:text/html},
  groups = {Wavelets,Wavelets}
}

@article{cisse_parseval_2017,
  title = {Parseval {{Networks}}: {{Improving Robustness}} to {{Adversarial Examples}}},
  shorttitle = {Parseval {{Networks}}},
  abstract = {We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than 1. Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation. The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (approximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices. We describe how these constraints can be maintained efficiently during SGD. We show that Parseval networks match the state-of-the-art in terms of accuracy on CIFAR-10/100 and Street View House Numbers (SVHN) while being more robust than their vanilla counterpart against adversarial examples. Incidentally, Parseval networks also tend to train faster and make a better usage of the full capacity of the networks.},
  timestamp = {2017-08-11T14:52:53Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.08847},
  primaryClass = {cs, stat},
  journal = {arXiv:1704.08847 [cs, stat]},
  author = {Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard and Dauphin, Yann and Usunier, Nicolas},
  month = apr,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Learning,Statistics - Machine Learning},
  annote = {Comment: submitted},
  file = {Cisse et al_2017_Parseval Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Cisse et al_2017_Parseval Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/E9ZVEJIS/1704.html:text/html},
  groups = {CNNS,CNNS}
}

@article{gatys_preserving_2016,
  title = {Preserving {{Color}} in {{Neural Artistic Style Transfer}}},
  abstract = {This note presents an extension to the neural artistic style transfer algorithm (Gatys et al.). The original algorithm transforms an image to have the style of another given image. For example, a photograph can be transformed to have the style of a famous painting. Here we address a potential shortcoming of the original method: the algorithm transfers the colors of the original painting, which can alter the appearance of the scene in undesirable ways. We describe simple linear methods for transferring style while preserving colors.},
  timestamp = {2017-08-15T19:11:53Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.05897},
  primaryClass = {cs},
  journal = {arXiv:1606.05897 [cs]},
  author = {Gatys, Leon A. and Bethge, Matthias and Hertzmann, Aaron and Shechtman, Eli},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Gatys et al_2016_Preserving Color in Neural Artistic Style Transfer.pdf:/Users/fergalcotter/Dropbox/Papers/Gatys et al_2016_Preserving Color in Neural Artistic Style Transfer.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/JM3SW5ES/1606.html:text/html},
  groups = {style,style,style}
}

@article{ruder_artistic_2016,
  title = {Artistic Style Transfer for Videos},
  volume = {9796},
  doi = {10.1007/978-3-319-45886-1_3},
  abstract = {In the past, manually re-drawing an image in a certain artistic style required a professional artist and a long time. Doing this for a video sequence single-handed was beyond imagination. Nowadays computers provide new possibilities. We present an approach that transfers the style from one image (for example, a painting) to a whole video sequence. We make use of recent advances in style transfer in still images and propose new initializations and loss functions applicable to videos. This allows us to generate consistent and stable stylized video sequences, even in cases with large motion and strong occlusion. We show that the proposed method clearly outperforms simpler baselines both qualitatively and quantitatively.},
  timestamp = {2017-08-15T19:12:20Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1604.08610},
  primaryClass = {cs},
  journal = {arXiv:1604.08610 [cs]},
  author = {Ruder, Manuel and Dosovitskiy, Alexey and Brox, Thomas},
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  pages = {26--36},
  annote = {Comment: final version appeared in GCPR-2016; minor changes to improve the clarity},
  file = {Ruder et al_2016_Artistic style transfer for videos.pdf:/Users/fergalcotter/Dropbox/Papers/Ruder et al_2016_Artistic style transfer for videos.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/K5A9MV83/1604.html:text/html},
  groups = {style,style,style}
}

@article{olhede_monogenic_2009,
  title = {The {{Monogenic Wavelet Transform}}},
  volume = {57},
  issn = {1053-587X},
  doi = {10.1109/TSP.2009.2023397},
  abstract = {This paper extends the 1-D analytic wavelet transform to the 2-D monogenic wavelet transform. The transformation requires care in its specification to ensure suitable transform coefficients are calculated, and it is constructed so that the wavelet transform may be considered as both local and monogenic. This is consistent with defining the transform as a real wavelet transform of a monogenic signal in analogy with the analytic wavelet transform. Classes of monogenic wavelets are proposed with suitable local properties. It is shown that the monogenic wavelet annihilates anti-monogenic signals, that the monogenic wavelet transform is phase-shift covariant and that the transform magnitude is phase-shift invariant. A simple form for the magnitude and orientation of the isotropic transform coefficients of a unidirectional signal when observed in a rotated frame of reference is derived. The monogenic wavelet ridges of local plane waves are given.},
  timestamp = {2017-08-18T11:37:26Z},
  number = {9},
  journal = {IEEE Transactions on Signal Processing},
  author = {Olhede, S. C. and Metikas, G.},
  month = sep,
  year = {2009},
  keywords = {1D analytic wavelet transform,2D monogenic wavelet transform,Analytic signal,analytic wavelet transform,Hilbert transform,Hilbert transforms,isotropic transform coefficient,local plane wave,monogenic signal,phase-shift covariant,phase-shift invariant,Riesz transform,signal processing,unidirectional signal,wavelet transforms},
  pages = {3426--3441},
  file = {Olhede_Metikas_2009_The Monogenic Wavelet Transform.pdf:/Users/fergalcotter/Dropbox/Papers/Olhede_Metikas_2009_The Monogenic Wavelet Transform.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/FZQPXX2B/4956988.html:text/html},
  groups = {Monogenic,Monogenic,Monogenic}
}

@misc{_monogenic_????,
  title = {The {{Monogenic Riesz}}-{{Laplace Wavelet Transform}} ({{PDF Download Available}})},
  abstract = {Official Full-Text Paper (PDF): The Monogenic Riesz-Laplace Wavelet Transform},
  timestamp = {2017-08-18T11:36:25Z},
  urldate = {2017-08-18},
  howpublished = {\url{https://www.researchgate.net/publication/229149145_The_Monogenic_Riesz-Laplace_Wavelet_Transform}},
  journal = {ResearchGate},
  file = {The Monogenic Riesz-Laplace Wavelet Transform (PDF Download Available).pdf:/Users/fergalcotter/Dropbox/Papers/The Monogenic Riesz-Laplace Wavelet Transform (PDF Download Available).pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/TGI8T66R/229149145_The_Monogenic_Riesz-Laplace_Wavelet_Transform.html:text/html},
  groups = {Monogenic,Monogenic,Monogenic}
}

@misc{_sleep_????,
  title = {Sleep {{Disorders}} and {{Gastroesophageal Reflux Disease}} ({{GERD}}) - {{Full Text View}} - {{ClinicalTrials}}.Gov},
  timestamp = {2017-08-29T10:50:24Z},
  urldate = {2017-08-29},
  howpublished = {\url{https://clinicaltrials.gov/ct/show/NCT00287391?order=1}}
}

@article{russ_druggable_2005-1,
  title = {The Druggable Genome: An Update},
  volume = {10},
  issn = {1359-6446},
  shorttitle = {The Druggable Genome},
  doi = {10.1016/S1359-6446(05)03666-4},
  timestamp = {2017-08-29T11:03:03Z},
  number = {23},
  journal = {Drug Discovery Today},
  author = {Russ, Andreas P. and Lampel, Stefan},
  month = dec,
  year = {2005},
  keywords = {druggability,Genome,protein prediction,sequence homology,sequencing},
  pages = {1607--1610},
  file = {Russ_Lampel_2005_The druggable genome.pdf:/Users/fergalcotter/Dropbox/Papers/Russ_Lampel_2005_The druggable genome.pdf:application/pdf;ScienceDirect Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/C6S7AF73/S1359644605036664.html:text/html},
  groups = {CCG}
}

@article{rask-andersen_trends_2011,
  title = {Trends in the Exploitation of Novel Drug Targets},
  volume = {10},
  copyright = {\textcopyright{} 2011 Nature Publishing Group},
  issn = {1474-1776},
  doi = {10.1038/nrd3478},
  abstract = {The discovery and exploitation of new drug targets is a key focus for both the pharmaceutical industry and academic biomedical research. To provide an insight into trends in the exploitation of new drug targets, we have analysed the drugs that were approved by the US Food and Drug Administration during the past three decades and examined the interactions of these drugs with therapeutic targets that are encoded by the human genome, using the DrugBank database and extensive manual curation. We have identified 435 effect-mediating drug targets in the human genome, which are modulated by 989 unique drugs, through 2,242 drug\textendash{}target interactions. We also analyse trends in the introduction of drugs that modulate previously unexploited targets, and discuss the network pharmacology of the drugs in our data set.},
  language = {en},
  timestamp = {2017-08-29T11:03:12Z},
  number = {8},
  urldate = {2017-08-29},
  journal = {Nature Reviews Drug Discovery},
  author = {Rask-Andersen, Mathias and Alm{\'e}n, Markus S{\"a}llman and Schi{\"o}th, Helgi B.},
  month = aug,
  year = {2011},
  pages = {579--590},
  file = {Rask-Andersen et al_2011_Trends in the exploitation of novel drug targets.pdf:/Users/fergalcotter/Dropbox/Papers/Rask-Andersen et al_2011_Trends in the exploitation of novel drug targets.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/7X5NRKHP/nrd3478.html:text/html},
  groups = {CCG}
}

@incollection{national_library_of_medicine_clinicaltrials.gov_2000,
  title = {{{ClinicalTrials}}.Gov},
  abstract = {Sleep disorders and
gastroesophageal reflux disease (GERD);},
  timestamp = {2017-08-29T10:57:14Z},
  author = {{National Library of Medicine}},
  month = feb,
  year = {2000},
  groups = {CCG}
}

@article{finan_druggable_2017-1,
  title = {The Druggable Genome and Support for Target Identification and Validation in Drug Development},
  volume = {9},
  copyright = {Copyright \textcopyright{} 2017, American Association for the Advancement of Science},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.aag1166},
  abstract = {An organized way to drug the genome
Many drugs that are already approved for specific diseases have known protein targets, which may be relevant for other disease types as well. In addition, a systematic way of identifying druggable genes in various diseases should help streamline the process of developing new drugs for these targets, even if no specific drugs are available for them yet. Finan et al. designed a computational approach to do this, combining data from numerous existing genome-wide association studies to identify druggable proteins, connect them with known drugs where available, and facilitate the design of new targeted therapeutics.
Target identification (determining the correct drug targets for a disease) and target validation (demonstrating an effect of target perturbation on disease biomarkers and disease end points) are important steps in drug development. Clinically relevant associations of variants in genes encoding drug targets model the effect of modifying the same targets pharmacologically. To delineate drug development (including repurposing) opportunities arising from this paradigm, we connected complex disease- and biomarker-associated loci from genome-wide association studies to an updated set of genes encoding druggable human proteins, to agents with bioactivity against these targets, and, where there were licensed drugs, to clinical indications. We used this set of genes to inform the design of a new genotyping array, which will enable association studies of druggable genes for drug target selection and validation in human disease.
The druggable genome and genome-wide association study data reveal new drug development and repurposing opportunities.
The druggable genome and genome-wide association study data reveal new drug development and repurposing opportunities.},
  language = {en},
  timestamp = {2017-08-29T11:11:20Z},
  number = {383},
  urldate = {2017-08-29},
  journal = {Science Translational Medicine},
  author = {Finan, Chris and Gaulton, Anna and Kruger, Felix A. and Lumbers, R. Thomas and Shah, Tina and Engmann, Jorgen and Galver, Luana and Kelley, Ryan and Karlsson, Anneli and Santos, Rita and Overington, John P. and Hingorani, Aroon D. and Casas, Juan P.},
  month = mar,
  year = {2017},
  pages = {eaag1166},
  file = {Finan et al_2017_The druggable genome and support for target identification and validation in.pdf:/Users/fergalcotter/Dropbox/Papers/Finan et al_2017_The druggable genome and support for target identification and validation in.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/PI7CGGS4/eaag1166.html:text/html},
  groups = {CCG},
  pmid = {28356508}
}

@article{wagner_dgidb_2016,
  title = {{{DGIdb}} 2.0: Mining Clinically Relevant Drug\textendash{}gene Interactions},
  volume = {44},
  issn = {0305-1048},
  shorttitle = {{{DGIdb}} 2.0},
  doi = {10.1093/nar/gkv1165},
  abstract = {The Drug\textendash{}Gene Interaction Database (DGIdb, www.dgidb.org) is a web resource that consolidates disparate data sources describing drug\textendash{}gene interactions and gene druggability. It provides an intuitive graphical user interface and a documented application programming interface (API) for querying these data. DGIdb was assembled through an extensive manual curation effort, reflecting the combined information of twenty-seven sources. For DGIdb 2.0, substantial updates have been made to increase content and improve its usefulness as a resource for mining clinically actionable drug targets. Specifically, nine new sources of drug\textendash{}gene interactions have been added, including seven resources specifically focused on interactions linked to clinical trials. These additions have more than doubled the overall count of drug\textendash{}gene interactions. The total number of druggable gene claims has also increased by 30\%. Importantly, a majority of the unrestricted, publicly-accessible sources used in DGIdb are now automatically updated on a weekly basis, providing the most current information for these sources. Finally, a new web view and API have been developed to allow searching for interactions by drug identifiers to complement existing gene-based search functionality. With these updates, DGIdb represents a comprehensive and user friendly tool for mining the druggable genome for precision medicine hypothesis generation.},
  timestamp = {2017-08-29T10:49:10Z},
  number = {D1},
  urldate = {2017-08-29},
  journal = {Nucleic Acids Research},
  author = {Wagner, Alex H. and Coffman, Adam C. and Ainscough, Benjamin J. and Spies, Nicholas C. and Skidmore, Zachary L. and Campbell, Katie M. and Krysiak, Kilannin and Pan, Deng and McMichael, Joshua F. and Eldred, James M. and Walker, Jason R. and Wilson, Richard K. and Mardis, Elaine R. and Griffith, Malachi and Griffith, Obi L.},
  month = jan,
  year = {2016},
  pages = {D1036--D1044},
  file = {Wagner et al_2016_DGIdb 2.pdf:/Users/fergalcotter/Dropbox/Papers/Wagner et al_2016_DGIdb 2.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/73IGPDJV/DGIdb-2-0-mining-clinically-relevant-drug-gene.html:text/html},
  groups = {CCG}
}

@article{hopkins_druggable_2002,
  title = {The Druggable Genome},
  volume = {1},
  copyright = {\textcopyright{} 2002 Nature Publishing Group},
  issn = {1474-1776},
  doi = {10.1038/nrd892},
  abstract = {An assessment of the number of molecular targets that represent an opportunity for therapeutic intervention is crucial to the development of post-genomic research strategies within the pharmaceutical industry. Now that we know the size of the human genome, it is interesting to consider just how many molecular targets this opportunity represents. We start from the position that we understand the properties that are required for a good drug, and therefore must be able to understand what makes a good drug target.},
  language = {en},
  timestamp = {2017-08-29T11:02:17Z},
  number = {9},
  urldate = {2017-08-29},
  journal = {Nature Reviews Drug Discovery},
  author = {Hopkins, Andrew L. and Groom, Colin R.},
  month = sep,
  year = {2002},
  pages = {727--730},
  file = {Hopkins_Groom_2002_The druggable genome.pdf:/Users/fergalcotter/Dropbox/Papers/Hopkins_Groom_2002_The druggable genome.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/EBKSXISN/nrd892.html:text/html},
  groups = {CCG}
}

@article{simon_tabulated_2014,
  title = {A {{Tabulated Summary}} of {{Targeted}} and {{Biologic Therapies}} for {{Non}}\textendash{}{{Small}}-{{Cell Lung Cancer}}},
  volume = {15},
  issn = {1525-7304},
  doi = {10.1016/j.cllc.2013.11.009},
  abstract = {The current pace of development of targeted agents in lung cancer is unprecedented. This rapid pace of development is facilitated by the identification of novel targets, development of new ways of inhibiting older and more familiar targets and some very innovative therapeutic engineering that allows us to inhibit multiple targets simultaneously. In this tabulated summary of over 320 targeted therapies currently in practice and in clinical trials for patients with lung cancer are listed. Given that the information included is constantly changing, the readers are encouraged to ascertain the current status of the ongoing clinical trials by checking the clinicaltrials.gov website. To~facilitate this, a hyperlink for each agent is inserted in the left hand column of this reference tool. Compounds in pre-clinical development that have not yet entered clinical trials are not listed. Target therapies that are in clinical development but not enrolling lung cancer patients are also not included. Save for these exceptions the list is intended to be comprehensive. In conclusion, there are a plethora of novel agents currently in development for lung cancer. The emergence of these agents offers hope to a group of patients for whom progress has been slow until now. However dramatic improvements in survival have already been made in specific subsets of patients and this pace of advancements is only expected to accelerate dramatically for the foreseeable future.},
  timestamp = {2017-08-29T11:03:23Z},
  number = {1},
  journal = {Clinical Lung Cancer},
  author = {Simon, George R. and Somaiah, Neeta},
  month = jan,
  year = {2014},
  pages = {21--51},
  file = {Simon_Somaiah_2014_A Tabulated Summary of Targeted and Biologic Therapies for Non–Small-Cell Lung.pdf:/Users/fergalcotter/Dropbox/Papers/Simon_Somaiah_2014_A Tabulated Summary of Targeted and Biologic Therapies for Non–Small-Cell Lung.pdf:application/pdf;ScienceDirect Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/M6WBHIZW/S1525730413002350.html:text/html},
  groups = {CCG}
}

@article{law_drugbank_2014,
  title = {{{DrugBank}} 4.0: Shedding New Light on Drug Metabolism},
  volume = {42},
  issn = {0305-1048},
  shorttitle = {{{DrugBank}} 4.0},
  doi = {10.1093/nar/gkt1068},
  abstract = {DrugBank (http://www.drugbank.ca) is a comprehensive online database containing extensive biochemical and pharmacological information about drugs, their mechanisms and their targets. Since it was first described in 2006, DrugBank has rapidly evolved, both in response to user requests and in response to changing trends in drug research and development. Previous versions of DrugBank have been widely used to facilitate drug and in silico drug target discovery. The latest update, DrugBank 4.0, has been further expanded to contain data on drug metabolism, absorption, distribution, metabolism, excretion and toxicity (ADMET) and other kinds of quantitative structure activity relationships (QSAR) information. These enhancements are intended to facilitate research in xenobiotic metabolism (both prediction and characterization), pharmacokinetics, pharmacodynamics and drug design/discovery. For this release, \&gt;1200 drug metabolites (including their structures, names, activity, abundance and other detailed data) have been added along with \&gt;1300 drug metabolism reactions (including metabolizing enzymes and reaction types) and dozens of drug metabolism pathways. Another 30 predicted or measured ADMET parameters have been added to each DrugCard, bringing the average number of quantitative ADMET values for Food and Drug Administration-approved drugs close to 40. Referential nuclear magnetic resonance and MS spectra have been added for almost 400 drugs as well as spectral and mass matching tools to facilitate compound identification. This expanded collection of drug information is complemented by a number of new or improved search tools, including one that provides a simple analyses of drug\textendash{}target, \textendash{}enzyme and \textendash{}transporter associations to provide insight on drug\textendash{}drug interactions.},
  timestamp = {2017-08-29T11:11:18Z},
  number = {D1},
  urldate = {2017-08-29},
  journal = {Nucleic Acids Research},
  author = {Law, Vivian and Knox, Craig and Djoumbou, Yannick and Jewison, Tim and Guo, An Chi and Liu, Yifeng and Maciejewski, Adam and Arndt, David and Wilson, Michael and Neveu, Vanessa and Tang, Alexandra and Gabriel, Geraldine and Ly, Carol and Adamjee, Sakina and Dame, Zerihun T. and Han, Beomsoo and Zhou, You and Wishart, David S.},
  month = jan,
  year = {2014},
  pages = {D1091--D1097},
  file = {Law et al_2014_DrugBank 4.pdf:/Users/fergalcotter/Dropbox/Papers/Law et al_2014_DrugBank 4.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/S5ZEUAZA/gkt1068.html:text/html},
  groups = {CCG}
}

@article{jing_neural_2017,
  title = {Neural {{Style Transfer}}: {{A Review}}},
  shorttitle = {Neural {{Style Transfer}}},
  abstract = {The recent work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNN) in creating artistic fantastic imagery by separating and recombing the image content and style. This process of using CNN to migrate the semantic content of one image to different styles is referred to as Neural Style Transfer. Since then, Neural Style Transfer has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention from computer vision researchers and several methods are proposed to either improve or extend the original neural algorithm proposed by Gatys et al. However, there is no comprehensive survey presenting and summarizing recent Neural Style Transfer literature. This review aims to provide an overview of the current progress towards Neural Style Transfer, as well as discussing its various applications and open problems for future research.},
  timestamp = {2017-08-31T11:04:02Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.04058},
  primaryClass = {cs},
  journal = {arXiv:1705.04058 [cs]},
  author = {Jing, Yongcheng and Yang, Yezhou and Feng, Zunlei and Ye, Jingwen and Song, Mingli},
  month = may,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Jing et al_2017_Neural Style Transfer.pdf:/Users/fergalcotter/Dropbox/Papers/Jing et al_2017_Neural Style Transfer.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/4BJI25FG/1705.html:text/html},
  groups = {style,style,style}
}

@article{nikulin_exploring_2016-1,
  title = {Exploring the {{Neural Algorithm}} of {{Artistic Style}}},
  abstract = {We explore the method of style transfer presented in the article "A Neural Algorithm of Artistic Style" by Leon A. Gatys, Alexander S. Ecker and Matthias Bethge (arXiv:1508.06576). We first demonstrate the power of the suggested style space on a few examples. We then vary different hyper-parameters and program properties that were not discussed in the original paper, among which are the recognition network used, starting point of the gradient descent and different ways to partition style and content layers. We also give a brief comparison of some of the existing algorithm implementations and deep learning frameworks used. To study the style space further we attempt to generate synthetic images by maximizing a single entry in one of the Gram matrices \$$\backslash$mathcal\{G\}\_l\$ and some interesting results are observed. Next, we try to mimic the sparsity and intensity distribution of Gram matrices obtained from a real painting and generate more complex textures. Finally, we propose two new style representations built on top of network's features and discuss how one could be used to achieve local and potentially content-aware style transfer.},
  timestamp = {2017-08-31T11:48:38Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.07188},
  primaryClass = {cs},
  journal = {arXiv:1602.07188 [cs]},
  author = {Nikulin, Yaroslav and Novak, Roman},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote = {Comment: A short class project report (14 pages, 14 figures)},
  file = {Nikulin_Novak_2016_Exploring the Neural Algorithm of Artistic Style.pdf:/Users/fergalcotter/Dropbox/Papers/Nikulin_Novak_2016_Exploring the Neural Algorithm of Artistic Style_2.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/EPPJ7RRT/1602.html:text/html},
  groups = {style,style,style}
}

@article{li_combining_2016,
  title = {Combining {{Markov Random Fields}} and {{Convolutional Neural Networks}} for {{Image Synthesis}}},
  abstract = {This paper studies a combination of generative Markov random field (MRF) models and discriminatively trained deep convolutional neural networks (dCNNs) for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN feature pyramid, controling the image layout at an abstract level. We apply the method to both photographic and non-photo-realistic (artwork) synthesis tasks. The MRF regularizer prevents over-excitation artifacts and reduces implausible feature mixtures common to previous dCNN inversion approaches, permitting synthezing photographic content with increased visual plausibility. Unlike standard MRF-based texture synthesis, the combined system can both match and adapt local features with considerable variability, yielding results far out of reach of classic generative MRF methods.},
  timestamp = {2017-08-31T11:18:02Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1601.04589},
  primaryClass = {cs},
  journal = {arXiv:1601.04589 [cs]},
  author = {Li, Chuan and Wand, Michael},
  month = jan,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote = {Comment: 9 pages, 9 figures},
  file = {Li_Wand_2016_Combining Markov Random Fields and Convolutional Neural Networks for Image.pdf:/Users/fergalcotter/Dropbox/Papers/Li_Wand_2016_Combining Markov Random Fields and Convolutional Neural Networks for Image.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/PQXFT3KX/1601.html:text/html},
  groups = {style,style,style}
}

@article{champandard_semantic_2016,
  title = {Semantic {{Style Transfer}} and {{Turning Two}}-{{Bit Doodles}} into {{Fine Artworks}}},
  abstract = {Convolutional neural networks (CNNs) have proven highly effective at image synthesis and style transfer. For most users, however, using them as tools can be a challenging task due to their unpredictable behavior that goes against common intuitions. This paper introduces a novel concept to augment such generative architectures with semantic annotations, either by manually authoring pixel labels or using existing solutions for semantic segmentation. The result is a content-aware generative algorithm that offers meaningful control over the outcome. Thus, we increase the quality of images generated by avoiding common glitches, make the results look significantly more plausible, and extend the functional range of these algorithms---whether for portraits or landscapes, etc. Applications include semantic style transfer and turning doodles with few colors into masterful paintings!},
  timestamp = {2017-08-31T14:44:26Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.01768},
  primaryClass = {cs},
  journal = {arXiv:1603.01768 [cs]},
  author = {Champandard, Alex J.},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Champandard_2016_Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks.pdf:/Users/fergalcotter/Dropbox/Papers/Champandard_2016_Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/4M788N8Q/1603.html:text/html},
  groups = {style,style,style}
}

@article{li_demystifying_2017,
  title = {Demystifying {{Neural Style Transfer}}},
  abstract = {Neural Style Transfer has recently demonstrated very exciting results which catches eyes in both academia and industry. Despite the amazing results, the principle of neural style transfer, especially why the Gram matrices could represent style remains unclear. In this paper, we propose a novel interpretation of neural style transfer by treating it as a domain adaptation problem. Specifically, we theoretically show that matching the Gram matrices of feature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with the second order polynomial kernel. Thus, we argue that the essence of neural style transfer is to match the feature distributions between the style images and the generated images. To further support our standpoint, we experiment with several other distribution alignment methods, and achieve appealing results. We believe this novel interpretation connects these two important research fields, and could enlighten future researches.},
  timestamp = {2017-08-31T11:14:21Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.01036},
  primaryClass = {cs},
  journal = {arXiv:1701.01036 [cs]},
  author = {Li, Yanghao and Wang, Naiyan and Liu, Jiaying and Hou, Xiaodi},
  month = jan,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: Accepted by IJCAI 2017},
  file = {Li et al_2017_Demystifying Neural Style Transfer.pdf:/Users/fergalcotter/Dropbox/Papers/Li et al_2017_Demystifying Neural Style Transfer.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/GVMZUKUM/1701.html:text/html},
  groups = {style,style,style}
}

@article{risser_stable_2017,
  title = {Stable and {{Controllable Neural Texture Synthesis}} and {{Style Transfer Using Histogram Losses}}},
  abstract = {Recently, methods have been proposed that perform texture synthesis and style transfer by using convolutional neural networks (e.g. Gatys et al. [2015,2016]). These methods are exciting because they can in some cases create results with state-of-the-art quality. However, in this paper, we show these methods also have limitations in texture quality, stability, requisite parameter tuning, and lack of user controls. This paper presents a multiscale synthesis pipeline based on convolutional neural networks that ameliorates these issues. We first give a mathematical explanation of the source of instabilities in many previous approaches. We then improve these instabilities by using histogram losses to synthesize textures that better statistically match the exemplar. We also show how to integrate localized style losses in our multiscale framework. These losses can improve the quality of large features, improve the separation of content and style, and offer artistic controls such as paint by numbers. We demonstrate that our approach offers improved quality, convergence in fewer iterations, and more stability over the optimization.},
  timestamp = {2017-08-31T11:12:43Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.08893},
  primaryClass = {cs},
  journal = {arXiv:1701.08893 [cs]},
  author = {Risser, Eric and Wilmot, Pierre and Barnes, Connelly},
  month = jan,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Neural and Evolutionary Computing},
  file = {Risser et al_2017_Stable and Controllable Neural Texture Synthesis and Style Transfer Using.pdf:/Users/fergalcotter/Dropbox/Papers/Risser et al_2017_Stable and Controllable Neural Texture Synthesis and Style Transfer Using.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/F8387ZAZ/1701.html:text/html},
  groups = {style,style,style}
}

@article{gatys_controlling_2016,
  title = {Controlling {{Perceptual Factors}} in {{Neural Style Transfer}}},
  abstract = {Neural Style Transfer has shown very exciting results enabling new forms of image manipulation. Here we extend the existing method to introduce control over spatial location, colour information and across spatial scale. We demonstrate how this enhances the method by allowing high-resolution controlled stylisation and helps to alleviate common failure cases such as applying ground textures to sky regions. Furthermore, by decomposing style into these perceptual factors we enable the combination of style information from multiple sources to generate new, perceptually appealing styles from existing ones. We also describe how these methods can be used to more efficiently produce large size, high-quality stylisation. Finally we show how the introduced control measures can be applied in recent methods for Fast Neural Style Transfer.},
  timestamp = {2017-08-31T12:56:53Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.07865},
  primaryClass = {cs},
  journal = {arXiv:1611.07865 [cs]},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias and Hertzmann, Aaron and Shechtman, Eli},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote = {Comment: Accepted at CVPR2017},
  file = {Gatys et al_2016_Controlling Perceptual Factors in Neural Style Transfer.pdf:/Users/fergalcotter/Dropbox/Papers/Gatys et al_2016_Controlling Perceptual Factors in Neural Style Transfer.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/XHESX4Z9/1611.html:text/html},
  groups = {style,style,style}
}

@article{novak_improving_2016,
  title = {Improving the {{Neural Algorithm}} of {{Artistic Style}}},
  abstract = {In this work we investigate different avenues of improving the Neural Algorithm of Artistic Style (by Leon A. Gatys, Alexander S. Ecker and Matthias Bethge, arXiv:1508.06576). While showing great results when transferring homogeneous and repetitive patterns, the original style representation often fails to capture more complex properties, like having separate styles of foreground and background. This leads to visual artifacts and undesirable textures appearing in unexpected regions when performing style transfer. We tackle this issue with a variety of approaches, mostly by modifying the style representation in order for it to capture more information and impose a tighter constraint on the style transfer result. In our experiments, we subjectively evaluate our best method as producing from barely noticeable to significant improvements in the quality of style transfer.},
  timestamp = {2017-08-31T11:48:49Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.04603},
  primaryClass = {cs},
  journal = {arXiv:1605.04603 [cs]},
  author = {Novak, Roman and Nikulin, Yaroslav},
  month = may,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote = {Comment: A short class project report (15 pages)},
  file = {Novak_Nikulin_2016_Improving the Neural Algorithm of Artistic Style.pdf:/Users/fergalcotter/Dropbox/Papers/Novak_Nikulin_2016_Improving the Neural Algorithm of Artistic Style.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/FM46E666/1605.html:text/html},
  groups = {style,style,style}
}

@book{Goodfellow-et-al-2016,
  title = {Deep {{Learning}}},
  timestamp = {2017-09-04T20:25:59Z},
  publisher = {{MIT Press}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  note = {http://www.deeplearningbook.org}
}

@misc{_introduction_????-1,
  title = {An {{Introduction}} to {{Computational Learning Theory}}},
  abstract = {Emphasizing issues of computational efficiency, Michael Kearns and Umesh Vazirani introduce a number of central topics in computational learning theory for researchers and students in artificial intelligence, neural networks, theoretical computer science, and statistics.},
  timestamp = {2017-09-04T20:41:37Z},
  urldate = {2017-09-04},
  howpublished = {\url{https://mitpress.mit.edu/books/introduction-computational-learning-theory}},
  journal = {MIT Press},
  file = {Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/KBUPS9TC/introduction-computational-learning-theory.html:text/html}
}

@book{barber_bayesian_2012,
  address = {New York, NY, USA},
  title = {Bayesian {{Reasoning}} and {{Machine Learning}}},
  isbn = {978-0-521-51814-7},
  abstract = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
  timestamp = {2017-09-04T20:30:52Z},
  publisher = {{Cambridge University Press}},
  author = {Barber, David},
  year = {2012},
  file = {Barber_2012_Bayesian Reasoning and Machine Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Barber_2012_Bayesian Reasoning and Machine Learning.pdf:application/pdf},
  groups = {Books}
}

@book{kearns_introduction_1994,
  title = {An {{Introduction}} to {{Computational Learning Theory}}},
  isbn = {978-0-262-11193-5},
  abstract = {Emphasizing issues of computational efficiency, Michael Kearns and Umesh Vazirani introduce a number of central topics in computational learning theory for researchers and students in artificial intelligence, neural networks, theoretical computer science, and statistics.Computational learning theory is a new and rapidly expanding area of research that examines formal models of induction with the goals of discovering the common methods underlying efficient learning algorithms and identifying the computational impediments to learning.Each topic in the book has been chosen to elucidate a general principle, which is explored in a precise formal setting. Intuition has been emphasized in the presentation to make the material accessible to the nontheoretician while still providing precise arguments for the specialist. This balance is the result of new proofs of established theorems, and new presentations of the standard proofs.The topics covered include the motivation, definitions, and fundamental results, both positive and negative, for the widely studied L. G. Valiant model of Probably Approximately Correct Learning; Occam's Razor, which formalizes a relationship between learning and data compression; the Vapnik-Chervonenkis dimension; the equivalence of weak and strong learning; efficient learning in the presence of noise by the method of statistical queries; relationships between learning and cryptography, and the resulting computational limitations on efficient learning; reducibility between learning problems; and algorithms for learning finite automata from active experimentation.},
  language = {en},
  timestamp = {2017-09-04T20:41:48Z},
  publisher = {{MIT Press}},
  author = {Kearns, Michael J. and Vazirani, Umesh Virkumar},
  year = {1994},
  note = {Google-Books-ID: vCA01wY6iywC},
  keywords = {Computers / Computer Science,Computers / Intelligence (AI) \& Semantics,Education / General},
  file = {Kearns_Vazirani_1994_An Introduction to Computational Learning Theory.pdf:/Users/fergalcotter/Dropbox/Papers/Kearns_Vazirani_1994_An Introduction to Computational Learning Theory.pdf:application/pdf},
  groups = {Books}
}

@article{gehring_convolutional_2017,
  title = {Convolutional {{Sequence}} to {{Sequence Learning}}},
  abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.},
  timestamp = {2017-09-06T10:22:02Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.03122},
  primaryClass = {cs},
  journal = {arXiv:1705.03122 [cs]},
  author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
  month = may,
  year = {2017},
  keywords = {Computer Science - Computation and Language},
  file = {Gehring et al_2017_Convolutional Sequence to Sequence Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Gehring et al_2017_Convolutional Sequence to Sequence Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/5ZUDSS6F/1705.html:text/html},
  groups = {Fun,Fun}
}

@misc{_two-dimensional_????,
  title = {Two-{{Dimensional Wavelets}} and Their {{Relatives}} - {{Jean}}-{{Pierre Antoine}}, {{Romain Murenzi}}, {{Pierre Vandergheynst}}, {{Syed Twareque Ali}} - {{Google Books}}},
  timestamp = {2017-09-04T20:19:18Z},
  urldate = {2017-09-04},
  howpublished = {\url{https://books.google.co.uk/books/about/Two_Dimensional_Wavelets_and_their_Relat.html?id=rKuTb4UHDA0C\&redir_esc=y}}
}

@book{murphy_machine_2012,
  title = {Machine {{Learning}}: {{A Probabilistic Perspective}}},
  isbn = {978-0-262-01802-9},
  shorttitle = {Machine {{Learning}}},
  abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
  timestamp = {2017-09-04T20:21:26Z},
  publisher = {{The MIT Press}},
  author = {Murphy, Kevin P.},
  year = {2012},
  file = {Murphy_2012_Machine Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Murphy_2012_Machine Learning.pdf:application/pdf},
  groups = {Books}
}

@misc{_schaums_????,
  title = {Schaum's {{Outline}} of {{Complex Variables}}, 2ed},
  abstract = {The guide that helps students study faster, learn better, and get top grades

More than 40 million students have trusted Schaum's to help them study faster, learn better, and get top grades. Now Schaum's is better than ever-with a new look, a new format with hundreds of practice problems, and completely updated information to conform to the latest developments in every field of study.

Fully compatible with your classroom text, Schaum's highlights all the important facts you need to know. Use Schaum's to shorten your study time-and get your best test scores!

Schaum's Outlines-Problem Solved.},
  timestamp = {2017-09-04T20:33:26Z},
  urldate = {2017-09-04},
  howpublished = {\url{https://www.mhprofessional.com/9780071615693-usa-schaums-outline-of-complex-variables-2ed-group}},
  journal = {McGraw-Hill Education},
  file = {Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/VICI2CEP/9780071615693-usa-schaums-outline-of-complex-variables-2ed-group.html:text/html}
}

@book{starck_sparse_2010,
  address = {New York, NY, USA},
  title = {Sparse {{Image}} and {{Signal Processing}}: {{Wavelets}}, {{Curvelets}}, {{Morphological Diversity}}},
  isbn = {978-0-521-11913-9},
  shorttitle = {Sparse {{Image}} and {{Signal Processing}}},
  abstract = {This book presents the state of the art in sparse and multiscale image and signal processing, covering linear multiscale transforms, such as wavelet, ridgelet, or curvelet transforms, and non-linear multiscale transforms based on the median and mathematical morphology operators. Recent concepts of sparsity and morphological diversity are described and exploited for various problems such as denoising, inverse problem regularization, sparse signal decomposition, blind source separation, and compressed sensing. This book weds theory and practice in examining applications in areas such as astronomy, biology, physics, digital media, and forensics. A final chapter explores a paradigm shift in signal processing, showing that previous limits to information sampling and extraction can be overcome in very significant ways. Matlab and IDL code accompany these methods and applications to reproduce the experiments and illustrate the reasoning and methodology of the research available for download at the associated Web site.},
  timestamp = {2017-09-04T20:18:46Z},
  publisher = {{Cambridge University Press}},
  author = {Starck, Jean-Luc and Murtagh, Fionn and Fadili, Jalal},
  year = {2010},
  file = {Starck et al_2010_Sparse Image and Signal Processing.pdf:/Users/fergalcotter/Dropbox/Papers/Starck et al_2010_Sparse Image and Signal Processing.pdf:application/pdf},
  groups = {Books}
}

@book{jensen_ripples_2001,
  title = {Ripples in {{Mathematics}}: {{The Discrete Wavelet Transform}}},
  isbn = {978-3-540-41662-3},
  shorttitle = {Ripples in {{Mathematics}}},
  abstract = {Yet another book on wavelets. There are many books on wavelets available, written for readers with different backgrounds. But the topic is becoming ever more important in mainstream signal processing, since the new JPEG2000 standard is based on wavelet techniques. Wavelet techniques are also impor tant in the MPEG-4 standard. So we thought that there might be room for yet another book on wavelets. This one is limited in scope, since it only covers the discrete wavelet trans form, which is central in modern digital signal processing. The presentation is based on the lifting technique discovered by W. Sweldens in 1994. Due to a result by I. Daubechies and W. Sweldens from 1996 this approach covers the same class of discrete wavelet transforms as the one based on two channel filter banks with perfect reconstruction. The goal of this book is to enable readers, with modest backgrounds in mathematics, signal analysis, and programming, to understand wavelet based techniques in signal analysis, and perhaps to enable them to apply such methods to real world problems. The book started as a set of lecture notes, written in Danish, for a group of teachers of signal analysis at Danish Engineering Colleges. The material has also been presented to groups of engineers working in industry, and used in mathematics courses at Aalborg University.},
  language = {en},
  timestamp = {2017-09-04T20:17:27Z},
  publisher = {{Springer Science \& Business Media}},
  author = {Jensen, A. and la Cour-Harbo, Anders},
  month = jun,
  year = {2001},
  note = {Google-Books-ID: hMvhjWxb0\_MC},
  keywords = {Computers / Computer Graphics,Computers / Intelligence (AI) \& Semantics,Computers / Optical Data Processing,Mathematics / Algebra / General,Mathematics / Algebra / Linear,Mathematics / Applied,Mathematics / Calculus,Mathematics / Mathematical Analysis,Science / Physics / Mathematical \& Computational,Technology \& Engineering / General},
  file = {Jensen_Cour-Harbo_2001_Ripples in Mathematics.pdf:/Users/fergalcotter/Dropbox/Papers/Jensen_Cour-Harbo_2001_Ripples in Mathematics.pdf:application/pdf},
  groups = {Books}
}

@book{mackay_information_2002,
  address = {New York, NY, USA},
  title = {Information {{Theory}}, {{Inference}} \& {{Learning Algorithms}}},
  isbn = {978-0-521-64298-9},
  timestamp = {2017-09-04T20:31:29Z},
  publisher = {{Cambridge University Press}},
  author = {MacKay, David J. C.},
  year = {2002},
  file = {MacKay_2002_Information Theory, Inference & Learning Algorithms.pdf:/Users/fergalcotter/Dropbox/Papers/MacKay_2002_Information Theory, Inference & Learning Algorithms.pdf:application/pdf},
  groups = {Books}
}

@misc{_sparse_????,
  title = {Sparse {{Image}} and {{Signal Processing}}},
  timestamp = {2017-09-04T20:18:25Z},
  urldate = {2017-09-04},
  howpublished = {\url{http://www.multiresolutions.com/sparsesignalrecipes/}}
}

@book{taubman_jpeg_2001,
  address = {Norwell, MA, USA},
  title = {{{JPEG}} 2000: {{Image Compression Fundamentals}}, {{Standards}} and {{Practice}}},
  isbn = {978-0-7923-7519-7},
  shorttitle = {{{JPEG}} 2000},
  timestamp = {2017-09-04T20:20:29Z},
  publisher = {{Kluwer Academic Publishers}},
  author = {Taubman, David S. and Marcellin, Michael W.},
  year = {2001},
  file = {Taubman_Marcellin_2001_JPEG 2000.pdf:/Users/fergalcotter/Dropbox/Papers/Taubman_Marcellin_2001_JPEG 2000.pdf:application/pdf},
  groups = {Books}
}

@book{rasmussen_gaussian_2006,
  title = {Gaussian {{Processes}} for {{Machine Learning}}},
  timestamp = {2017-09-04T20:39:28Z},
  publisher = {{the MIT Press}},
  author = {Rasmussen, C. E. and Williams, C. K. I.},
  year = {2006},
  file = {Rasmussen_Williams_2006_Gaussian Processes for Machine Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Rasmussen_Williams_2006_Gaussian Processes for Machine Learning.pdf:application/pdf},
  groups = {Books}
}

@book{bishop_pattern_2006,
  address = {Secaucus, NJ, USA},
  title = {Pattern {{Recognition}} and {{Machine Learning}} ({{Information Science}} and {{Statistics}})},
  isbn = {978-0-387-31073-2},
  timestamp = {2017-09-04T20:24:07Z},
  publisher = {{Springer-Verlag New York, Inc.}},
  author = {Bishop, Christopher M.},
  year = {2006},
  file = {Bishop_2006_Pattern Recognition and Machine Learning (Information Science and Statistics).pdf:/Users/fergalcotter/Dropbox/Papers/Bishop_2006_Pattern Recognition and Machine Learning (Information Science and Statistics).pdf:application/pdf;Bishop solutions.pdf:/Users/fergalcotter/Dropbox/Papers/Books/Bishop solutions.pdf:application/pdf;Bishop solutions.pdf:/Users/fergalcotter/Dropbox/Papers/Books/Bishop solutions.pdf:application/pdf},
  groups = {Books}
}

@book{antoine_two-dimensional_2004,
  title = {Two-{{Dimensional Wavelets}} and Their {{Relatives}}},
  timestamp = {2017-09-04T20:22:44Z},
  urldate = {2017-09-04},
  author = {Antoine, J. and Murenzi, R. and Vandergheynst, P. and Ali, S.},
  year = {2004},
  file = {Antoine et al_2004_Two-Dimensional Wavelets and their Relatives.pdf:/Users/fergalcotter/Dropbox/Papers/Antoine et al_2004_Two-Dimensional Wavelets and their Relatives.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/B45RPUW4/87060.html:text/html},
  groups = {Books}
}

@book{winkler_image_2006,
  address = {Secaucus, NJ, USA},
  title = {Image {{Analysis}}, {{Random Fields}} and {{Markov Chain Monte Carlo Methods}}: {{A Mathematical Introduction}} ({{Stochastic Modelling}} and {{Applied Probability}})},
  isbn = {978-3-540-44213-4},
  shorttitle = {Image {{Analysis}}, {{Random Fields}} and {{Markov Chain Monte Carlo Methods}}},
  timestamp = {2017-09-04T20:27:15Z},
  publisher = {{Springer-Verlag New York, Inc.}},
  author = {Winkler, Gerhard},
  year = {2006},
  file = {Winkler_2006_Image Analysis, Random Fields and Markov Chain Monte Carlo Methods.pdf:/Users/fergalcotter/Dropbox/Papers/Winkler_2006_Image Analysis, Random Fields and Markov Chain Monte Carlo Methods.pdf:application/pdf},
  groups = {Books}
}

@misc{_sparse_????-2,
  title = {Sparse {{Image}} and {{Signal Processing}}: {{Wavelets}}, {{Curvelets}}, {{Morphological Diversity}}: {{Amazon}}.Co.Uk: {{Jean}}-{{Luc Starck}}, {{Fionn Murtagh}}, {{Jalal M}}. {{Fadili}}: 9780521119139: {{Books}}},
  timestamp = {2017-09-04T20:18:33Z},
  urldate = {2017-09-04},
  howpublished = {\url{https://www.amazon.co.uk/Sparse-Image-Signal-Processing-Morphological/dp/0521119138}}
}

@inproceedings{abadi_deep_2016,
  address = {New York, NY, USA},
  series = {CCS '16},
  title = {Deep {{Learning}} with {{Differential Privacy}}},
  isbn = {978-1-4503-4139-4},
  doi = {10.1145/2976749.2978318},
  abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
  timestamp = {2017-09-04T20:25:12Z},
  booktitle = {Proceedings of the 2016 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  publisher = {{ACM}},
  author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  year = {2016},
  keywords = {deep learning,differential privacy},
  pages = {308--318},
  file = {Abadi et al_2016_Deep Learning with Differential Privacy.pdf:/Users/fergalcotter/Dropbox/Papers/Abadi et al_2016_Deep Learning with Differential Privacy.pdf:application/pdf}
}

@book{_ripples_????,
  title = {Ripples in {{Mathematics}} - {{The Discrete Wavelet Transform}} | {{A}}. {{Jensen}} | {{Springer}}},
  isbn = {978-3-642-56702-5},
  abstract = {Yet another book on wavelets. There are many books on wavelets available, written for readers with different backgrounds. But the topic is becoming ever...},
  timestamp = {2017-09-04T20:16:42Z},
  urldate = {2017-09-04},
  file = {Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/VZCDTCSW/9783540416623.html:text/html}
}

@misc{_deep_????,
  title = {Deep {{Learning}}},
  timestamp = {2017-09-04T20:25:26Z},
  urldate = {2017-09-04},
  howpublished = {\url{http://www.deeplearningbook.org/}}
}

@book{spiegel_schaums_2009,
  address = {New York},
  edition = {2 edition},
  title = {Schaum's {{Outline}} of {{Complex Variables}}, 2ed},
  isbn = {978-0-07-161569-3},
  abstract = {The guide that helps students study faster, learn better, and get top gradesMore than 40 million students have trusted Schaum's to help them study faster, learn better, and get top grades. Now Schaum's is better than ever-with a new look, a new format with hundreds of practice problems, and completely updated information to conform to the latest developments in every field of study.Fully compatible with your classroom text, Schaum's highlights all the important facts you need to know. Use Schaum's to shorten your study time-and get your best test scores!Schaum's Outlines-Problem Solved.},
  language = {English},
  timestamp = {2017-09-04T20:33:48Z},
  publisher = {{McGraw-Hill Education}},
  author = {Spiegel, Murray R. and Lipschutz, Seymour and Schiller, John J. and Spellman, Dennis},
  month = jul,
  year = {2009},
  file = {Spiegel et al_2009_Schaum's Outline of Complex Variables, 2ed.pdf:/Users/fergalcotter/Dropbox/Papers/Spiegel et al_2009_Schaum's Outline of Complex Variables, 2ed.pdf:application/pdf},
  groups = {Books}
}

@misc{_reinforcement_????,
  title = {Reinforcement {{Learning}}},
  abstract = {Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications.},
  timestamp = {2017-09-04T20:34:37Z},
  urldate = {2017-09-04},
  howpublished = {\url{https://mitpress.mit.edu/books/reinforcement-learning}},
  journal = {MIT Press},
  file = {Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/ZIF2CJGA/reinforcement-learning.html:text/html}
}

@book{richard_sutton_reinforcement_????,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  isbn = {978-0-262-30384-2},
  abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
  timestamp = {2017-09-04T20:38:41Z},
  urldate = {2017-09-04},
  publisher = {{MIT Press}},
  author = {{Richard Sutton} and {Andrew Barto}},
  file = {Richard Sutton_Andrew Barto_Reinforcement Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Richard Sutton_Andrew Barto_Reinforcement Learning.pdf:application/pdf},
  groups = {Books}
}

@book{montavon_neural_2012,
  edition = {2nd},
  title = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  isbn = {978-3-642-35288-1},
  shorttitle = {Neural {{Networks}}},
  abstract = {The twenty last years have been marked by an increase in available data and computing power. In parallel to this trend, the focus of neural network research and the practice of training neural networks has undergone a number of important changes, for example, use of deep learning machines. The second edition of the book augments the first edition with more tricks, which have resulted from 14 years of theory and experimentation by some of the world's most prominent neural network researchers. These tricks can make a substantial difference (in terms of speed, ease of implementation, and accuracy) when it comes to putting algorithms to work on real problems.},
  timestamp = {2017-09-04T20:28:40Z},
  publisher = {{Springer Publishing Company, Incorporated}},
  author = {Montavon, Grgoire and Orr, Genevive and Mller, Klaus-Robert},
  year = {2012},
  file = {Montavon et al_2012_Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Montavon et al_2012_Neural Networks.pdf:application/pdf},
  groups = {Books}
}

@misc{_machine_????,
  title = {Machine {{Learning}}},
  abstract = {A comprehensive introduction to machine learning that uses probabilistic models and inference as a unifying approach.},
  timestamp = {2017-09-04T20:21:00Z},
  urldate = {2017-09-04},
  howpublished = {\url{https://mitpress.mit.edu/books/machine-learning-0}},
  journal = {MIT Press},
  file = {Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/HDMJGS2Z/machine-learning-0.html:text/html}
}

@book{nick_mcclure_tensorflow_2017,
  title = {{{TensorFlow Machine Learning Cookbook}}},
  isbn = {978-1-78646-216-9},
  abstract = {TensorFlow is an open source software library for Machine Intelligence. The independent recipes in this book will teach you how to use TensorFlow for complex data computations and will let you dig deeper and gain more insights into your data than ever before. You'll work through recipes on training models, model evaluation, sentiment analysis, regression analysis, clustering analysis, artificial neural networks, and deep learning \textendash{} each using Google's machine learning library TensorFlow.

This guide starts with the fundamentals of the TensorFlow library which includes variables, matrices, and various data sources. Moving ahead, you will get hands-on experience with Linear Regression techniques with TensorFlow. The next chapters cover important high-level concepts such as neural networks, CNN, RNN, and NLP.

Once you are familiar and comfortable with the TensorFlow ecosystem, the last chapter will show you how to take it to production.},
  timestamp = {2017-09-12T16:56:30Z},
  urldate = {2017-09-12},
  publisher = {{Packt}},
  author = {{Nick McClure}},
  month = feb,
  year = {2017},
  file = {Nick McClure_2017_TensorFlow Machine Learning Cookbook.pdf:/Users/fergalcotter/Dropbox/Papers/Nick McClure_2017_TensorFlow Machine Learning Cookbook.pdf:application/pdf},
  groups = {Books}
}

@article{gomez_reversible_2017,
  title = {The {{Reversible Residual Network}}: {{Backpropagation Without Storing Activations}}},
  shorttitle = {The {{Reversible Residual Network}}},
  abstract = {Deep residual networks (ResNets) have significantly pushed forward the state-of-the-art on image classification, increasing in performance as networks grow both deeper and wider. However, memory consumption becomes a bottleneck, as one needs to store the activations in order to calculate gradients using backpropagation. We present the Reversible Residual Network (RevNet), a variant of ResNets where each layer's activations can be reconstructed exactly from the next layer's. Therefore, the activations for most layers need not be stored in memory during backpropagation. We demonstrate the effectiveness of RevNets on CIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification accuracy to equally-sized ResNets, even though the activation storage requirements are independent of depth.},
  timestamp = {2017-09-16T11:33:58Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.04585},
  primaryClass = {cs},
  journal = {arXiv:1707.04585 [cs]},
  author = {Gomez, Aidan N. and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B.},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  file = {Gomez et al_2017_The Reversible Residual Network.pdf:/Users/fergalcotter/Dropbox/Papers/Gomez et al_2017_The Reversible Residual Network.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/A6URSW8R/1707.html:text/html},
  groups = {Deconv,Lifting,Deconv,Lifting}
}

@article{cotter_visualizing_2017,
  title = {Visualizing and {{Improving Scattering Networks}}},
  abstract = {Scattering Transforms (or ScatterNets) introduced by Mallat are a promising start into creating a well-defined feature extractor to use for pattern recognition and image classification tasks. They are of particular interest due to their architectural similarity to Convolutional Neural Networks (CNNs), while requiring no parameter learning and still performing very well (particularly in constrained classification tasks). In this paper we visualize what the deeper layers of a ScatterNet are sensitive to using a 'DeScatterNet'. We show that the higher orders of ScatterNets are sensitive to complex, edge-like patterns (checker-boards and rippled edges). These complex patterns may be useful for texture classification, but are quite dissimilar from the patterns visualized in second and third layers of Convolutional Neural Networks (CNNs) - the current state of the art Image Classifiers. We propose that this may be the source of the current gaps in performance between ScatterNets and CNNs (83\% vs 93\% on CIFAR-10 for ScatterNet+SVM vs ResNet). We then use these visualization tools to propose possible enhancements to the ScatterNet design, which show they have the power to extract features more closely resembling CNNs, while still being well-defined and having the invariance properties fundamental to ScatterNets.},
  timestamp = {2017-09-19T10:41:08Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.01355},
  primaryClass = {cs},
  journal = {arXiv:1709.01355 [cs]},
  author = {Cotter, Fergal and Kingsbury, Nick},
  month = sep,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote = {Comment: To Appear in the 27th IEEE International Workshop on Machine Learning For Signal Processing (MLSP) 2017. 6 pages, 3 figures},
  file = {Cotter_Kingsbury_2017_Visualizing and Improving Scattering Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Cotter_Kingsbury_2017_Visualizing and Improving Scattering Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/R6ZVM5CK/1709.html:text/html},
  groups = {Scatternets,Scatternets}
}

@article{mathieu_fast_2013,
  title = {Fast {{Training}} of {{Convolutional Networks}} through {{FFTs}}},
  abstract = {Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.},
  timestamp = {2017-09-28T02:55:09Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.5851},
  primaryClass = {cs},
  journal = {arXiv:1312.5851 [cs]},
  author = {Mathieu, Michael and Henaff, Mikael and LeCun, Yann},
  month = dec,
  year = {2013},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Mathieu et al_2013_Fast Training of Convolutional Networks through FFTs.pdf:/Users/fergalcotter/Dropbox/Papers/Mathieu et al_2013_Fast Training of Convolutional Networks through FFTs.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/TJTJIS3E/1312.html:text/html},
  groups = {FFT + Spectral,FFT + Spectral}
}

@article{highlander_very_2016,
  title = {Very {{Efficient Training}} of {{Convolutional Neural Networks}} Using {{Fast Fourier Transform}} and {{Overlap}}-and-{{Add}}},
  abstract = {Convolutional neural networks (CNNs) are currently state-of-the-art for various classification tasks, but are computationally expensive. Propagating through the convolutional layers is very slow, as each kernel in each layer must sequentially calculate many dot products for a single forward and backward propagation which equates to \$$\backslash$mathcal\{O\}(N\^\{2\}n\^\{2\})\$ per kernel per layer where the inputs are \$N $\backslash$times N\$ arrays and the kernels are \$n $\backslash$times n\$ arrays. Convolution can be efficiently performed as a Hadamard product in the frequency domain. The bottleneck is the transformation which has a cost of \$$\backslash$mathcal\{O\}(N\^\{2\}$\backslash$log\_2 N)\$ using the fast Fourier transform (FFT). However, the increase in efficiency is less significant when \$N$\backslash$gg n\$ as is the case in CNNs. We mitigate this by using the "overlap-and-add" technique reducing the computational complexity to \$$\backslash$mathcal\{O\}(N\^2$\backslash$log\_2 n)\$ per kernel. This method increases the algorithm's efficiency in both the forward and backward propagation, reducing the training and testing time for CNNs. Our empirical results show our method reduces computational time by a factor of up to 16.3 times the traditional convolution implementation for a 8 \$$\backslash$times\$ 8 kernel and a 224 \$$\backslash$times\$ 224 image.},
  timestamp = {2017-09-28T02:55:25Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1601.06815},
  primaryClass = {cs},
  journal = {arXiv:1601.06815 [cs]},
  author = {Highlander, Tyler and Rodriguez, Andres},
  month = jan,
  year = {2016},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: British Machine Vision Conference 2015},
  file = {Highlander_Rodriguez_2016_Very Efficient Training of Convolutional Neural Networks using Fast Fourier.pdf:/Users/fergalcotter/Dropbox/Papers/Highlander_Rodriguez_2016_Very Efficient Training of Convolutional Neural Networks using Fast Fourier.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/74B4JN5E/1601.html:text/html},
  groups = {FFT + Spectral,FFT + Spectral}
}

@article{vasilache_fast_2014,
  title = {Fast {{Convolutional Nets With}} Fbfft: {{A GPU Performance Evaluation}}},
  shorttitle = {Fast {{Convolutional Nets With}} Fbfft},
  abstract = {We examine the performance profile of Convolutional Neural Network training on the current generation of NVIDIA Graphics Processing Units. We introduce two new Fast Fourier Transform convolution implementations: one based on NVIDIA's cuFFT library, and another based on a Facebook authored FFT implementation, fbfft, that provides significant speedups over cuFFT (over 1.5x) for whole CNNs. Both of these convolution implementations are available in open source, and are faster than NVIDIA's cuDNN implementation for many common convolutional layers (up to 23.5x for some synthetic kernel configurations). We discuss different performance regimes of convolutions, comparing areas where straightforward time domain convolutions outperform Fourier frequency domain convolutions. Details on algorithmic applications of NVIDIA GPU hardware specifics in the implementation of fbfft are also provided.},
  timestamp = {2017-09-28T02:59:45Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.7580},
  primaryClass = {cs},
  journal = {arXiv:1412.7580 [cs]},
  author = {Vasilache, Nicolas and Johnson, Jeff and Mathieu, Michael and Chintala, Soumith and Piantino, Serkan and LeCun, Yann},
  month = dec,
  year = {2014},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: Camera ready for ICLR2015},
  file = {Vasilache et al_2014_Fast Convolutional Nets With fbfft.pdf:/Users/fergalcotter/Dropbox/Papers/Vasilache et al_2014_Fast Convolutional Nets With fbfft.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/H5FVBN98/1412.html:text/html},
  groups = {FFT + Spectral,FFT + Spectral}
}

@incollection{rippel_spectral_2015-1,
  title = {Spectral {{Representations}} for {{Convolutional Neural Networks}}},
  timestamp = {2017-09-28T03:02:50Z},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  publisher = {{Curran Associates, Inc.}},
  author = {Rippel, Oren and Snoek, Jasper and Adams, Ryan P},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {2449--2457},
  file = {Rippel et al_2015_Spectral Representations for Convolutional Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Rippel et al_2015_Spectral Representations for Convolutional Neural Networks_2.pdf:application/pdf;NIPS Snapshort:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/DTIRZVEX/5649-spectral-representations-for-convolutional-neural-networks.html:text/html},
  groups = {FFT + Spectral,FFT + Spectral}
}

@article{trabelsi_deep_2017,
  title = {Deep {{Complex Networks}}},
  abstract = {At present, the vast majority of building blocks, techniques, and architectures for deep learning are based on real-valued operations and representations. However, recent work on recurrent neural networks and older fundamental theoretical analysis suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms. Despite their attractive properties and potential for opening up entirely new neural architectures, complex-valued deep neural networks have been marginalized due to the absence of the building blocks required to design such models. In this work, we provide the key atomic components for complex-valued deep neural networks and apply them to convolutional feed-forward networks. More precisely, we rely on complex convolutions and present algorithms for complex batch-normalization, complex weight initialization strategies for complex-valued neural nets and we use them in experiments with end-to-end training schemes. We demonstrate that such complex-valued models are able to achieve comparable or better performance than their real-valued counterparts. We test deep complex models on several computer vision tasks and on music transcription using the MusicNet dataset where we achieve state of the art performance.},
  timestamp = {2017-09-28T03:26:54Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.09792},
  primaryClass = {cs},
  journal = {arXiv:1705.09792 [cs]},
  author = {Trabelsi, Chiheb and Bilaniuk, Olexa and Serdyuk, Dmitriy and Subramanian, Sandeep and Santos, Jo{\~a}o Felipe and Mehri, Soroush and Rostamzadeh, Negar and Bengio, Yoshua and Pal, Christopher J.},
  month = may,
  year = {2017},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Trabelsi et al_2017_Deep Complex Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Trabelsi et al_2017_Deep Complex Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/UPS4BB94/1705.html:text/html},
  groups = {FFT + Spectral,FFT + Spectral}
}

@article{fujieda_wavelet_2017,
  title = {Wavelet {{Convolutional Neural Networks}} for {{Texture Classification}}},
  abstract = {Texture classification is an important and challenging problem in many image processing applications. While convolutional neural networks (CNNs) achieved significant successes for image classification, texture classification remains a difficult problem since textures usually do not contain enough information regarding the shape of object. In image processing, texture classification has been traditionally studied well with spectral analyses which exploit repeated structures in many textures. Since CNNs process images as-is in the spatial domain whereas spectral analyses process images in the frequency domain, these models have different characteristics in terms of performance. We propose a novel CNN architecture, wavelet CNNs, which integrates a spectral analysis into CNNs. Our insight is that the pooling layer and the convolution layer can be viewed as a limited form of a spectral analysis. Based on this insight, we generalize both layers to perform a spectral analysis with wavelet transform. Wavelet CNNs allow us to utilize spectral information which is lost in conventional CNNs but useful in texture classification. The experiments demonstrate that our model achieves better accuracy in texture classification than existing models. We also show that our model has significantly fewer parameters than CNNs, making our model easier to train with less memory.},
  timestamp = {2017-10-12T15:54:27Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.07394},
  primaryClass = {cs},
  journal = {arXiv:1707.07394 [cs]},
  author = {Fujieda, Shin and Takayama, Kohei and Hachisuka, Toshiya},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  annote = {Comment: 9 pages, 7 figures, 2 tables},
  file = {Fujieda et al_2017_Wavelet Convolutional Neural Networks for Texture Classification.pdf:/Users/fergalcotter/Dropbox/Papers/Fujieda et al_2017_Wavelet Convolutional Neural Networks for Texture Classification.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/IV3K5KEG/1707.html:text/html},
  groups = {FFT + Spectral,FFT + Spectral}
}

@inproceedings{sun_design_2016,
  series = {Lecture Notes in Computer Science},
  title = {Design of {{Kernels}} in {{Convolutional Neural Networks}} for {{Image Classification}}},
  isbn = {978-3-319-46477-0 978-3-319-46478-7},
  doi = {10.1007/978-3-319-46478-7_4},
  abstract = {Despite the effectiveness of convolutional neural networks (CNNs) for image classification, our understanding of the effect of shape of convolution kernels on learned representations is limited. In this work, we explore and employ the relationship between shape of kernels which define receptive fields (RFs) in CNNs for learning of feature representations and image classification. For this purpose, we present a feature visualization method for visualization of pixel-wise classification score maps of learned features. Motivated by our experimental results, and observations reported in the literature for modeling of visual systems, we propose a novel design of shape of kernels for learning of representations in CNNs.In the experimental results, the proposed models also outperform the state-of-the-art methods employed on the CIFAR-10/100 datasets [1] for image classification. We also achieved an outstanding performance in the classification task, comparing to a base CNN model that introduces more parameters and computational time, using the ILSVRC-2012 dataset [2]. Additionally, we examined the region of interest (ROI) of different models in the classification task and analyzed the robustness of the proposed method to occluded images. Our results indicate the effectiveness of the proposed approach.},
  language = {en},
  timestamp = {2017-10-12T15:54:49Z},
  urldate = {2017-10-12},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2016},
  publisher = {{Springer, Cham}},
  author = {Sun, Zhun and Ozay, Mete and Okatani, Takayuki},
  month = oct,
  year = {2016},
  pages = {51--66},
  file = {Sun et al_2016_Design of Kernels in Convolutional Neural Networks for Image Classification.pdf:/Users/fergalcotter/Dropbox/Papers/Sun et al_2016_Design of Kernels in Convolutional Neural Networks for Image Classification.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/MFINQPXZ/978-3-319-46478-7_4.html:text/html},
  groups = {FFT + Spectral,FFT + Spectral}
}

@article{jacobsen_dynamic_2017,
  title = {Dynamic {{Steerable Blocks}} in {{Deep Residual Networks}}},
  abstract = {Filters in convolutional networks are typically parameterized in a pixel basis, that does not take prior knowledge about the visual world into account. We investigate the generalized notion of frames designed with image properties in mind, as alternatives to this parametrization. We show that frame-based ResNets and Densenets can improve performance on Cifar-10+ consistently, while having additional pleasant properties like steerability. By exploiting these transformation properties explicitly, we arrive at dynamic steerable blocks. They are an extension of residual blocks, that are able to seamlessly transform filters under pre-defined transformations, conditioned on the input at training and inference time. Dynamic steerable blocks learn the degree of invariance from data and locally adapt filters, allowing them to apply a different geometrical variant of the same filter to each location of the feature map. When evaluated on the Berkeley Segmentation contour detection dataset, our approach outperforms all competing approaches that do not utilize pre-training. Our results highlight the benefits of image-based regularization to deep networks.},
  timestamp = {2017-10-12T15:55:13Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.00598},
  primaryClass = {cs, stat},
  journal = {arXiv:1706.00598 [cs, stat]},
  author = {Jacobsen, J{\"o}rn-Henrik and {de Brabandere}, Bert and Smeulders, Arnold W. M.},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  file = {Jacobsen et al_2017_Dynamic Steerable Blocks in Deep Residual Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Jacobsen et al_2017_Dynamic Steerable Blocks in Deep Residual Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/QDC4QZ9R/1706.html:text/html},
  groups = {FFT + Spectral,FFT + Spectral}
}

@article{chen_compressing_2015,
  title = {Compressing {{Convolutional Neural Networks}}},
  abstract = {Convolutional neural networks (CNN) are increasingly used in many areas of computer vision. They are particularly attractive because of their ability to "absorb" great quantities of labeled data through millions of parameters. However, as model sizes increase, so do the storage and memory requirements of the classifiers. We present a novel network architecture, Frequency-Sensitive Hashed Nets (FreshNets), which exploits inherent redundancy in both convolutional layers and fully-connected layers of a deep learning model, leading to dramatic savings in memory and storage consumption. Based on the key observation that the weights of learned convolutional filters are typically smooth and low-frequency, we first convert filter weights to the frequency domain with a discrete cosine transform (DCT) and use a low-cost hash function to randomly group frequency parameters into hash buckets. All parameters assigned the same hash bucket share a single value learned with standard back-propagation. To further reduce model size we allocate fewer hash buckets to high-frequency components, which are generally less important. We evaluate FreshNets on eight data sets, and show that it leads to drastically better compressed performance than several relevant baselines.},
  timestamp = {2017-10-12T15:56:04Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.04449},
  primaryClass = {cs},
  journal = {arXiv:1506.04449 [cs]},
  author = {Chen, Wenlin and Wilson, James T. and Tyree, Stephen and Weinberger, Kilian Q. and Chen, Yixin},
  month = jun,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Chen et al_2015_Compressing Convolutional Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Chen et al_2015_Compressing Convolutional Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/4UIZUU29/1506.html:text/html},
  groups = {FFT + Spectral,FFT + Spectral}
}

@inproceedings{chen_compressing_2016,
  address = {New York, NY, USA},
  series = {KDD '16},
  title = {Compressing {{Convolutional Neural Networks}} in the {{Frequency Domain}}},
  isbn = {978-1-4503-4232-2},
  doi = {10.1145/2939672.2939839},
  abstract = {Convolutional neural networks (CNN) are increasingly used in many areas of computer vision. They are particularly attractive because of their ability to "absorb" great quantities of labeled data through millions of parameters. However, as model sizes increase, so do the storage and memory requirements of the classifiers, hindering many applications such as image and speech recognition on mobile phones and other devices. In this paper, we present a novel net- work architecture, Frequency-Sensitive Hashed Nets (FreshNets), which exploits inherent redundancy in both convolutional layers and fully-connected layers of a deep learning model, leading to dramatic savings in memory and storage consumption. Based on the key observation that the weights of learned convolutional filters are typically smooth and low-frequency, we first convert filter weights to the frequency domain with a discrete cosine transform (DCT) and use a low-cost hash function to randomly group frequency parameters into hash buckets. All parameters assigned the same hash bucket share a single value learned with standard back-propagation. To further reduce model size, we allocate fewer hash buckets to high-frequency components, which are generally less important. We evaluate FreshNets on eight data sets, and show that it leads to better compressed performance than several relevant baselines.},
  timestamp = {2017-10-12T15:56:16Z},
  booktitle = {Proceedings of the {{22Nd ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{ACM}},
  author = {Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian Q. and Chen, Yixin},
  year = {2016},
  keywords = {convolutional neural networks,hashing,model compression},
  pages = {1475--1484},
  file = {Chen et al_2016_Compressing Convolutional Neural Networks in the Frequency Domain.pdf:/Users/fergalcotter/Dropbox/Papers/Chen et al_2016_Compressing Convolutional Neural Networks in the Frequency Domain.pdf:application/pdf},
  groups = {FFT + Spectral,FFT + Spectral}
}

@inproceedings{lu_hierarchical_2014,
  title = {Hierarchical Image Representation via Multi-Level Sparse Coding},
  doi = {10.1109/ICIP.2014.7025993},
  abstract = {This paper presents a hierarchical model for robust image representation. We first introduce multi-level sparse coding algorithm and normalized max pooling strategy which are designed to obtain meaningful sparse codes and robust pooled codes, respectively. With the sparse codes and pooled codes, a hierarchical architecture is built and more robust features are extracted at the second layer. The proposed method has been evaluated on two widely used datasets: Caltech-101 and Caltech-256, and experimental results demonstrate that the proposed method is both effective and robust in image representation compared with the state-of-the-art.},
  timestamp = {2017-10-18T18:55:29Z},
  booktitle = {2014 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Lu, K. and Li, J. and An, X. and He, H.},
  month = oct,
  year = {2014},
  keywords = {Caltech-101,Caltech-256,Dictionaries,Encoding,Feature extraction,hierarchical,hierarchical image representation,Image coding,Image representation,multi-level sparse coding,multilevel sparse coding,normalized max pooling,normalized max pooling strategy,PSNR,robust image representation,Robustness,robust pooled codes},
  pages = {4902--4906},
  file = {Lu et al_2014_Hierarchical image representation via multi-level sparse coding.pdf:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/QQELJF4M/Lu et al_2014_Hierarchical image representation via multi-level sparse coding.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/CM9VHJAM/7025993.html:text/html},
  groups = {Sparse Coding,Sparse Coding}
}

@article{mallat_understanding_2016-1,
  title = {Understanding {{Deep Convolutional Networks}}},
  volume = {374},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2015.0203},
  abstract = {Deep convolutional networks provide state of the art classifications and regressions results over many high-dimensional problems. We review their architecture, which scatters data with a cascade of linear filter weights and non-linearities. A mathematical framework is introduced to analyze their properties. Computations of invariants involve multiscale contractions, the linearization of hierarchical symmetries, and sparse separations. Applications are discussed.},
  timestamp = {2017-10-24T12:08:09Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1601.04920},
  number = {2065},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  author = {Mallat, St{\'e}phane},
  month = apr,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Statistics - Machine Learning},
  pages = {20150203},
  annote = {Comment: 17 pages, 4 Figures},
  file = {Mallat_2016_Understanding Deep Convolutional Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Mallat_2016_Understanding Deep Convolutional Networks_2.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/SSPIWG2K/1601.html:text/html},
  groups = {Maths,Maths}
}

@article{wiatowski_energy_2017,
  title = {Energy {{Propagation}} in {{Deep Convolutional Neural Networks}}},
  abstract = {Many practical machine learning tasks employ very deep convolutional neural networks. Such large depths pose formidable computational challenges in training and operating the network. It is therefore important to understand how fast the energy contained in the propagated signals (a.k.a. feature maps) decays across layers. In addition, it is desirable that the feature extractor generated by the network be informative in the sense of the only signal mapping to the all-zeros feature vector being the zero input signal. This "trivial null-space" property can be accomplished by asking for "energy conservation" in the sense of the energy in the feature vector being proportional to that of the corresponding input signal. This paper establishes conditions for energy conservation (and thus for a trivial null-space) for a wide class of deep convolutional neural network-based feature extractors and characterizes corresponding feature map energy decay rates. Specifically, we consider general scattering networks employing the modulus non-linearity and we find that under mild analyticity and high-pass conditions on the filters (which encompass, inter alia, various constructions of Weyl-Heisenberg filters, wavelets, ridgelets, (\$$\backslash$alpha\$)-curvelets, and shearlets) the feature map energy decays at least polynomially fast. For broad families of wavelets and Weyl-Heisenberg filters, the guaranteed decay rate is shown to be exponential. Moreover, we provide handy estimates of the number of layers needed to have at least \$((1-$\backslash$varepsilon)$\backslash$cdot 100)$\backslash$\%\$ of the input signal energy be contained in the feature vector.},
  timestamp = {2017-10-24T12:05:24Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.03636},
  primaryClass = {cs, math, stat},
  journal = {arXiv:1704.03636 [cs, math, stat]},
  author = {Wiatowski, Thomas and Grohs, Philipp and B{\"o}lcskei, Helmut},
  month = apr,
  year = {2017},
  keywords = {Computer Science - Information Theory,Computer Science - Learning,Mathematics - Functional Analysis,Statistics - Machine Learning},
  annote = {Comment: IEEE Transactions on Information Theory, September 2017, to appear},
  file = {Wiatowski et al_2017_Energy Propagation in Deep Convolutional Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Wiatowski et al_2017_Energy Propagation in Deep Convolutional Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/4573BKCM/1704.html:text/html},
  groups = {Maths,Maths}
}

@incollection{mairal_convolutional_2014-1,
  title = {Convolutional {{Kernel Networks}}},
  timestamp = {2017-10-19T16:21:18Z},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  publisher = {{Curran Associates, Inc.}},
  author = {Mairal, Julien and Koniusz, Piotr and Harchaoui, Zaid and Schmid, Cordelia},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  pages = {2627--2635},
  file = {Mairal et al_2014_Convolutional Kernel Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Mairal et al_2014_Convolutional Kernel Networks_2.pdf:application/pdf;NIPS Snapshort:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/75IT2ZCP/5348-convolutional-kernel-networks.html:text/html},
  groups = {Scat Ideas,Scat Ideas}
}

@article{badrinarayanan_segnet:_2017,
  title = {{{SegNet}}: {{A Deep Convolutional Encoder}}-{{Decoder Architecture}} for {{Scene Segmentation}}},
  volume = {PP},
  issn = {0162-8828},
  shorttitle = {{{SegNet}}},
  doi = {10.1109/TPAMI.2016.2644615},
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.a- .uk/projects/segnet/.},
  timestamp = {2017-10-19T12:07:03Z},
  number = {99},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Badrinarayanan, V. and Kendall, A. and Cipolla, R.},
  year = {2017},
  keywords = {Computer architecture,Decoder,Decoding,Deep Convolutional Neural Networks,Encoder,image segmentation,Indoor Scenes,Neural networks,Pooling,Roads,Road Scenes,Semantic Pixel-Wise Segmentation,Semantics,Training,Upsampling},
  pages = {1--1},
  file = {Badrinarayanan et al_2017_SegNet.pdf:/Users/fergalcotter/Dropbox/Papers/Badrinarayanan et al_2017_SegNet.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/9TW59FXI/7803544.html:text/html},
  groups = {FCNs,FCNs}
}

@article{li_fully_2016,
  title = {Fully {{Convolutional Instance}}-Aware {{Semantic Segmentation}}},
  abstract = {We present the first fully convolutional end-to-end solution for instance-aware semantic segmentation task. It inherits all the merits of FCNs for semantic segmentation and instance mask proposal. It performs instance mask prediction and classification jointly. The underlying convolutional representation is fully shared between the two sub-tasks, as well as between all regions of interest. The proposed network is highly integrated and achieves state-of-the-art performance in both accuracy and efficiency. It wins the COCO 2016 segmentation competition by a large margin. Code would be released at $\backslash$url\{https://github.com/daijifeng001/TA-FCN\}.},
  timestamp = {2017-10-19T11:40:52Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.07709},
  primaryClass = {cs},
  journal = {arXiv:1611.07709 [cs]},
  author = {Li, Yi and Qi, Haozhi and Dai, Jifeng and Ji, Xiangyang and Wei, Yichen},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Li et al_2016_Fully Convolutional Instance-aware Semantic Segmentation.pdf:/Users/fergalcotter/Dropbox/Papers/Li et al_2016_Fully Convolutional Instance-aware Semantic Segmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/7NCEF5CK/1611.html:text/html},
  groups = {FCNs,FCNs}
}

@article{anden_deep_2014,
  title = {Deep {{Scattering Spectrum}}},
  volume = {62},
  issn = {1053-587X},
  doi = {10.1109/TSP.2014.2326991},
  abstract = {A scattering transform defines a locally translation invariant representation which is stable to time-warping deformation. It extends MFCC representations by computing modulation spectrum coefficients of multiple orders, through cascades of wavelet convolutions and modulus operators. Second-order scattering coefficients characterize transient phenomena such as attacks and amplitude modulation. A frequency transposition invariant representation is obtained by applying a scattering transform along log-frequency. State-the-of-art classification results are obtained for musical genre and phone classification on GTZAN and TIMIT databases, respectively.},
  timestamp = {2017-10-19T16:21:29Z},
  number = {16},
  journal = {IEEE Transactions on Signal Processing},
  author = {And{\'e}n, J. and Mallat, S.},
  month = aug,
  year = {2014},
  keywords = {acoustic wave scattering,amplitude modulation,audio classification,audio signal processing,cepstral analysis,Convolution,deep neural networks,deep scattering spectrum,Frequency modulation,frequency transposition invariant representation,GTZAN database,mel-frequency cepstral coefficients,MFCC,modulation spectrum,modulus operators,musical genre,phone classification,Scattering,scattering transform,second-order scattering coefficients,signal classification,signal representation,Spectrogram,spectrum coefficients,time-warping deformation,TIMIT database,transient phenomena,Wavelet analysis,wavelet convolutions,wavelets,wavelet transforms},
  pages = {4114--4128},
  file = {Andén_Mallat_2014_Deep Scattering Spectrum.pdf:/Users/fergalcotter/Dropbox/Papers/Andén_Mallat_2014_Deep Scattering Spectrum.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/PNHGXRS3/6822556.html:text/html},
  groups = {Scat Ideas,Scat Ideas}
}

@article{wiatowski_discrete_2016,
  title = {Discrete {{Deep Feature Extraction}}: {{A Theory}} and {{New Architectures}}},
  shorttitle = {Discrete {{Deep Feature Extraction}}},
  abstract = {First steps towards a mathematical theory of deep convolutional neural networks for feature extraction were made---for the continuous-time case---in Mallat, 2012, and Wiatowski and B$\backslash$"olcskei, 2015. This paper considers the discrete case, introduces new convolutional neural network architectures, and proposes a mathematical framework for their analysis. Specifically, we establish deformation and translation sensitivity results of local and global nature, and we investigate how certain structural properties of the input signal are reflected in the corresponding feature vectors. Our theory applies to general filters and general Lipschitz-continuous non-linearities and pooling operators. Experiments on handwritten digit classification and facial landmark detection---including feature importance evaluation---complement the theoretical findings.},
  timestamp = {2017-10-24T12:05:37Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.08283},
  primaryClass = {cs, math, stat},
  journal = {arXiv:1605.08283 [cs, math, stat]},
  author = {Wiatowski, Thomas and Tschannen, Michael and Stani{\'c}, Aleksandar and Grohs, Philipp and B{\"o}lcskei, Helmut},
  month = may,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Theory,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annote = {Comment: Proc. of International Conference on Machine Learning (ICML), New York, USA, June 2016, to appear},
  file = {Wiatowski et al_2016_Discrete Deep Feature Extraction.pdf:/Users/fergalcotter/Dropbox/Papers/Wiatowski et al_2016_Discrete Deep Feature Extraction.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/6T68HXQW/1605.html:text/html},
  groups = {Maths,Maths}
}

@article{kendall_what_2017,
  title = {What {{Uncertainties Do We Need}} in {{Bayesian Deep Learning}} for {{Computer Vision}}?},
  abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model -- uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
  timestamp = {2017-10-19T12:34:52Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.04977},
  primaryClass = {cs},
  journal = {arXiv:1703.04977 [cs]},
  author = {Kendall, Alex and Gal, Yarin},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote = {Comment: NIPS 2017},
  file = {Kendall_Gal_2017_What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision.pdf:/Users/fergalcotter/Dropbox/Papers/Kendall_Gal_2017_What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/P9NX7B53/1703.html:text/html},
  groups = {FCNs,FCNs}
}

@article{wiatowski_topology_2017,
  title = {Topology {{Reduction}} in {{Deep Convolutional Feature Extraction Networks}}},
  abstract = {Deep convolutional neural networks (CNNs) used in practice employ potentially hundreds of layers and \$10\$,\$000\$s of nodes. Such network sizes entail significant computational complexity due to the large number of convolutions that need to be carried out; in addition, a large number of parameters needs to be learned and stored. Very deep and wide CNNs may therefore not be well suited to applications operating under severe resource constraints as is the case, e.g., in low-power embedded and mobile platforms. This paper aims at understanding the impact of CNN topology, specifically depth and width, on the network's feature extraction capabilities. We address this question for the class of scattering networks that employ either Weyl-Heisenberg filters or wavelets, the modulus non-linearity, and no pooling. The exponential feature map energy decay results in Wiatowski et al., 2017, are generalized to \$$\backslash$mathcal\{O\}(a\^\{-N\})\$, where an arbitrary decay factor \$a$>$1\$ can be realized through suitable choice of the Weyl-Heisenberg prototype function or the mother wavelet. We then show how networks of fixed (possibly small) depth \$N\$ can be designed to guarantee that \$((1-$\backslash$varepsilon)$\backslash$cdot 100)$\backslash$\%\$ of the input signal's energy are contained in the feature vector. Based on the notion of operationally significant nodes, we characterize, partly rigorously and partly heuristically, the topology-reducing effects of (effectively) band-limited input signals, band-limited filters, and feature map symmetries. Finally, for networks based on Weyl-Heisenberg filters, we determine the prototype function bandwidth that minimizes---for fixed network depth \$N\$---the average number of operationally significant nodes per layer.},
  timestamp = {2017-10-24T12:05:58Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.02711},
  primaryClass = {cs, math, stat},
  journal = {arXiv:1707.02711 [cs, math, stat]},
  author = {Wiatowski, Thomas and Grohs, Philipp and B{\"o}lcskei, Helmut},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Theory,Computer Science - Learning,Mathematics - Functional Analysis,Statistics - Machine Learning},
  annote = {Comment: 12 pages, 7 Figures, Proc. of SPIE (Wavelets and Sparsity XVII), San Diego, USA, July 2017, to appear, (invited paper)},
  file = {Wiatowski et al_2017_Topology Reduction in Deep Convolutional Feature Extraction Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Wiatowski et al_2017_Topology Reduction in Deep Convolutional Feature Extraction Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/9CHUFB78/1707.html:text/html},
  groups = {Maths,Maths}
}

@article{long_fully_2014-1,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
  timestamp = {2017-10-19T11:42:45Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.4038},
  primaryClass = {cs},
  journal = {arXiv:1411.4038 [cs]},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  month = nov,
  year = {2014},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote = {Comment: to appear in CVPR (2015)},
  file = {Long et al_2014_Fully Convolutional Networks for Semantic Segmentation.pdf:/Users/fergalcotter/Dropbox/Papers/Long et al_2014_Fully Convolutional Networks for Semantic Segmentation_2.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/J3C9FTHS/1411.html:text/html},
  groups = {FCNs,FCNs}
}

@article{kendall_bayesian_2015,
  title = {Bayesian {{SegNet}}: {{Model Uncertainty}} in {{Deep Convolutional Encoder}}-{{Decoder Architectures}} for {{Scene Understanding}}},
  shorttitle = {Bayesian {{SegNet}}},
  abstract = {We present a deep learning framework for probabilistic pixel-wise semantic segmentation, which we term Bayesian SegNet. Semantic segmentation is an important tool for visual scene understanding and a meaningful measure of uncertainty is essential for decision making. Our contribution is a practical system which is able to predict pixel-wise class labels with a measure of model uncertainty. We achieve this by Monte Carlo sampling with dropout at test time to generate a posterior distribution of pixel class labels. In addition, we show that modelling uncertainty improves segmentation performance by 2-3\% across a number of state of the art architectures such as SegNet, FCN and Dilation Network, with no additional parametrisation. We also observe a significant improvement in performance for smaller datasets where modelling uncertainty is more effective. We benchmark Bayesian SegNet on the indoor SUN Scene Understanding and outdoor CamVid driving scenes datasets.},
  timestamp = {2017-10-19T12:07:10Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.02680},
  primaryClass = {cs},
  journal = {arXiv:1511.02680 [cs]},
  author = {Kendall, Alex and Badrinarayanan, Vijay and Cipolla, Roberto},
  month = nov,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {Kendall et al_2015_Bayesian SegNet.pdf:/Users/fergalcotter/Dropbox/Papers/Kendall et al_2015_Bayesian SegNet.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/ERWN6HS5/1511.html:text/html},
  groups = {FCNs,FCNs}
}

@article{wiatowski_mathematical_2015,
  title = {A {{Mathematical Theory}} of {{Deep Convolutional Neural Networks}} for {{Feature Extraction}}},
  abstract = {Deep convolutional neural networks have led to breakthrough results in numerous practical machine learning tasks such as classification of images in the ImageNet data set, control-policy-learning to play Atari games or the board game Go, and image captioning. Many of these applications first perform feature extraction and then feed the results thereof into a trainable classifier. The mathematical analysis of deep convolutional neural networks for feature extraction was initiated by Mallat, 2012. Specifically, Mallat considered so-called scattering networks based on a wavelet transform followed by the modulus non-linearity in each network layer, and proved translation invariance (asymptotically in the wavelet scale parameter) and deformation stability of the corresponding feature extractor. This paper complements Mallat's results by developing a theory of deep convolutional neural networks for feature extraction encompassing general convolutional transforms, or in more technical parlance, general semi-discrete frames (including Weyl-Heisenberg, curvelet, shearlet, ridgelet, and wavelet frames), general Lipschitz-continuous non-linearities (e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents, and modulus functions), and general Lipschitz-continuous pooling operators emulating sub-sampling and averaging. In addition, all of these elements can be different in different network layers. For the resulting feature extractor we prove a translation invariance result which is of vertical nature in the sense of the network depth determining the amount of invariance, and we establish deformation sensitivity bounds that apply to signal classes with inherent deformation insensitivity such as, e.g., band-limited functions.},
  timestamp = {2017-10-24T12:05:10Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.06293},
  primaryClass = {cs, math, stat},
  journal = {arXiv:1512.06293 [cs, math, stat]},
  author = {Wiatowski, Thomas and B{\"o}lcskei, Helmut},
  month = dec,
  year = {2015},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Learning,Mathematics - Functional Analysis,Statistics - Machine Learning},
  annote = {Comment: IEEE Transactions on Information Theory, submitted},
  file = {Wiatowski_Bölcskei_2015_A Mathematical Theory of Deep Convolutional Neural Networks for Feature.pdf:/Users/fergalcotter/Dropbox/Papers/Wiatowski_Bölcskei_2015_A Mathematical Theory of Deep Convolutional Neural Networks for Feature.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/5K2557UE/1512.html:text/html},
  groups = {Maths,Maths}
}

@incollection{ranzato_efficient_2007,
  title = {Efficient {{Learning}} of {{Sparse Representations}} with an {{Energy}}-{{Based Model}}},
  timestamp = {2017-10-24T16:59:19Z},
  booktitle = {Advances in {{Neural Information Processing Systems}} 19},
  publisher = {{MIT Press}},
  author = {aurelio Ranzato, Marc$\backslash$textquotesingle and Poultney, Christopher and Chopra, Sumit and Cun, Yann L.},
  editor = {Sch{\"o}lkopf, P. B. and Platt, J. C. and Hoffman, T.},
  year = {2007},
  pages = {1137--1144},
  file = {Ranzato et al_2007_Efficient Learning of Sparse Representations with an Energy-Based Model.pdf:/Users/fergalcotter/Dropbox/Papers/Ranzato et al_2007_Efficient Learning of Sparse Representations with an Energy-Based Model.pdf:application/pdf;NIPS Snapshort:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/C68BF8V8/3112-efficient-learning-of-sparse-representations-with-an-energy-based-model.html:text/html},
  groups = {Unsupervised,Unsupervised}
}

@inproceedings{ranzato_unsupervised_2007,
  title = {Unsupervised {{Learning}} of {{Invariant Feature Hierarchies}} with {{Applications}} to {{Object Recognition}}},
  doi = {10.1109/CVPR.2007.383157},
  abstract = {We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64\% error on MNIST, and 54\% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples.},
  timestamp = {2017-10-24T16:59:45Z},
  booktitle = {2007 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ranzato, M. and Huang, F. J. and Boureau, Y. L. and LeCun, Y.},
  month = jun,
  year = {2007},
  keywords = {Computer architecture,computer vision,Convolution,Detectors,Feature extraction,feature extractor,feature-pooling layer,Gabor filters,invariant feature hierarchy,multiple convolution filters,object detection,object recognition,supervised learning,unsupervised learning},
  pages = {1--8},
  file = {Ranzato et al_2007_Unsupervised Learning of Invariant Feature Hierarchies with Applications to.pdf:/Users/fergalcotter/Dropbox/Papers/Ranzato et al_2007_Unsupervised Learning of Invariant Feature Hierarchies with Applications to.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/JSKDA74N/4270182.html:text/html},
  groups = {Unsupervised,Unsupervised}
}

@article{freeman_design_1991,
  title = {The {{Design}} and {{Use}} of {{Steerable Filters}}},
  volume = {13},
  issn = {0162-8828},
  doi = {10.1109/34.93808},
  abstract = {The authors present an efficient architecture to synthesize filters of arbitrary orientations from linear combinations of basis filters, allowing one to adaptively steer a filter to any orientation, and to determine analytically the filter output as a function of orientation. Steerable filters may be designed in quadrature pairs to allow adaptive control over phase as well as orientation. The authors show how to design and steer the filters and present examples of their use in the analysis of orientation and phase, angularly adaptive filtering, edge detection, and shape from shading. One can also build a self-similar steerable pyramid representation. The same concepts can be generalized to the design of 3-D steerable filters.},
  timestamp = {2017-10-27T11:24:44Z},
  number = {9},
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  author = {Freeman, William T. and Adelson, Edward H.},
  month = sep,
  year = {1991},
  keywords = {adaptive filtering,adaptive filters,design,edge detection,filtering and prediction theory,picture processing,shape from shading,steerable filters},
  pages = {891--906},
  file = {Freeman_Adelson_1991_The Design and Use of Steerable Filters.pdf:/Users/fergalcotter/Dropbox/Papers/Freeman_Adelson_1991_The Design and Use of Steerable Filters.pdf:application/pdf},
  groups = {Wavelets,Wavelets}
}

@article{mizrahi_linear_2013,
  title = {Linear and {{Parallel Learning}} of {{Markov Random Fields}}},
  abstract = {We introduce a new embarrassingly parallel parameter learning algorithm for Markov random fields with untied parameters which is efficient for a large class of practical models. Our algorithm parallelizes naturally over cliques and, for graphs of bounded degree, its complexity is linear in the number of cliques. Unlike its competitors, our algorithm is fully parallel and for log-linear models it is also data efficient, requiring only the local sufficient statistics of the data to estimate parameters.},
  timestamp = {2017-10-27T17:35:38Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1308.6342},
  primaryClass = {cs, stat},
  journal = {arXiv:1308.6342 [cs, stat]},
  author = {Mizrahi, Yariv Dror and Denil, Misha and {de Freitas}, Nando},
  month = aug,
  year = {2013},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {Mizrahi et al_2013_Linear and Parallel Learning of Markov Random Fields.pdf:/Users/fergalcotter/Dropbox/Papers/Mizrahi et al_2013_Linear and Parallel Learning of Markov Random Fields.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/FVI5VM6A/1308.html:text/html},
  groups = {Unsupervised,Unsupervised}
}

@article{paulin_convolutional_2016-1,
  title = {Convolutional {{Patch Representations}} for {{Image Retrieval}}: An {{Unsupervised Approach}}},
  shorttitle = {Convolutional {{Patch Representations}} for {{Image Retrieval}}},
  abstract = {Convolutional neural networks (CNNs) have recently received a lot of attention due to their ability to model local stationary structures in natural images in a multi-scale fashion, when learning all model parameters with supervision. While excellent performance was achieved for image classification when large amounts of labeled visual data are available, their success for un-supervised tasks such as image retrieval has been moderate so far. Our paper focuses on this latter setting and explores several methods for learning patch descriptors without supervision with application to matching and instance-level retrieval. To that effect, we propose a new family of convolutional descriptors for patch representation , based on the recently introduced convolutional kernel networks. We show that our descriptor, named Patch-CKN, performs better than SIFT as well as other convolutional networks learned by artificially introducing supervision and is significantly faster to train. To demonstrate its effectiveness, we perform an extensive evaluation on standard benchmarks for patch and image retrieval where we obtain state-of-the-art results. We also introduce a new dataset called RomePatches, which allows to simultaneously study descriptor performance for patch and image retrieval.},
  timestamp = {2017-10-30T19:15:44Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.00438},
  primaryClass = {cs},
  journal = {arXiv:1603.00438 [cs]},
  author = {Paulin, Mattis and Mairal, Julien and Douze, Matthijs and Harchaoui, Zaid and Perronnin, Florent and Schmid, Cordelia},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Paulin et al_2016_Convolutional Patch Representations for Image Retrieval.pdf:/Users/fergalcotter/Dropbox/Papers/Paulin et al_2016_Convolutional Patch Representations for Image Retrieval_2.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/27GHV98Q/1603.html:text/html},
  groups = {Maths,Maths}
}

@incollection{sutton_introduction_2012,
  title = {An {{Introduction}} to {{Conditional Random Fields}}},
  volume = {4},
  isbn = {978-1-60198-573-6},
  abstract = {Many tasks involve predicting a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling. They combine the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This survey describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in many areas, including natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large-scale CRFs. We do not assume previous knowledge of graphical modeling, so this survey is intended to be useful to practitioners in a wide variety of fields.},
  timestamp = {2017-10-31T10:45:26Z},
  booktitle = {Foundations and {{Trends}} in {{Machine Learning}}},
  publisher = {{Now Publishers}},
  author = {Sutton, Charles and McCallum, Andrew},
  month = apr,
  year = {2012},
  file = {Sutton_McCallum_2012_An Introduction to Conditional Random Fields.pdf:/Users/fergalcotter/Dropbox/Papers/Sutton_McCallum_2012_An Introduction to Conditional Random Fields.pdf:application/pdf},
  groups = {Books}
}

@article{sabour_dynamic_2017,
  title = {Dynamic {{Routing Between Capsules}}},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation paramters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  timestamp = {2017-10-31T23:35:10Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.09829},
  primaryClass = {cs},
  journal = {arXiv:1710.09829 [cs]},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
  month = oct,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Sabour et al_2017_Dynamic Routing Between Capsules.pdf:/Users/fergalcotter/Dropbox/Papers/Sabour et al_2017_Dynamic Routing Between Capsules.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/VVKZ5ZAC/1710.html:text/html},
  groups = {Third Year}
}

@article{su_one_2017,
  title = {One Pixel Attack for Fooling Deep Neural Networks},
  abstract = {Recent research has revealed that the output of Deep neural networks(DNN) is not continuous and very sensitive to tiny perturbation on the input vectors and accordingly several methods have been proposed for crafting effective perturbation against the networks. In this paper, we propose a novel method for optically calculating extremely small adversarial perturbation (few-pixels attack), based on differential evolution. It requires much less adversarial information and works with a broader classes of DNN models. The results show that 73.8\$$\backslash$\%\$ of the test images can be crafted to adversarial images with modification just on one pixel with 98.7\$$\backslash$\%\$ confidence on average. In addition, it is known that investigating the robustness problem of DNN can bring critical clues for understanding the geometrical features of the DNN decision map in high dimensional input space. The results of conducting few-pixels attack contribute quantitative measurements and analysis to the geometrical understanding from a different perspective compared to previous works.},
  timestamp = {2017-10-31T23:45:02Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.08864},
  primaryClass = {cs, stat},
  journal = {arXiv:1710.08864 [cs, stat]},
  author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Kouichi, Sakurai},
  month = oct,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Statistics - Machine Learning},
  file = {Su et al_2017_One pixel attack for fooling deep neural networks.pdf:/Users/fergalcotter/Dropbox/Papers/Su et al_2017_One pixel attack for fooling deep neural networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/98UNC6NQ/1710.html:text/html},
  groups = {Third Year}
}

@inproceedings{hinton_transforming_2011,
  series = {Lecture Notes in Computer Science},
  title = {Transforming {{Auto}}-{{Encoders}}},
  isbn = {978-3-642-21734-0 978-3-642-21735-7},
  doi = {10.1007/978-3-642-21735-7_6},
  abstract = {The artificial neural networks that are used to recognize shapes typically use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered features, like SIFT [6], that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instantiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the hand-engineered features currently used in computer vision because it provides an efficient way of adapting the features to the domain.},
  language = {en},
  timestamp = {2017-11-01T10:23:25Z},
  urldate = {2017-11-01},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2011},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Hinton, Geoffrey E. and Krizhevsky, Alex and Wang, Sida D.},
  month = jun,
  year = {2011},
  pages = {44--51},
  file = {Hinton et al_2011_Transforming Auto-Encoders.pdf:/Users/fergalcotter/Dropbox/Papers/Hinton et al_2011_Transforming Auto-Encoders.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/U9F7S3KS/10.html:text/html},
  groups = {Third Year}
}

@article{jacobsen_multiscale_2017-1,
  title = {Multiscale {{Hierarchical Convolutional Networks}}},
  abstract = {Deep neural network algorithms are difficult to analyze because they lack structure allowing to understand the properties of underlying transforms and invariants. Multiscale hierarchical convolutional networks are structured deep convolutional networks where layers are indexed by progressively higher dimensional attributes, which are learned from training data. Each new layer is computed with multidimensional convolutions along spatial and attribute variables. We introduce an efficient implementation of such networks where the dimensionality is progressively reduced by averaging intermediate layers along attribute indices. Hierarchical networks are tested on CIFAR image data bases where they obtain comparable precisions to state of the art networks, with much fewer parameters. We study some properties of the attributes learned from these databases.},
  timestamp = {2017-11-01T10:41:35Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.04140},
  primaryClass = {cs, stat},
  journal = {arXiv:1703.04140 [cs, stat]},
  author = {Jacobsen, J{\"o}rn-Henrik and Oyallon, Edouard and Mallat, St{\'e}phane and Smeulders, Arnold W. M.},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {Jacobsen et al_2017_Multiscale Hierarchical Convolutional Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Jacobsen et al_2017_Multiscale Hierarchical Convolutional Networks_2.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/C9PDDRCD/1703.html:text/html},
  groups = {multiscale,multiscale}
}

@article{francesca_spectral_2016,
  title = {Spectral {{Convolution Networks}}},
  abstract = {Previous research has shown that computation of convolution in the frequency domain provides a significant speedup versus traditional convolution network implementations. However, this performance increase comes at the expense of repeatedly computing the transform and its inverse in order to apply other network operations such as activation, pooling, and dropout. We show, mathematically, how convolution and activation can both be implemented in the frequency domain using either the Fourier or Laplace transformation. The main contributions are a description of spectral activation under the Fourier transform and a further description of an efficient algorithm for computing both convolution and activation under the Laplace transform. By computing both the convolution and activation functions in the frequency domain, we can reduce the number of transforms required, as well as reducing overall complexity. Our description of a spectral activation function, together with existing spectral analogs of other network functions may then be used to compose a fully spectral implementation of a convolution network.},
  timestamp = {2017-11-01T13:49:40Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.05378},
  primaryClass = {cs, stat},
  journal = {arXiv:1611.05378 [cs, stat]},
  author = {Francesca, Maria and Hughes, Arthur and Gregg, David},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {Francesca et al_2016_Spectral Convolution Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Francesca et al_2016_Spectral Convolution Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/6U6RMPZX/1611.html:text/html},
  groups = {multiscale,multiscale}
}

@article{shen_convolutional_2017,
  title = {Convolutional {{Neural Pyramid}} for {{Image Processing}}},
  abstract = {We propose a principled convolutional neural pyramid (CNP) framework for general low-level vision and image processing tasks. It is based on the essential finding that many applications require large receptive fields for structure understanding. But corresponding neural networks for regression either stack many layers or apply large kernels to achieve it, which is computationally very costly. Our pyramid structure can greatly enlarge the field while not sacrificing computation efficiency. Extra benefit includes adaptive network depth and progressive upsampling for quasi-realtime testing on VGA-size input. Our method profits a broad set of applications, such as depth/RGB image restoration, completion, noise/artifact removal, edge refinement, image filtering, image enhancement and colorization.},
  timestamp = {2017-11-01T13:53:03Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.02071},
  primaryClass = {cs},
  journal = {arXiv:1704.02071 [cs]},
  author = {Shen, Xiaoyong and Chen, Ying-Cong and Tao, Xin and Jia, Jiaya},
  month = apr,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Shen et al_2017_Convolutional Neural Pyramid for Image Processing.pdf:/Users/fergalcotter/Dropbox/Papers/Shen et al_2017_Convolutional Neural Pyramid for Image Processing.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/UMKUQUIC/1704.html:text/html},
  groups = {multiscale,multiscale}
}

@article{haber_learning_2017,
  title = {Learning across Scales - {{A}} Multiscale Method for {{Convolution Neural Networks}}},
  abstract = {In this work we establish the relation between optimal control and training deep Convolution Neural Networks (CNNs). We show that the forward propagation in CNNs can be interpreted as a time-dependent nonlinear differential equation and learning as controlling the parameters of the differential equation such that the network approximates the data-label relation for given training data. Using this continuous interpretation we derive two new methods to scale CNNs with respect to two different dimensions. The first class of multiscale methods connects low-resolution and high-resolution data through prolongation and restriction of CNN parameters. We demonstrate that this enables classifying high-resolution images using CNNs trained with low-resolution images and vice versa and warm-starting the learning process. The second class of multiscale methods connects shallow and deep networks and leads to new training strategies that gradually increase the depths of the CNN while re-using parameters for initializations.},
  timestamp = {2017-11-01T10:41:32Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.02009},
  primaryClass = {cs},
  journal = {arXiv:1703.02009 [cs]},
  author = {Haber, Eldad and Ruthotto, Lars and Holtham, Elliot and Jun, Seong-Hwan},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {Haber et al_2017_Learning across scales - A multiscale method for Convolution Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Haber et al_2017_Learning across scales - A multiscale method for Convolution Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/N4F8B873/1703.html:text/html},
  groups = {multiscale,multiscale}
}

@techreport{cotter_understanding_2016,
  title = {Understanding and {{Building Deep Convolutional Networks}} with {{Wavelets}}},
  timestamp = {2017-11-02T21:25:31Z},
  author = {Cotter, Fergal},
  month = aug,
  year = {2016},
  pages = {126},
  file = {Cotter_2016_Understanding and Building Deep Convolutional Networks with Wavelets.pdf:/Users/fergalcotter/Dropbox/Papers/Cotter_2016_Understanding and Building Deep Convolutional Networks with Wavelets.pdf:application/pdf},
  groups = {First Year}
}

@inproceedings{wietzke_geometry_2009,
  title = {The Geometry of {{2D}} Image Signals},
  doi = {10.1109/CVPR.2009.5206784},
  abstract = {This paper covers a fundamental problem of local phase based signal processing: the isotropic generalization of the classical 1D analytic signal to two dimensions. The well known analytic signal enables the analysis of local phase and amplitude information of 1D signals. Local phase, amplitude and additional orientation information can be extracted by the 2D monogenic signal with the restriction to the subclass of intrinsically one dimensional signals. In case of 2D image signals the monogenic signal enables the rotationally invariant analysis of lines and edges. In this work we present the 2D analytic signal as a novel generalization of both the analytic signal and the 2D monogenic signal. In case of 2D image signals the 2D analytic signal enables the isotropic analysis of lines, edges, corners and junctions in one unified framework. Furthermore, we show that 2D signals exist per se in a 3D projective subspace of the homogeneous conformal space which delivers a descriptive geometric interpretation of signals providing new insights on the relation of geometry and 2D signals.},
  timestamp = {2017-11-02T22:03:23Z},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Wietzke, L. and Sommer, G. and Fleischmann, O.},
  month = jun,
  year = {2009},
  keywords = {1D analytic signal,2D analytic signal,2D image signal,2D monogenic signal,3D projective subspace,computational geometry,descriptive geometric signal interpretation,Geometry,homogeneous conformal space,image processing,invariant analysis,isotropic analysis,isotropic generalization,phase based signal processing},
  pages = {1690--1697},
  file = {Wietzke et al_2009_The geometry of 2D image signals.pdf:/Users/fergalcotter/Dropbox/Papers/Wietzke et al_2009_The geometry of 2D image signals.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/6CHP7P3H/5206784.html:text/html},
  groups = {Wavelets,Wavelets}
}

@article{bridge_introduction_2017,
  title = {Introduction {{To The Monogenic Signal}}},
  abstract = {The monogenic signal is an image analysis methodology that was introduced by Felsberg and Sommer in 2001 and has been employed for a variety of purposes in image processing and computer vision research. In particular, it has been found to be useful in the analysis of ultrasound imagery in several research scenarios mostly in work done within the BioMedIA lab at Oxford. However, the literature on the monogenic signal can be difficult to penetrate due to the lack of a single resource to explain the various principles from basics. The purpose of this document is therefore to introduce the principles, purpose, applications, and limitations of the methodology. It assumes some background knowledge from the fields of image and signal processing, in particular a good knowledge of Fourier transforms as applied to signals and images. We will not attempt to provide a thorough math- ematical description or derivation of the monogenic signal, but rather focus on developing an intuition for understanding and using the methodology and refer the reader elsewhere for a more mathematical treatment.},
  timestamp = {2017-11-02T22:05:47Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.09199},
  primaryClass = {cs},
  journal = {arXiv:1703.09199 [cs]},
  author = {Bridge, Christopher P.},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Bridge_2017_Introduction To The Monogenic Signal.pdf:/Users/fergalcotter/Dropbox/Papers/Bridge_2017_Introduction To The Monogenic Signal.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/MSVJQ4CE/1703.html:text/html},
  groups = {Wavelets,Wavelets}
}

@article{worrall_harmonic_2016,
  title = {Harmonic {{Networks}}: {{Deep Translation}} and {{Rotation Equivariance}}},
  shorttitle = {Harmonic {{Networks}}},
  abstract = {Translating or rotating an input image should not affect the results of many computer vision tasks. Convolutional neural networks (CNNs) are already translation equivariant: input image translations produce proportionate feature map translations. This is not the case for rotations. Global rotation equivariance is typically sought through data augmentation, but patch-wise equivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance to patch-wise translation and 360-rotation. We achieve this by replacing regular CNN filters with circular harmonics, returning a maximal response and orientation for every receptive field patch. H-Nets use a rich, parameter-efficient and low computational complexity representation, and we show that deep feature maps within the network encode complicated rotational invariants. We demonstrate that our layers are general enough to be used in conjunction with the latest architectures and techniques, such as deep supervision and batch normalization. We also achieve state-of-the-art classification on rotated-MNIST, and competitive results on other benchmark challenges.},
  timestamp = {2017-11-03T15:10:33Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.04642},
  primaryClass = {cs, stat},
  journal = {arXiv:1612.04642 [cs, stat]},
  author = {Worrall, Daniel E. and Garbin, Stephan J. and Turmukhambetov, Daniyar and Brostow, Gabriel J.},
  month = dec,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Statistics - Machine Learning},
  annote = {Comment: Submitted to CVPR 2017},
  file = {Worrall et al_2016_Harmonic Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Worrall et al_2016_Harmonic Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/ZQXCURWH/1612.html:text/html},
  groups = {Wavelets,Wavelets}
}

@inproceedings{liu_2d/3d_2012,
  title = {{{2D}}/{{3D}} Rotation-Invariant Detection Using Equivariant Filters and Kernel Weighted Mapping},
  doi = {10.1109/CVPR.2012.6247766},
  abstract = {In many vision problems, rotation-invariant analysis is necessary or preferred. Popular solutions are mainly based on pose normalization or brute-force learning, neglecting the intrinsic properties of rotations. In this paper, we present a rotation invariant detection approach built on the equivariant filter framework, with a new model for learning the filtering behavior. The special properties of the harmonic basis, which is related to the irreducible representation of the rotation group, directly guarantees rotation invariance of the whole approach. The proposed kernel weighted mapping ensures high learning capability while respecting the invariance constraint. We demonstrate its performance on 2D object detection with in-plane rotations, and a 3D application on rotation-invariant landmark detection in microscopic volumetric data.},
  timestamp = {2017-11-03T15:23:28Z},
  booktitle = {2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Liu, K. and Wang, Q. and Driever, W. and Ronneberger, O.},
  month = jun,
  year = {2012},
  keywords = {2D object detection,2D rotation-invariant detection,3D rotation-invariant detection,brute-force learning,Computational modeling,computer vision,equivariant filter,Estimation,Feature extraction,filtering theory,Harmonic analysis,harmonic basis,intrinsic property,Kernel,kernel weighted mapping,microscopic volumetric data,pose estimation,pose normalization,rotation-invariant landmark detection,Training,Vectors,vision problem},
  pages = {917--924},
  file = {Liu et al_2012_2D-3D rotation-invariant detection using equivariant filters and kernel.pdf:/Users/fergalcotter/Dropbox/Papers/Liu et al_2012_2D-3D rotation-invariant detection using equivariant filters and kernel.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/FKRD2EZV/6247766.html:text/html},
  groups = {Wavelets,Wavelets}
}

@article{luan_gabor_2017,
  title = {Gabor {{Convolutional Networks}}},
  abstract = {Steerable properties dominate the design of traditional filters, e.g., Gabor filters, and endow features the capability of dealing with spatial transformations. However, such excellent properties have not been well explored in the popular deep convolutional neural networks (DCNNs). In this paper, we propose a new deep model, termed Gabor Convolutional Networks (GCNs or Gabor CNNs), which incorporates Gabor filters into DCNNs to enhance the resistance of deep learned features to the orientation and scale changes. By only manipulating the basic element of DCNNs based on Gabor filters, i.e., the convolution operator, GCNs can be easily implemented and are compatible with any popular deep learning architecture. Experimental results demonstrate the super capability of our algorithm in recognizing objects, where the scale and rotation changes occur frequently. The proposed GCNs have much fewer learnable network parameters, and thus is easier to train with an end-to-end pipeline. To encourage further developments, the source code is released at Github.},
  timestamp = {2017-11-05T15:53:39Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.01450},
  primaryClass = {cs},
  journal = {arXiv:1705.01450 [cs]},
  author = {Luan, Shangzhen and Zhang, Baochang and Chen, Chen and Cao, Xianbin and Han, Jungong and Liu, Jianzhuang},
  month = may,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Luan et al_2017_Gabor Convolutional Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Luan et al_2017_Gabor Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/GTKK6688/1705.html:text/html},
  groups = {Wavelets,Wavelets}
}

@book{granlund_signal_1995,
  address = {Norwell, MA, USA},
  title = {Signal {{Processing}} for {{Computer Vision}}},
  isbn = {978-0-7923-9530-0},
  abstract = {From the Publisher:Signal Processing for Computer Vision provides a unique and thorough treatment of the signal processing aspects of filters and operators for low level computer vision. Computer Vision has progressed considerably over the years. From methods only applicable to simple images, it has developed to deal with increasingly complex scenes, volumes and time sequences. A substantial part of this book deals with the problem of designing models that can be used for several purposes with computer vision. These partial models have some general properties of invariance generation and generality in model generation. Signal Processing for Computer Vision is the first book to give a unified treatment of representation and filtering of higher order data, such as vectors and tensors in multidimensional space. Included is a systematic organisation for the implementation of complex models in a hierarchical modular structure and novel material on adaptive filtering using tensor data representation. Signal Processing for Computer Vision is intended for final year undergraduate and graduate students as well as engineers and researchers in the field of computer vision and image processing.},
  timestamp = {2017-11-06T18:01:09Z},
  publisher = {{Kluwer Academic Publishers}},
  author = {Granlund, GFosta H. and Knutsson, Hans},
  year = {1995},
  file = {Granlund_Knutsson_1995_Signal Processing for Computer Vision.pdf:/Users/fergalcotter/Dropbox/Papers/Granlund_Knutsson_1995_Signal Processing for Computer Vision.pdf:application/pdf},
  groups = {Books}
}

@article{felsberg_monogenic_2001,
  title = {The Monogenic Signal},
  volume = {49},
  issn = {1053-587X},
  doi = {10.1109/78.969520},
  abstract = {This paper introduces a two-dimensional (2-D) generalization of the analytic signal. This novel approach is based on the Riesz transform, which is used instead of the Hilbert transform. The combination of a 2-D signal with the Riesz transformed one yields a sophisticated 2-D analytic signal: the monogenic signal. The approach is derived analytically from irrotational and solenoidal vector fields. Based on local amplitude and local phase, an appropriate local signal representation that preserves the split of identity, i.e., the invariance-equivariance property of signal decomposition, is presented. This is one of the central properties of the one-dimensional (1-D) analytic signal that decomposes a signal into structural and energetic information. We show that further properties of the analytic signal concerning symmetry, energy, allpass transfer function, and orthogonality are also preserved, and we compare this with the behavior of other approaches for a 2-D analytic signal. As a central topic of this paper, a geometric phase interpretation that is based on the relation between the 1-D analytic signal and the 2-D monogenic signal established by the Radon (1986) transform is introduced. Possible applications of this relationship are sketched, and references to other applications of the monogenic signal are given},
  timestamp = {2017-11-06T17:33:02Z},
  number = {12},
  journal = {IEEE Transactions on Signal Processing},
  author = {Felsberg, M. and Sommer, G.},
  month = dec,
  year = {2001},
  keywords = {1D analytic signal,2D analytic signal,2D monogenic signal,allpass transfer function,Band pass filters,Frequency estimation,geometric phase interpretation,Hilbert transform,Hilbert transforms,Image edge detection,image processing,Information analysis,information filtering,irrotational vector fields,local amplitude representation,local phase representation,local signal representation,monogenic signal,Multidimensional signal processing,Phase measurement,Radar signal processing,Radon transform,Radon transforms,Riesz transform,Signal analysis,signal energy,signal processing,signal representation,signal symmetry,solenoidal vector fields},
  pages = {3136--3144},
  file = {Felsberg_Sommer_2001_The monogenic signal.pdf:/Users/fergalcotter/Dropbox/Papers/Felsberg_Sommer_2001_The monogenic signal.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/3P2FIQ6A/969520.html:text/html},
  groups = {Wavelets,Wavelets}
}

@phdthesis{stewart_forshaw_vehicle_2017,
  title = {Vehicle {{Re}}-Identification {{Using Phase Functions}} of {{Complex Wavelets}}},
  timestamp = {2017-11-08T13:49:15Z},
  school = {University of Cambridge},
  author = {{Stewart Forshaw}},
  month = mar,
  year = {2017},
  file = {Stewart Forshaw_2017_Vehicle Re-identification Using Phase Functions of Complex Wavelets.pdf:/Users/fergalcotter/Dropbox/Papers/Stewart Forshaw_2017_Vehicle Re-identification Using Phase Functions of Complex Wavelets.pdf:application/pdf},
  groups = {Theses}
}

@article{graves_generating_2013,
  title = {Generating {{Sequences With Recurrent Neural Networks}}},
  abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
  timestamp = {2017-11-14T14:15:00Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1308.0850},
  primaryClass = {cs},
  journal = {arXiv:1308.0850 [cs]},
  author = {Graves, Alex},
  month = aug,
  year = {2013},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  annote = {Comment: Thanks to Peng Liu and Sergey Zyrianov for various corrections},
  file = {Graves_2013_Generating Sequences With Recurrent Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Graves_2013_Generating Sequences With Recurrent Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/G6VSHCBR/1308.html:text/html},
  groups = {reading group,reading group,reading group}
}

@misc{_home_????,
  title = {Home - Colah's Blog},
  timestamp = {2017-11-14T14:15:32Z},
  urldate = {2017-11-14},
  howpublished = {\url{https://colah.github.io/}},
  groups = {reading group,reading group,reading group}
}

@article{socher_recursive_2013,
  title = {Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank},
  volume = {1631},
  abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment composition-ality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80\% up to 85.4\%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7\%, an improvement of 9.7\% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
  timestamp = {2017-11-14T14:17:47Z},
  journal = {EMNLP},
  author = {Socher, R and Perelygin, A and Wu, J.Y. and Chuang, J and Manning, C.D. and Ng, A.Y. and Potts, C},
  month = jan,
  year = {2013},
  pages = {1631--1642},
  file = {Socher et al_2013_Recursive deep models for semantic compositionality over a sentiment treebank.pdf:/Users/fergalcotter/Dropbox/Papers/Socher et al_2013_Recursive deep models for semantic compositionality over a sentiment treebank.pdf:application/pdf;Socher et al_2013_Recursive deep models for semantic compositionality over a sentiment treebank.pdf:/Users/fergalcotter/Dropbox/Papers/Socher et al_2013_Recursive deep models for semantic compositionality over a sentiment treebank.pdf:application/pdf;Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/PIJ9SK3F/284039049_Recursive_deep_models_for_semantic_compositionality_over_a_sentiment_treebank.html:text/html},
  groups = {reading group,reading group,reading group}
}

@article{juefei-xu_local_2016,
  title = {Local {{Binary Convolutional Neural Networks}}},
  abstract = {We propose local binary convolution (LBC), an efficient alternative to convolutional layers in standard convolutional neural networks (CNN). The design principles of LBC are motivated by local binary patterns (LBP). The LBC layer comprises of a set of fixed sparse pre-defined binary convolutional filters that are not updated during the training process, a non-linear activation function and a set of learnable linear weights. The linear weights combine the activated filter responses to approximate the corresponding activated filter responses of a standard convolutional layer. The LBC layer affords significant parameter savings, 9x to 169x in the number of learnable parameters compared to a standard convolutional layer. Furthermore, the sparse and binary nature of the weights also results in up to 9x to 169x savings in model size compared to a standard convolutional layer. We demonstrate both theoretically and experimentally that our local binary convolution layer is a good approximation of a standard convolutional layer. Empirically, CNNs with LBC layers, called local binary convolutional neural networks (LBCNN), achieves performance parity with regular CNNs on a range of visual datasets (MNIST, SVHN, CIFAR-10, and ImageNet) while enjoying significant computational savings.},
  timestamp = {2017-11-14T16:02:59Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.06049},
  primaryClass = {cs},
  journal = {arXiv:1608.06049 [cs]},
  author = {Juefei-Xu, Felix and Boddeti, Vishnu Naresh and Savvides, Marios},
  month = aug,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  annote = {Comment: To appear in CVPR 2017 as Spotlight},
  file = {Juefei-Xu et al_2016_Local Binary Convolutional Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Juefei-Xu et al_2016_Local Binary Convolutional Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/4BIPIQZK/1608.html:text/html},
  groups = {Scat Ideas,Scat Ideas}
}

@article{courbariaux_binarized_2016,
  title = {Binarized {{Neural Networks}}: {{Training Deep Neural Networks}} with {{Weights}} and {{Activations Constrained}} to +1 or -1},
  shorttitle = {Binarized {{Neural Networks}}},
  abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
  timestamp = {2017-11-30T12:53:46Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.02830},
  primaryClass = {cs},
  journal = {arXiv:1602.02830 [cs]},
  author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Learning},
  annote = {Comment: 11 pages and 3 figures},
  file = {Courbariaux et al_2016_Binarized Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Courbariaux et al_2016_Binarized Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/IB8JECRD/1602.html:text/html},
  groups = {Fast ML,Fast ML}
}

@inproceedings{_discrete-valued_????,
  title = {{{DISCRETE}}-{{VALUED NEURAL NETWORKS USING VARIATIONAL INFERENCE}}},
  timestamp = {2017-11-30T12:54:38Z},
  file = {pdf.pdf:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/2W6UVMQ3/pdf.pdf:application/pdf},
  groups = {Fast ML,Fast ML}
}

@article{louizos_bayesian_2017,
  title = {Bayesian {{Compression}} for {{Deep Learning}}},
  abstract = {Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.},
  timestamp = {2017-11-30T12:55:06Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08665},
  primaryClass = {cs, stat},
  journal = {arXiv:1705.08665 [cs, stat]},
  author = {Louizos, Christos and Ullrich, Karen and Welling, Max},
  month = may,
  year = {2017},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  annote = {Comment: Published as a conference paper at NIPS 2017},
  file = {Louizos et al_2017_Bayesian Compression for Deep Learning.pdf:/Users/fergalcotter/Dropbox/Papers/Louizos et al_2017_Bayesian Compression for Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/RUTB5VDB/1705.html:text/html},
  groups = {Fast ML,Fast ML}
}

@misc{_new_2016,
  title = {New Paper: "{{Safely}} Interruptible Agents"},
  shorttitle = {New Paper},
  abstract = {Google DeepMind Research Scientist Laurent Orseau and MIRI Research Associate Stuart Armstrong have written a new paper on error-tolerant agent designs, ``Safely interruptible agents.'' The paper is forthcoming at the 32nd Conference on Uncertainty in Artificial Intelligence. Abstract: Reinforcement learning agents interacting with a complex environment like the real world are unlikely to behave optimally...  Read more \guillemotright{}},
  timestamp = {2017-12-06T18:07:36Z},
  urldate = {2017-12-06},
  howpublished = {\url{https://intelligence.org/2016/06/01/new-paper-safely-interruptible-agents/}},
  journal = {Machine Intelligence Research Institute},
  year = {2016-06-01T23:58:38+00:00},
  file = {Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/BC2WPR2J/new-paper-safely-interruptible-agents.html:text/html},
  groups = {Safe AI,Safe AI}
}

@article{armstrong_good_2017,
  title = {Good and Safe Uses of {{AI Oracles}}},
  abstract = {An Oracle is a design for potentially high power artificial intelligences (AIs), where the AI is made safe by restricting it to only answer questions. Unfortunately most designs cause the Oracle to be motivated to manipulate humans with the contents of their answers, and Oracles of potentially high intelligence might be very successful at this. Solving the problem, without compromising the accuracy of the answer, is tricky. This paper reduces the issue to a cryptographic-style problem of Alice ensuring that her Oracle answers her questions while not providing key information to an eavesdropping Eve. Two Oracle designs solve this problem, one counterfactual (the Oracle answers as if it expected its answer to never be read) and one on-policy (limited by the quantity of information it can transmit).},
  timestamp = {2017-12-06T18:11:14Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.05541},
  primaryClass = {cs},
  journal = {arXiv:1711.05541 [cs]},
  author = {Armstrong, Stuart},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence},
  annote = {Comment: 11 pages, 2 figures},
  file = {Armstrong_2017_Good and safe uses of AI Oracles.pdf:/Users/fergalcotter/Dropbox/Papers/Armstrong_2017_Good and safe uses of AI Oracles.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/S4K3D8BD/1711.html:text/html},
  groups = {Safe AI,Safe AI}
}

@inproceedings{chaux_2d_2007,
  title = {{{2D Dual}}-{{Tree Complex Biorthogonal M}}-{{Band Wavelet Transform}}},
  volume = {3},
  doi = {10.1109/ICASSP.2007.366812},
  abstract = {Dual-tree wavelet transforms have recently gained popularity since they provide low-redundancy directional analyses of images. In our recent work, dyadic real dual-tree decompositions have been extended to the M-band case, so adding much flexibility to this analysis tool. In this work, we propose to further extend this framework on two fronts by considering (i) biorthogonal and (ii) complex M-band dual-tree decompositions. Denoising results are finally provided to demonstrate the validity of the proposed design rules.},
  timestamp = {2017-12-07T22:22:41Z},
  booktitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  author = {Chaux, C. and Pesquet, J. C. and Duval, L.},
  month = apr,
  year = {2007},
  keywords = {2D dual-tree complex biorthogonal M-band wavelet transform,denoising results,filter bank,Frequency,Gaussian noise,Hilbert transforms,Image analysis,Image coding,image denoising,image processing,image restoration,low-redundancy directional analyses,Noise reduction,Wavelet analysis,wavelet transforms},
  pages = {III--845--III--848},
  file = {Chaux et al_2007_2D Dual-Tree Complex Biorthogonal M-Band Wavelet Transform.pdf:/Users/fergalcotter/Dropbox/Papers/Chaux et al_2007_2D Dual-Tree Complex Biorthogonal M-Band Wavelet Transform.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/HB595UTH/4217842.html:text/html},
  groups = {Wavelets,Wavelets}
}

@article{yu_theory_2008,
  title = {Theory of {{Dual}}-{{Tree Complex Wavelets}}},
  volume = {56},
  issn = {1053-587X},
  doi = {10.1109/TSP.2008.925970},
  abstract = {We study analyticity of the complex wavelets in Kingsbury's dual-tree wavelet transform. A notion of scaling transformation function that defines the relationship between the primal and dual scaling functions is introduced and studied in detail. The analyticity property is examined and dealt with via the transformation function. We separate analyticity from other properties of the wavelet such as orthogonality or biorthogonality. This separation allows a unified treatment of analyticity for general setting of the wavelet system, which can be dyadic or M-band; orthogonal or biorthogonal; scalar or multiple; bases or frames. We show that analyticity of the complex wavelets can be characterized by scaling filter relationship and wavelet filter relationship via the scaling transformation function. For general orthonormal wavelets and dyadic biorthogonal scalar wavelets, the transformation function is shown to be paraunitary and has a linear phase delay of omega/2 in (0, 2pi).},
  timestamp = {2017-12-07T22:23:33Z},
  number = {9},
  journal = {IEEE Transactions on Signal Processing},
  author = {Yu, R.},
  month = sep,
  year = {2008},
  keywords = {analyticity property,dual-tree complex wavelet,Dual-tree complex wavelets,dyadic biorthogonal scalar wavelet,filter banks,Hilbert transform,orthonormal wavelet,scaling transformation function,trees (mathematics),wavelet transforms},
  pages = {4263--4273},
  file = {Yu_2008_Theory of Dual-Tree Complex Wavelets.pdf:/Users/fergalcotter/Dropbox/Papers/Yu_2008_Theory of Dual-Tree Complex Wavelets.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/J3SXR5Q2/4527203.html:text/html},
  groups = {Wavelets,Wavelets}
}

@article{xie_aggregated_2016-1,
  title = {Aggregated {{Residual Transformations}} for {{Deep Neural Networks}}},
  abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
  timestamp = {2017-12-11T12:42:26Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.05431},
  primaryClass = {cs},
  journal = {arXiv:1611.05431 [cs]},
  author = {Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote = {Comment: Accepted to CVPR 2017. Code and models: https://github.com/facebookresearch/ResNeXt},
  file = {Xie et al_2016_Aggregated Residual Transformations for Deep Neural Networks.pdf:/Users/fergalcotter/Dropbox/Papers/Xie et al_2016_Aggregated Residual Transformations for Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/72D3UKFP/1611.html:text/html},
  groups = {state of the art,state of the art,state of the art}
}

@inproceedings{gregor_learning_2010,
  address = {USA},
  series = {ICML'10},
  title = {Learning {{Fast Approximations}} of {{Sparse Coding}}},
  isbn = {978-1-60558-907-7},
  abstract = {In Sparse Coding (SC), input vectors are reconstructed using a sparse linear combination of basis vectors. SC has become a popular method for extracting features from data. For a given input, SC minimizes a quadratic reconstruction error with an L1 penalty term on the code. The process is often too slow for applications such as real-time pattern recognition. We proposed two versions of a very fast algorithm that produces approximate estimates of the sparse code that can be used to compute good visual features, or to initialize exact iterative algorithms. The main idea is to train a non-linear, feed-forward predictor with a specific architecture and a fixed depth to produce the best possible approximation of the sparse code. A version of the method, which can be seen as a trainable version of Li and Osher's coordinate descent method, is shown to produce approximate solutions with 10 times less computation than Li and Os-her's for the same approximation error. Unlike previous proposals for sparse code predictors, the system allows a kind of approximate "explaining away" to take place during inference. The resulting predictor is differentiable and can be included into globally-trained recognition systems.},
  timestamp = {2017-12-20T21:04:10Z},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  publisher = {{Omnipress}},
  author = {Gregor, Karol and LeCun, Yann},
  year = {2010},
  pages = {399--406},
  groups = {Unsupervised,Unsupervised}
}

@article{ilyas_query-efficient_2017,
  title = {Query-{{Efficient Black}}-Box {{Adversarial Examples}}},
  abstract = {Current neural network-based image classifiers are susceptible to adversarial examples, even in the black-box setting, where the attacker is limited to query access without access to gradients. Previous methods --- substitute networks and coordinate-based finite-difference methods --- are either unreliable or query-inefficient, making these methods impractical for certain problems. We introduce a new method for reliably generating adversarial examples under more restricted, practical black-box threat models. First, we apply natural evolution strategies to perform black-box attacks using two to three orders of magnitude fewer queries than previous methods. Second, we introduce a new algorithm to perform targeted adversarial attacks in the partial-information setting, where the attacker only has access to a limited number of target classes. Using these techniques, we successfully perform the first targeted adversarial attack against a commercially deployed machine learning system, the Google Cloud Vision API, in the partial information setting.},
  timestamp = {2018-01-02T14:17:17Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.07113},
  primaryClass = {cs, stat},
  journal = {arXiv:1712.07113 [cs, stat]},
  author = {Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Jessy},
  month = dec,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Statistics - Machine Learning},
  file = {Ilyas et al_2017_Query-Efficient Black-box Adversarial Examples.pdf:/Users/fergalcotter/Dropbox/Papers/Ilyas et al_2017_Query-Efficient Black-box Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/DCD7CUT9/1712.html:text/html},
  groups = {Safe AI,Safe AI}
}

@article{gorban_approximation_2016,
  title = {Approximation with {{Random Bases}}: {{Pro}} et {{Contra}}},
  volume = {364-365},
  issn = {00200255},
  shorttitle = {Approximation with {{Random Bases}}},
  doi = {10.1016/j.ins.2015.09.021},
  abstract = {In this work we discuss the problem of selecting suitable approximators from families of parameterized elementary functions that are known to be dense in a Hilbert space of functions. We consider and analyze published procedures, both randomized and deterministic, for selecting elements from these families that have been shown to ensure the rate of convergence in \$L\_2\$ norm of order \$O(1/N)\$, where \$N\$ is the number of elements. We show that both randomized and deterministic procedures are successful if additional information about the families of functions to be approximated is provided. In the absence of such additional information one may observe exponential growth of the number of terms needed to approximate the function and/or extreme sensitivity of the outcome of the approximation to parameters. Implications of our analysis for applications of neural networks in modeling and control are illustrated with examples.},
  timestamp = {2018-01-02T19:39:27Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.04631},
  journal = {Information Sciences},
  author = {Gorban, Alexander N. and Tyukin, Ivan Yu and Prokhorov, Danil V. and Sofeikov, Konstantin I.},
  month = oct,
  year = {2016},
  keywords = {41A45; 41A45; 90C59; 92B20; 68W20,Computer Science - Discrete Mathematics,Computer Science - Numerical Analysis},
  pages = {129--145},
  annote = {Comment: arXiv admin note: text overlap with arXiv:0905.0677},
  file = {Gorban et al_2016_Approximation with Random Bases.pdf:/Users/fergalcotter/Dropbox/Papers/Gorban et al_2016_Approximation with Random Bases.pdf:application/pdf;arXiv.org Snapshot:/Users/fergalcotter/Library/Application Support/Zotero/Profiles/xmn09k4n.default/zotero/storage/DP97T2HC/1506.html:text/html},
  groups = {Books}
}

@comment{jabref-meta: databaseType:bibtex;}
@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:Books\;0\;;
1 ExplicitGroup:CCG\;0\;;
1 ExplicitGroup:First Year\;0\;;
2 ExplicitGroup:Tricks of the Trade\;0\;;
2 ExplicitGroup:Rigorous Deep Learning\;0\;;
2 ExplicitGroup:Biological Vision\;0\;;
2 ExplicitGroup:Other Networks\;0\;;
3 ExplicitGroup:Bayesian Models\;0\;;
2 ExplicitGroup:Latex\;0\;;
2 ExplicitGroup:Miscellaneous\;0\;;
2 ExplicitGroup:Notes\;0\;;
2 ExplicitGroup:Scatternets and Handcrafted\;0\;;
3 ExplicitGroup:advanced\;0\;;
3 ExplicitGroup:Background\;0\;;
2 ExplicitGroup:Eye Tracking\;0\;;
2 ExplicitGroup:ICML Viz\;0\;;
2 ExplicitGroup:ICVSS\;0\;;
2 ExplicitGroup:Sparsity for Vision\;0\;;
2 ExplicitGroup:Wavelets and DTCWT\;0\;;
3 ExplicitGroup:Lifting\;0\;;
3 ExplicitGroup:DTCWT applications\;0\;;
2 ExplicitGroup:Cords Papers\;0\;;
2 ExplicitGroup:Neural Networks\;0\;;
2 ExplicitGroup:Unsupervised Methods\;0\;;
2 ExplicitGroup:CNNs\;0\;;
3 ExplicitGroup:datasets\;0\;;
3 ExplicitGroup:Transfer Learning\;0\;;
3 ExplicitGroup:Visualization & Generative\;0\;;
3 ExplicitGroup:Specific Design\;0\;;
3 ExplicitGroup:criticisms\;0\;;
3 ExplicitGroup:network features\;0\;;
3 ExplicitGroup:state of the art\;0\;;
3 ExplicitGroup:Generic Design\;0\;;
3 ExplicitGroup:summary papers\;0\;;
3 ExplicitGroup:other\;0\;;
1 ExplicitGroup:Tricks of the Trade\;0\;;
1 ExplicitGroup:Rigorous Deep Learning\;0\;;
1 ExplicitGroup:Biological Vision\;0\;;
1 ExplicitGroup:Other Networks\;0\;;
2 ExplicitGroup:Bayesian Models\;0\;;
1 ExplicitGroup:Bayesian Models\;0\;;
1 ExplicitGroup:Latex\;0\;;
1 ExplicitGroup:Miscellaneous\;0\;;
1 ExplicitGroup:Notes\;0\;;
1 ExplicitGroup:Scatternets and Handcrafted\;0\;;
2 ExplicitGroup:advanced\;0\;;
2 ExplicitGroup:Background\;0\;;
1 ExplicitGroup:advanced\;0\;;
1 ExplicitGroup:Background\;0\;;
1 ExplicitGroup:Eye Tracking\;0\;;
1 ExplicitGroup:ICML Viz\;0\;;
1 ExplicitGroup:ICVSS\;0\;;
1 ExplicitGroup:Sparsity for Vision\;0\;;
1 ExplicitGroup:Wavelets and DTCWT\;0\;;
2 ExplicitGroup:Lifting\;0\;;
2 ExplicitGroup:DTCWT applications\;0\;;
1 ExplicitGroup:Lifting\;0\;;
1 ExplicitGroup:DTCWT applications\;0\;;
1 ExplicitGroup:Cords Papers\;0\;;
1 ExplicitGroup:Neural Networks\;0\;;
1 ExplicitGroup:Unsupervised Methods\;0\;;
1 ExplicitGroup:CNNs\;0\;;
2 ExplicitGroup:datasets\;0\;;
2 ExplicitGroup:Transfer Learning\;0\;;
2 ExplicitGroup:Visualization & Generative\;0\;;
2 ExplicitGroup:Specific Design\;0\;;
2 ExplicitGroup:criticisms\;0\;;
2 ExplicitGroup:network features\;0\;;
2 ExplicitGroup:state of the art\;0\;;
2 ExplicitGroup:Generic Design\;0\;;
2 ExplicitGroup:summary papers\;0\;;
2 ExplicitGroup:other\;0\;;
1 ExplicitGroup:datasets\;0\;;
1 ExplicitGroup:Transfer Learning\;0\;;
1 ExplicitGroup:Visualization & Generative\;0\;;
1 ExplicitGroup:Specific Design\;0\;;
1 ExplicitGroup:criticisms\;0\;;
1 ExplicitGroup:network features\;0\;;
1 ExplicitGroup:state of the art\;0\;;
1 ExplicitGroup:Generic Design\;0\;;
1 ExplicitGroup:summary papers\;0\;;
1 ExplicitGroup:other\;0\;;
1 ExplicitGroup:mlsp cites\;0\;;
1 ExplicitGroup:Second Year\;0\;;
2 ExplicitGroup:Graphs\;0\;;
2 ExplicitGroup:Statistical Methods\;0\;;
2 ExplicitGroup:Deconv\;0\;;
2 ExplicitGroup:Sparse Coding\;0\;;
3 ExplicitGroup:Pursuit Methods\;0\;;
3 ExplicitGroup:Compressed Learning\;0\;;
2 ExplicitGroup:Visualization\;0\;;
2 ExplicitGroup:Fun\;0\;;
3 ExplicitGroup:priors\;0\;;
3 ExplicitGroup:gan\;0\;;
3 ExplicitGroup:style\;0\;;
3 ExplicitGroup:finance\;0\;;
3 ExplicitGroup:memcnn\;0\;;
2 ExplicitGroup:CNNS\;0\;;
2 ExplicitGroup:Reading Groups\;0\;;
2 ExplicitGroup:Complex CNNs\;0\;;
2 ExplicitGroup:Scatternets\;0\;;
2 ExplicitGroup:Wavelets\;0\;;
3 ExplicitGroup:Monogenic\;0\;;
2 ExplicitGroup:object detection\;0\;;
2 ExplicitGroup:Lifting\;0\;;
2 ExplicitGroup:FFT + Spectral\;0\;;
1 ExplicitGroup:Graphs\;0\;;
1 ExplicitGroup:Statistical Methods\;0\;;
1 ExplicitGroup:Deconv\;0\;;
1 ExplicitGroup:Sparse Coding\;0\;;
2 ExplicitGroup:Pursuit Methods\;0\;;
2 ExplicitGroup:Compressed Learning\;0\;;
1 ExplicitGroup:Pursuit Methods\;0\;;
1 ExplicitGroup:Compressed Learning\;0\;;
1 ExplicitGroup:Visualization\;0\;;
1 ExplicitGroup:Fun\;0\;;
2 ExplicitGroup:priors\;0\;;
2 ExplicitGroup:gan\;0\;;
2 ExplicitGroup:style\;0\;;
2 ExplicitGroup:finance\;0\;;
2 ExplicitGroup:memcnn\;0\;;
1 ExplicitGroup:priors\;0\;;
1 ExplicitGroup:gan\;0\;;
1 ExplicitGroup:style\;0\;;
1 ExplicitGroup:finance\;0\;;
1 ExplicitGroup:memcnn\;0\;;
1 ExplicitGroup:CNNS\;0\;;
1 ExplicitGroup:Reading Groups\;0\;;
1 ExplicitGroup:Complex CNNs\;0\;;
1 ExplicitGroup:Scatternets\;0\;;
1 ExplicitGroup:Wavelets\;0\;;
2 ExplicitGroup:Monogenic\;0\;;
1 ExplicitGroup:Monogenic\;0\;;
1 ExplicitGroup:object detection\;0\;;
1 ExplicitGroup:Lifting\;0\;;
1 ExplicitGroup:FFT + Spectral\;0\;;
1 ExplicitGroup:Software\;0\;;
1 ExplicitGroup:Theses\;0\;;
1 ExplicitGroup:Third Year\;0\;;
2 ExplicitGroup:FCNs\;0\;;
2 ExplicitGroup:Maths\;0\;;
3 ExplicitGroup:reading group\;0\;;
2 ExplicitGroup:Scat Ideas\;0\;;
2 ExplicitGroup:Unsupervised\;0\;;
2 ExplicitGroup:Wavelets\;0\;;
2 ExplicitGroup:multiscale\;0\;;
2 ExplicitGroup:Fast ML\;0\;;
2 ExplicitGroup:Safe AI\;0\;;
1 ExplicitGroup:FCNs\;0\;;
1 ExplicitGroup:Maths\;0\;;
2 ExplicitGroup:reading group\;0\;;
1 ExplicitGroup:reading group\;0\;;
1 ExplicitGroup:Scat Ideas\;0\;;
1 ExplicitGroup:Unsupervised\;0\;;
1 ExplicitGroup:Wavelets\;0\;;
1 ExplicitGroup:multiscale\;0\;;
1 ExplicitGroup:Fast ML\;0\;;
1 ExplicitGroup:Safe AI\;0\;;
}

